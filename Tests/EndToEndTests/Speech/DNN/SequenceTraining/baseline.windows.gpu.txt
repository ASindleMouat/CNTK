CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 268381192 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 12 2016 06:56:04
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData
07/12/2016 08:01:49: -------------------------------------------------------------------
07/12/2016 08:01:49: Build info: 

07/12/2016 08:01:49: 		Built time: Jul 12 2016 06:56:04
07/12/2016 08:01:49: 		Last modified date: Fri Jul  8 10:29:45 2016
07/12/2016 08:01:49: 		Build type: Release
07/12/2016 08:01:49: 		Build target: GPU
07/12/2016 08:01:49: 		With 1bit-SGD: no
07/12/2016 08:01:49: 		Math lib: mkl
07/12/2016 08:01:49: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
07/12/2016 08:01:49: 		CUB_PATH: C:\src\cub-1.4.1
07/12/2016 08:01:49: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
07/12/2016 08:01:49: 		Build Branch: HEAD
07/12/2016 08:01:49: 		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
07/12/2016 08:01:49: 		Built by svcphil on liana-08-w
07/12/2016 08:01:49: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
07/12/2016 08:01:49: -------------------------------------------------------------------
07/12/2016 08:01:51: -------------------------------------------------------------------
07/12/2016 08:01:51: GPU info:

07/12/2016 08:01:51: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/12/2016 08:01:51: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/12/2016 08:01:51: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/12/2016 08:01:51: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/12/2016 08:01:51: -------------------------------------------------------------------

07/12/2016 08:01:51: Running on DPHAIM-24 at 2016/07/12 08:01:51
07/12/2016 08:01:51: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu  DeviceId=0  timestamping=true



07/12/2016 08:01:51: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/12/2016 08:01:51: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "$RunDir$/models/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.sequence.0"
    editPath  = "$ConfigDir$/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "$DataDir$/model.overalltying"
            transpFile = "$DataDir$/model.transprob"
        ]
        lattices = [
            denlatTocFile = "$DataDir$/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

07/12/2016 08:01:51: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/12/2016 08:01:51: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/12/2016 08:01:51: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

07/12/2016 08:01:51: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/12/2016 08:01:51: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
07/12/2016 08:01:51: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/12/2016 08:01:51: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
07/12/2016 08:01:51: Precision = "float"
07/12/2016 08:01:51: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech
07/12/2016 08:01:51: CNTKCommandTrainInfo: dptPre1 : 2
07/12/2016 08:01:51: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech
07/12/2016 08:01:51: CNTKCommandTrainInfo: dptPre2 : 2
07/12/2016 08:01:51: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech
07/12/2016 08:01:51: CNTKCommandTrainInfo: speechTrain : 4
07/12/2016 08:01:51: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence
07/12/2016 08:01:51: CNTKCommandTrainInfo: sequenceTrain : 3
07/12/2016 08:01:51: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11

07/12/2016 08:01:51: ##############################################################################
07/12/2016 08:01:51: #                                                                            #
07/12/2016 08:01:51: # Action "train"                                                             #
07/12/2016 08:01:51: #                                                                            #
07/12/2016 08:01:51: ##############################################################################

07/12/2016 08:01:51: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 08:01:52: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 08:01:52: Created model with 19 nodes on GPU 0.

07/12/2016 08:01:52: Training criterion node(s):
07/12/2016 08:01:52: 	ce = CrossEntropyWithSoftmax

07/12/2016 08:01:52: Evaluation criterion node(s):

07/12/2016 08:01:52: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
000000D4984EF4A0: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
000000D4984EF5E0: {[HL1.t Value[512 x *]] }
000000D4984EF900: {[ce Gradient[1]] }
000000D4984EFE00: {[scaledLogLikelihood Value[132 x 1 x *]] }
000000D4984EFEA0: {[logPrior Value[132 x 1]] }
000000D4984EFF40: {[featNorm Value[363 x *]] }
000000D4984EFFE0: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
000000D4984F01C0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
000000D4984F03A0: {[OL.t Gradient[132 x 1 x *]] }
000000D4984F0440: {[OL.W Value[132 x 512]] }
000000D4984F0580: {[OL.b Gradient[132 x 1]] }
000000D4984F06C0: {[ce Value[1]] }
000000D4984F0B20: {[OL.b Value[132 x 1]] }
000000D4984F0D00: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
000000D4984F1020: {[err Value[1]] }
000000D4984F10C0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
000000D4A6E30640: {[HL1.b Value[512 x 1]] }
000000D4A6E30780: {[features Value[363 x *]] }
000000D4A6E319A0: {[labels Value[132 x *]] }
000000D4A6E31AE0: {[globalInvStd Value[363 x 1]] }
000000D4A6E31E00: {[globalMean Value[363 x 1]] }
000000D4A6E31EA0: {[globalPrior Value[132 x 1]] }
000000D4A6E32120: {[HL1.W Value[512 x 363]] }

07/12/2016 08:01:52: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 08:01:52: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 08:01:53: Starting minibatch loop.
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.82139473 * 2560; err = 0.83359375 * 2560; time = 0.2054s; samplesPerSecond = 12461.7
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.94043388 * 2560; err = 0.72187500 * 2560; time = 0.0172s; samplesPerSecond = 148491.9
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.51675339 * 2560; err = 0.65664062 * 2560; time = 0.0170s; samplesPerSecond = 150173.1
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.26993408 * 2560; err = 0.61562500 * 2560; time = 0.0171s; samplesPerSecond = 150129.0
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.01949539 * 2560; err = 0.55781250 * 2560; time = 0.0170s; samplesPerSecond = 150898.9
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.89216309 * 2560; err = 0.52734375 * 2560; time = 0.0170s; samplesPerSecond = 150730.1
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.85436096 * 2560; err = 0.52539063 * 2560; time = 0.0170s; samplesPerSecond = 150437.8
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.71935120 * 2560; err = 0.50273437 * 2560; time = 0.0171s; samplesPerSecond = 149471.6
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.61281128 * 2560; err = 0.47304687 * 2560; time = 0.0170s; samplesPerSecond = 150632.5
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.55461884 * 2560; err = 0.45703125 * 2560; time = 0.0170s; samplesPerSecond = 150340.6
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.56578522 * 2560; err = 0.45742187 * 2560; time = 0.0171s; samplesPerSecond = 150014.6
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.56222534 * 2560; err = 0.46875000 * 2560; time = 0.0170s; samplesPerSecond = 150526.3
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.48975220 * 2560; err = 0.44375000 * 2560; time = 0.0170s; samplesPerSecond = 150508.6
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.51242676 * 2560; err = 0.45273438 * 2560; time = 0.0170s; samplesPerSecond = 150783.4
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46072083 * 2560; err = 0.42968750 * 2560; time = 0.0170s; samplesPerSecond = 150597.1
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.42187805 * 2560; err = 0.42343750 * 2560; time = 0.0170s; samplesPerSecond = 150287.7
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.40252686 * 2560; err = 0.42656250 * 2560; time = 0.0170s; samplesPerSecond = 150181.9
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.36772766 * 2560; err = 0.40625000 * 2560; time = 0.0170s; samplesPerSecond = 150464.3
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.32636719 * 2560; err = 0.40429688 * 2560; time = 0.0170s; samplesPerSecond = 150393.6
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.34658813 * 2560; err = 0.39531250 * 2560; time = 0.0170s; samplesPerSecond = 150243.6
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.37981262 * 2560; err = 0.41718750 * 2560; time = 0.0171s; samplesPerSecond = 149979.5
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.30067749 * 2560; err = 0.39062500 * 2560; time = 0.0170s; samplesPerSecond = 150606.0
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.34448547 * 2560; err = 0.40429688 * 2560; time = 0.0170s; samplesPerSecond = 150561.7
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.34487000 * 2560; err = 0.40742187 * 2560; time = 0.0171s; samplesPerSecond = 149611.4
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.33879700 * 2560; err = 0.39609375 * 2560; time = 0.0170s; samplesPerSecond = 150641.4
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.30860291 * 2560; err = 0.39257813 * 2560; time = 0.0171s; samplesPerSecond = 149690.1
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.28979187 * 2560; err = 0.37265625 * 2560; time = 0.0170s; samplesPerSecond = 150278.8
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.28291931 * 2560; err = 0.39296875 * 2560; time = 0.0170s; samplesPerSecond = 150270.0
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.24534607 * 2560; err = 0.37539062 * 2560; time = 0.0171s; samplesPerSecond = 149988.3
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.21101685 * 2560; err = 0.36562500 * 2560; time = 0.0170s; samplesPerSecond = 150561.7
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.25861206 * 2560; err = 0.37695313 * 2560; time = 0.0170s; samplesPerSecond = 150676.9
07/12/2016 08:01:53:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.22863464 * 2560; err = 0.36367187 * 2560; time = 0.0166s; samplesPerSecond = 154505.4
07/12/2016 08:01:53: Finished Epoch[ 1 of 2]: [Training] ce = 1.63096504 * 81920; err = 0.46358643 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.915926s
07/12/2016 08:01:53: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech.1'

07/12/2016 08:01:53: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 08:01:53: Starting minibatch loop.
07/12/2016 08:01:53:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.24892588 * 2560; err = 0.37382813 * 2560; time = 0.0187s; samplesPerSecond = 137023.0
07/12/2016 08:01:53:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.19152651 * 2560; err = 0.35351563 * 2560; time = 0.0172s; samplesPerSecond = 148638.4
07/12/2016 08:01:53:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.28531933 * 2560; err = 0.38359375 * 2560; time = 0.0172s; samplesPerSecond = 149245.0
07/12/2016 08:01:53:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.28695564 * 2560; err = 0.38046875 * 2560; time = 0.0170s; samplesPerSecond = 150323.0
07/12/2016 08:01:53:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.23508911 * 2560; err = 0.38320312 * 2560; time = 0.0171s; samplesPerSecond = 149419.2
07/12/2016 08:01:53:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.23924332 * 2560; err = 0.37851563 * 2560; time = 0.0170s; samplesPerSecond = 150659.1
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.21547470 * 2560; err = 0.38125000 * 2560; time = 0.0170s; samplesPerSecond = 150208.3
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16492844 * 2560; err = 0.35664062 * 2560; time = 0.0170s; samplesPerSecond = 150827.8
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.19786148 * 2560; err = 0.37343750 * 2560; time = 0.0170s; samplesPerSecond = 150323.0
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.17275467 * 2560; err = 0.35781250 * 2560; time = 0.0170s; samplesPerSecond = 150579.4
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.28627548 * 2560; err = 0.38593750 * 2560; time = 0.0170s; samplesPerSecond = 150730.1
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.20148163 * 2560; err = 0.37226562 * 2560; time = 0.0170s; samplesPerSecond = 150278.8
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.20188904 * 2560; err = 0.35195312 * 2560; time = 0.0170s; samplesPerSecond = 150164.2
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.20350037 * 2560; err = 0.35937500 * 2560; time = 0.0171s; samplesPerSecond = 149865.4
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.16490021 * 2560; err = 0.35546875 * 2560; time = 0.0170s; samplesPerSecond = 150305.3
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.16687012 * 2560; err = 0.36015625 * 2560; time = 0.0170s; samplesPerSecond = 150499.7
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.19524231 * 2560; err = 0.35625000 * 2560; time = 0.0170s; samplesPerSecond = 150517.4
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.18535919 * 2560; err = 0.36992188 * 2560; time = 0.0170s; samplesPerSecond = 150632.5
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.16953583 * 2560; err = 0.36250000 * 2560; time = 0.0170s; samplesPerSecond = 150535.1
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.24088135 * 2560; err = 0.37851563 * 2560; time = 0.0171s; samplesPerSecond = 149742.6
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.08954315 * 2560; err = 0.33554688 * 2560; time = 0.0171s; samplesPerSecond = 149882.9
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.17553406 * 2560; err = 0.34843750 * 2560; time = 0.0170s; samplesPerSecond = 150270.0
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.08467407 * 2560; err = 0.32656250 * 2560; time = 0.0170s; samplesPerSecond = 150420.1
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.05924072 * 2560; err = 0.32382813 * 2560; time = 0.0170s; samplesPerSecond = 150632.5
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.11544800 * 2560; err = 0.34921875 * 2560; time = 0.0171s; samplesPerSecond = 150102.6
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13805237 * 2560; err = 0.35156250 * 2560; time = 0.0170s; samplesPerSecond = 150217.1
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.07155151 * 2560; err = 0.33554688 * 2560; time = 0.0170s; samplesPerSecond = 150314.1
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.16022949 * 2560; err = 0.34570313 * 2560; time = 0.0170s; samplesPerSecond = 150173.1
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.09731750 * 2560; err = 0.34843750 * 2560; time = 0.0170s; samplesPerSecond = 150685.7
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10340576 * 2560; err = 0.33710937 * 2560; time = 0.0170s; samplesPerSecond = 150446.6
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.09576416 * 2560; err = 0.34453125 * 2560; time = 0.0170s; samplesPerSecond = 150765.6
07/12/2016 08:01:54:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.10093079 * 2560; err = 0.34843750 * 2560; time = 0.0166s; samplesPerSecond = 154198.3
07/12/2016 08:01:54: Finished Epoch[ 2 of 2]: [Training] ce = 1.17330332 * 81920; err = 0.35842285 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.549106s
07/12/2016 08:01:54: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech'
07/12/2016 08:01:54: CNTKCommandTrainEnd: dptPre1

07/12/2016 08:01:54: Action "train" complete.


07/12/2016 08:01:54: ##############################################################################
07/12/2016 08:01:54: #                                                                            #
07/12/2016 08:01:54: # Action "edit"                                                              #
07/12/2016 08:01:54: #                                                                            #
07/12/2016 08:01:54: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 08:01:54: Action "edit" complete.


07/12/2016 08:01:54: ##############################################################################
07/12/2016 08:01:54: #                                                                            #
07/12/2016 08:01:54: # Action "train"                                                             #
07/12/2016 08:01:54: #                                                                            #
07/12/2016 08:01:54: ##############################################################################

07/12/2016 08:01:54: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 08:01:54: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 08:01:54: Loaded model with 24 nodes on GPU 0.

07/12/2016 08:01:54: Training criterion node(s):
07/12/2016 08:01:54: 	ce = CrossEntropyWithSoftmax

07/12/2016 08:01:54: Evaluation criterion node(s):

07/12/2016 08:01:54: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
000000D4984EF400: {[logPrior Value[132 x 1]] }
000000D4984EF540: {[err Value[1]] }
000000D4984EF5E0: {[HL1.t Value[512 x *3]] }
000000D4984EF720: {[ce Value[1]] }
000000D4984EFA40: {[labels Value[132 x *3]] }
000000D4984F0120: {[scaledLogLikelihood Value[132 x 1 x *3]] }
000000D4984F01C0: {[featNorm Value[363 x *3]] }
000000D4984F1020: {[OL.b Value[132 x 1]] }
000000D4984F10C0: {[OL.W Value[132 x 512]] }
000000D4A6E30780: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
000000D4A6E30960: {[OL.b Gradient[132 x 1]] }
000000D4A6E30FA0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
000000D4A6E319A0: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
000000D4A6E31A40: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
000000D4A6E31AE0: {[ce Gradient[1]] }
000000D4A6E31FE0: {[OL.t Gradient[132 x 1 x *3]] }
000000D4A6E32080: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
000000D4A6E32260: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
000000D4A6E323A0: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
000000D4A6E324E0: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
000000D4A9F1ED90: {[globalMean Value[363 x 1]] }
000000D4A9F1EED0: {[HL2.b Value[512 x 1]] }
000000D4A9F1F010: {[HL2.W Value[512 x 512]] }
000000D4A9F1F6F0: {[globalPrior Value[132 x 1]] }
000000D4A9F1FD30: {[features Value[363 x *3]] }
000000D4A9F20730: {[globalInvStd Value[363 x 1]] }
000000D4A9F20870: {[HL1.b Value[512 x 1]] }
000000D4A9F20A50: {[HL1.W Value[512 x 363]] }

07/12/2016 08:01:54: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 08:01:54: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 08:01:55: Starting minibatch loop.
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 4.35745811 * 2560; err = 0.80976563 * 2560; time = 0.0283s; samplesPerSecond = 90619.5
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.79570580 * 2560; err = 0.69257813 * 2560; time = 0.0210s; samplesPerSecond = 122090.8
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.14324646 * 2560; err = 0.58906250 * 2560; time = 0.0209s; samplesPerSecond = 122564.3
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.88513947 * 2560; err = 0.51484375 * 2560; time = 0.0209s; samplesPerSecond = 122623.0
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.67164993 * 2560; err = 0.46796875 * 2560; time = 0.0209s; samplesPerSecond = 122699.4
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.56040955 * 2560; err = 0.44648437 * 2560; time = 0.0209s; samplesPerSecond = 122599.5
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.54447632 * 2560; err = 0.45507813 * 2560; time = 0.0209s; samplesPerSecond = 122681.7
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.45865936 * 2560; err = 0.43046875 * 2560; time = 0.0209s; samplesPerSecond = 122570.1
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.41931305 * 2560; err = 0.41367188 * 2560; time = 0.0209s; samplesPerSecond = 122353.4
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.36146545 * 2560; err = 0.42148438 * 2560; time = 0.0209s; samplesPerSecond = 122540.8
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.39081421 * 2560; err = 0.40664062 * 2560; time = 0.0209s; samplesPerSecond = 122711.1
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.37024078 * 2560; err = 0.40625000 * 2560; time = 0.0209s; samplesPerSecond = 122740.6
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.32750397 * 2560; err = 0.39765625 * 2560; time = 0.0209s; samplesPerSecond = 122482.2
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.34634552 * 2560; err = 0.40742187 * 2560; time = 0.0209s; samplesPerSecond = 122758.2
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.33963623 * 2560; err = 0.40351562 * 2560; time = 0.0209s; samplesPerSecond = 122646.5
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.32297668 * 2560; err = 0.40703125 * 2560; time = 0.0209s; samplesPerSecond = 122617.1
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29603577 * 2560; err = 0.39687500 * 2560; time = 0.0208s; samplesPerSecond = 122976.4
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28720703 * 2560; err = 0.39101562 * 2560; time = 0.0208s; samplesPerSecond = 122828.9
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.25335693 * 2560; err = 0.38281250 * 2560; time = 0.0209s; samplesPerSecond = 122722.9
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.27086792 * 2560; err = 0.38476563 * 2560; time = 0.0209s; samplesPerSecond = 122693.5
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.26213684 * 2560; err = 0.38398437 * 2560; time = 0.0208s; samplesPerSecond = 122935.1
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.21245422 * 2560; err = 0.37656250 * 2560; time = 0.0208s; samplesPerSecond = 123000.0
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.23672485 * 2560; err = 0.36640625 * 2560; time = 0.0209s; samplesPerSecond = 122552.5
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.22503357 * 2560; err = 0.38320312 * 2560; time = 0.0209s; samplesPerSecond = 122728.8
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.17672729 * 2560; err = 0.33906250 * 2560; time = 0.0209s; samplesPerSecond = 122499.8
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.19516296 * 2560; err = 0.35976562 * 2560; time = 0.0209s; samplesPerSecond = 122758.2
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.22902832 * 2560; err = 0.36406250 * 2560; time = 0.0208s; samplesPerSecond = 122817.1
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.24520874 * 2560; err = 0.38984375 * 2560; time = 0.0208s; samplesPerSecond = 122834.8
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.21764832 * 2560; err = 0.36328125 * 2560; time = 0.0209s; samplesPerSecond = 122670.0
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.17650452 * 2560; err = 0.35742188 * 2560; time = 0.0209s; samplesPerSecond = 122599.5
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.20307007 * 2560; err = 0.36093750 * 2560; time = 0.0209s; samplesPerSecond = 122775.9
07/12/2016 08:01:55:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.15778809 * 2560; err = 0.34179688 * 2560; time = 0.0204s; samplesPerSecond = 125232.4
07/12/2016 08:01:55: Finished Epoch[ 1 of 2]: [Training] ce = 1.49812489 * 81920; err = 0.42536621 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.851044s
07/12/2016 08:01:55: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.1'

07/12/2016 08:01:55: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 08:01:55: Starting minibatch loop.
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.16856174 * 2560; err = 0.34492187 * 2560; time = 0.0223s; samplesPerSecond = 114989.0
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.10856857 * 2560; err = 0.31992188 * 2560; time = 0.0209s; samplesPerSecond = 122435.3
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.19007549 * 2560; err = 0.36367187 * 2560; time = 0.0208s; samplesPerSecond = 123053.3
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.17703590 * 2560; err = 0.35234375 * 2560; time = 0.0208s; samplesPerSecond = 123047.3
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.14981766 * 2560; err = 0.35468750 * 2560; time = 0.0208s; samplesPerSecond = 122876.1
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14499779 * 2560; err = 0.35234375 * 2560; time = 0.0208s; samplesPerSecond = 123142.0
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.15067291 * 2560; err = 0.35703125 * 2560; time = 0.0208s; samplesPerSecond = 123082.8
07/12/2016 08:01:55:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.13103180 * 2560; err = 0.35078125 * 2560; time = 0.0207s; samplesPerSecond = 123701.4
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.14212036 * 2560; err = 0.35390625 * 2560; time = 0.0207s; samplesPerSecond = 123605.8
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.10146179 * 2560; err = 0.33437500 * 2560; time = 0.0208s; samplesPerSecond = 123082.8
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16786346 * 2560; err = 0.35625000 * 2560; time = 0.0208s; samplesPerSecond = 123266.6
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.13780289 * 2560; err = 0.34218750 * 2560; time = 0.0207s; samplesPerSecond = 123391.3
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.15288086 * 2560; err = 0.33828125 * 2560; time = 0.0208s; samplesPerSecond = 123153.9
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.13797455 * 2560; err = 0.34804687 * 2560; time = 0.0208s; samplesPerSecond = 122970.5
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10251465 * 2560; err = 0.33281250 * 2560; time = 0.0208s; samplesPerSecond = 122846.6
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.08231812 * 2560; err = 0.33515625 * 2560; time = 0.0213s; samplesPerSecond = 120363.0
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.12229462 * 2560; err = 0.33984375 * 2560; time = 0.0219s; samplesPerSecond = 116793.6
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11593323 * 2560; err = 0.34531250 * 2560; time = 0.0208s; samplesPerSecond = 123236.9
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.13078003 * 2560; err = 0.34609375 * 2560; time = 0.0208s; samplesPerSecond = 122982.3
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.18909912 * 2560; err = 0.36367187 * 2560; time = 0.0208s; samplesPerSecond = 123059.2
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.06661682 * 2560; err = 0.33046875 * 2560; time = 0.0209s; samplesPerSecond = 122564.3
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.15032043 * 2560; err = 0.34765625 * 2560; time = 0.0207s; samplesPerSecond = 123647.6
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.06511230 * 2560; err = 0.32734375 * 2560; time = 0.0208s; samplesPerSecond = 122811.2
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.05144958 * 2560; err = 0.31640625 * 2560; time = 0.0208s; samplesPerSecond = 123136.1
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.08416748 * 2560; err = 0.33476563 * 2560; time = 0.0208s; samplesPerSecond = 122882.0
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07935486 * 2560; err = 0.33945313 * 2560; time = 0.0208s; samplesPerSecond = 122893.8
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.05736389 * 2560; err = 0.32695313 * 2560; time = 0.0208s; samplesPerSecond = 123165.7
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.10845032 * 2560; err = 0.33671875 * 2560; time = 0.0208s; samplesPerSecond = 123225.0
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.05507813 * 2560; err = 0.31679687 * 2560; time = 0.0208s; samplesPerSecond = 123000.0
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.06679688 * 2560; err = 0.32890625 * 2560; time = 0.0208s; samplesPerSecond = 123017.8
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06694031 * 2560; err = 0.33242187 * 2560; time = 0.0208s; samplesPerSecond = 123065.1
07/12/2016 08:01:56:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06564636 * 2560; err = 0.34101562 * 2560; time = 0.0203s; samplesPerSecond = 125872.8
07/12/2016 08:01:56: Finished Epoch[ 2 of 2]: [Training] ce = 1.11628447 * 81920; err = 0.34095459 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.670902s
07/12/2016 08:01:56: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech'
07/12/2016 08:01:56: CNTKCommandTrainEnd: dptPre2

07/12/2016 08:01:56: Action "train" complete.


07/12/2016 08:01:56: ##############################################################################
07/12/2016 08:01:56: #                                                                            #
07/12/2016 08:01:56: # Action "edit"                                                              #
07/12/2016 08:01:56: #                                                                            #
07/12/2016 08:01:56: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 08:01:56: Action "edit" complete.


07/12/2016 08:01:56: ##############################################################################
07/12/2016 08:01:56: #                                                                            #
07/12/2016 08:01:56: # Action "train"                                                             #
07/12/2016 08:01:56: #                                                                            #
07/12/2016 08:01:56: ##############################################################################

07/12/2016 08:01:56: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 08:01:56: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 08:01:57: Loaded model with 29 nodes on GPU 0.

07/12/2016 08:01:57: Training criterion node(s):
07/12/2016 08:01:57: 	ce = CrossEntropyWithSoftmax

07/12/2016 08:01:57: Evaluation criterion node(s):

07/12/2016 08:01:57: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
000000D49846E690: {[OL.W Value[132 x 512]] }
000000D4984EF5E0: {[HL1.W Value[512 x 363]] }
000000D4984EFA40: {[HL2.W Value[512 x 512]] }
000000D4984F0760: {[HL3.b Value[512 x 1]] }
000000D4984F1020: {[HL2.b Value[512 x 1]] }
000000D4A6E30780: {[OL.b Value[132 x 1]] }
000000D4A6E31FE0: {[HL3.W Value[512 x 512]] }
000000D4A6E32260: {[labels Value[132 x *6]] }
000000D4A9F1EE30: {[globalMean Value[363 x 1]] }
000000D4A9F1F6F0: {[HL1.b Value[512 x 1]] }
000000D4A9F1FD30: {[globalPrior Value[132 x 1]] }
000000D4A9F1FE70: {[globalInvStd Value[363 x 1]] }
000000D4A9F20910: {[features Value[363 x *6]] }
000000D4ACD5B8B0: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
000000D4ACD5B950: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
000000D4ACD5B9F0: {[scaledLogLikelihood Value[132 x 1 x *6]] }
000000D4ACD5BA90: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
000000D4ACD5BBD0: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
000000D4ACD5BD10: {[ce Value[1]] }
000000D4ACD5BE50: {[featNorm Value[363 x *6]] }
000000D4ACD5C170: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
000000D4ACD5C210: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
000000D4ACD5C3F0: {[logPrior Value[132 x 1]] }
000000D4ACD5C490: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
000000D4ACD5C530: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
000000D4ACD5C5D0: {[ce Gradient[1]] }
000000D4ACD5C670: {[OL.b Gradient[132 x 1]] }
000000D4ACD5C8F0: {[HL1.t Value[512 x *6]] }
000000D4ACD5CB70: {[err Value[1]] }
000000D4ACD5CC10: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
000000D4ACD5CCB0: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
000000D4ACD5D070: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
000000D4ACD5D110: {[OL.t Gradient[132 x 1 x *6]] }

07/12/2016 08:01:57: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 08:01:57: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 08:01:57: Starting minibatch loop.
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.15954781 * 2560; err = 0.84453125 * 2560; time = 0.0338s; samplesPerSecond = 75780.0
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.66692467 * 2560; err = 0.64023438 * 2560; time = 0.0248s; samplesPerSecond = 103080.3
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.02206192 * 2560; err = 0.56523437 * 2560; time = 0.0248s; samplesPerSecond = 103155.1
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.76702805 * 2560; err = 0.49375000 * 2560; time = 0.0248s; samplesPerSecond = 103242.5
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.54464188 * 2560; err = 0.43789062 * 2560; time = 0.0247s; samplesPerSecond = 103845.5
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.44837112 * 2560; err = 0.41523437 * 2560; time = 0.0247s; samplesPerSecond = 103626.9
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.43359833 * 2560; err = 0.42187500 * 2560; time = 0.0247s; samplesPerSecond = 103530.6
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36250305 * 2560; err = 0.40390625 * 2560; time = 0.0247s; samplesPerSecond = 103677.3
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.31944885 * 2560; err = 0.38125000 * 2560; time = 0.0248s; samplesPerSecond = 103309.1
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.26297455 * 2560; err = 0.39062500 * 2560; time = 0.0251s; samplesPerSecond = 101939.2
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.30148926 * 2560; err = 0.38554688 * 2560; time = 0.0247s; samplesPerSecond = 103689.9
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27950897 * 2560; err = 0.37851563 * 2560; time = 0.0246s; samplesPerSecond = 103904.5
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.23256378 * 2560; err = 0.36093750 * 2560; time = 0.0246s; samplesPerSecond = 104027.0
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.24608154 * 2560; err = 0.38359375 * 2560; time = 0.0247s; samplesPerSecond = 103795.0
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.24440918 * 2560; err = 0.37343750 * 2560; time = 0.0246s; samplesPerSecond = 103875.0
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.24601135 * 2560; err = 0.37968750 * 2560; time = 0.0246s; samplesPerSecond = 103950.9
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.22501221 * 2560; err = 0.38164063 * 2560; time = 0.0246s; samplesPerSecond = 103925.6
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.21055298 * 2560; err = 0.37304688 * 2560; time = 0.0246s; samplesPerSecond = 103925.6
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.18856812 * 2560; err = 0.35937500 * 2560; time = 0.0247s; samplesPerSecond = 103803.4
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.19142151 * 2560; err = 0.35742188 * 2560; time = 0.0247s; samplesPerSecond = 103841.3
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.18467102 * 2560; err = 0.36015625 * 2560; time = 0.0247s; samplesPerSecond = 103824.5
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.14170532 * 2560; err = 0.35742188 * 2560; time = 0.0247s; samplesPerSecond = 103816.1
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.17152405 * 2560; err = 0.35351563 * 2560; time = 0.0246s; samplesPerSecond = 104010.1
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.16666870 * 2560; err = 0.36445312 * 2560; time = 0.0246s; samplesPerSecond = 103854.0
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.12550659 * 2560; err = 0.32812500 * 2560; time = 0.0247s; samplesPerSecond = 103790.8
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.14731445 * 2560; err = 0.35117188 * 2560; time = 0.0247s; samplesPerSecond = 103828.7
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.16336975 * 2560; err = 0.34414062 * 2560; time = 0.0247s; samplesPerSecond = 103778.2
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.18981628 * 2560; err = 0.36796875 * 2560; time = 0.0246s; samplesPerSecond = 103875.0
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.16020813 * 2560; err = 0.35820313 * 2560; time = 0.0246s; samplesPerSecond = 104247.3
07/12/2016 08:01:57:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.13352051 * 2560; err = 0.34882812 * 2560; time = 0.0247s; samplesPerSecond = 103631.1
07/12/2016 08:01:58:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.16027832 * 2560; err = 0.34375000 * 2560; time = 0.0247s; samplesPerSecond = 103769.8
07/12/2016 08:01:58:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.10745239 * 2560; err = 0.32265625 * 2560; time = 0.0241s; samplesPerSecond = 106330.0
07/12/2016 08:01:58: Finished Epoch[ 1 of 4]: [Training] ce = 1.41264858 * 81920; err = 0.40400391 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.981378s
07/12/2016 08:01:58: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.1'

07/12/2016 08:01:58: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 08:01:58: Starting minibatch loop.
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.09969606 * 5120; err = 0.32890625 * 5120; time = 0.0399s; samplesPerSecond = 128356.2
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.38625898 * 5120; err = 0.39238281 * 5120; time = 0.0307s; samplesPerSecond = 166737.2
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.39283295 * 5120; err = 0.39921875 * 5120; time = 0.0308s; samplesPerSecond = 166250.0
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.21238785 * 5120; err = 0.36933594 * 5120; time = 0.0307s; samplesPerSecond = 166731.8
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14539108 * 5120; err = 0.35800781 * 5120; time = 0.0306s; samplesPerSecond = 167145.5
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.15972366 * 5120; err = 0.35585937 * 5120; time = 0.0307s; samplesPerSecond = 166558.2
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.12575455 * 5120; err = 0.33769531 * 5120; time = 0.0307s; samplesPerSecond = 166634.1
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.06435776 * 5120; err = 0.32519531 * 5120; time = 0.0307s; samplesPerSecond = 166894.8
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09842834 * 5120; err = 0.34101562 * 5120; time = 0.0307s; samplesPerSecond = 166927.5
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.13542252 * 5120; err = 0.34707031 * 5120; time = 0.0307s; samplesPerSecond = 166655.8
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.11245499 * 5120; err = 0.34257813 * 5120; time = 0.0307s; samplesPerSecond = 166607.0
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.05484619 * 5120; err = 0.32265625 * 5120; time = 0.0306s; samplesPerSecond = 167052.8
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05807037 * 5120; err = 0.33027344 * 5120; time = 0.0307s; samplesPerSecond = 166786.1
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.08376770 * 5120; err = 0.33515625 * 5120; time = 0.0307s; samplesPerSecond = 166547.4
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05951996 * 5120; err = 0.32792969 * 5120; time = 0.0307s; samplesPerSecond = 167014.6
07/12/2016 08:01:58:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06747437 * 5120; err = 0.33535156 * 5120; time = 0.0306s; samplesPerSecond = 167074.6
07/12/2016 08:01:58: Finished Epoch[ 2 of 4]: [Training] ce = 1.14102421 * 81920; err = 0.34678955 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.503583s
07/12/2016 08:01:58: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.2'

07/12/2016 08:01:58: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

07/12/2016 08:01:58: Starting minibatch loop.
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.06460552 * 5120; err = 0.32851562 * 5120; time = 0.0316s; samplesPerSecond = 162071.5
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.11498146 * 5120; err = 0.33867188 * 5120; time = 0.0305s; samplesPerSecond = 167791.8
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.09012623 * 5120; err = 0.34101562 * 5120; time = 0.0306s; samplesPerSecond = 167429.7
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.15339394 * 5120; err = 0.36074219 * 5120; time = 0.0307s; samplesPerSecond = 166715.5
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.08357811 * 5120; err = 0.33886719 * 5120; time = 0.0307s; samplesPerSecond = 166514.9
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07425880 * 5120; err = 0.33261719 * 5120; time = 0.0306s; samplesPerSecond = 167402.3
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08439636 * 5120; err = 0.33125000 * 5120; time = 0.0307s; samplesPerSecond = 166693.8
07/12/2016 08:01:58:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.06031189 * 5120; err = 0.31562500 * 5120; time = 0.0307s; samplesPerSecond = 166867.6
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09160309 * 5120; err = 0.33808594 * 5120; time = 0.0306s; samplesPerSecond = 167221.9
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.10127335 * 5120; err = 0.34179688 * 5120; time = 0.0307s; samplesPerSecond = 167003.7
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.07826691 * 5120; err = 0.33554688 * 5120; time = 0.0307s; samplesPerSecond = 167003.7
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06313019 * 5120; err = 0.34121094 * 5120; time = 0.0306s; samplesPerSecond = 167238.3
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.03379974 * 5120; err = 0.31933594 * 5120; time = 0.0307s; samplesPerSecond = 166824.2
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.01062927 * 5120; err = 0.32304688 * 5120; time = 0.0307s; samplesPerSecond = 166932.9
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04116974 * 5120; err = 0.31757812 * 5120; time = 0.0306s; samplesPerSecond = 167183.7
07/12/2016 08:01:59:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.05690460 * 5120; err = 0.32128906 * 5120; time = 0.0307s; samplesPerSecond = 166873.1
07/12/2016 08:01:59: Finished Epoch[ 3 of 4]: [Training] ce = 1.07515182 * 81920; err = 0.33282471 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.49434s
07/12/2016 08:01:59: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.3'

07/12/2016 08:01:59: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

07/12/2016 08:01:59: Starting minibatch loop.
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.01812296 * 5120; err = 0.31796875 * 5120; time = 0.0317s; samplesPerSecond = 161565.2
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.01859186 * 4926; err = 0.31628096 * 4926; time = 0.0892s; samplesPerSecond = 55247.8
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.03707047 * 5120; err = 0.32382813 * 5120; time = 0.0307s; samplesPerSecond = 166520.3
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 0.99918213 * 5120; err = 0.31425781 * 5120; time = 0.0307s; samplesPerSecond = 166949.3
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.02076111 * 5120; err = 0.31972656 * 5120; time = 0.0307s; samplesPerSecond = 167036.4
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00163498 * 5120; err = 0.31171875 * 5120; time = 0.0308s; samplesPerSecond = 166466.2
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.01088753 * 5120; err = 0.31269531 * 5120; time = 0.0307s; samplesPerSecond = 166976.5
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 0.99696808 * 5120; err = 0.30644531 * 5120; time = 0.0306s; samplesPerSecond = 167183.7
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99020691 * 5120; err = 0.30585937 * 5120; time = 0.0307s; samplesPerSecond = 166911.2
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.99083939 * 5120; err = 0.31230469 * 5120; time = 0.0307s; samplesPerSecond = 166943.8
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.00112152 * 5120; err = 0.31289062 * 5120; time = 0.0308s; samplesPerSecond = 166201.4
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.98365173 * 5120; err = 0.30937500 * 5120; time = 0.0307s; samplesPerSecond = 167020.1
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.98795090 * 5120; err = 0.31054688 * 5120; time = 0.0307s; samplesPerSecond = 166748.1
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.98707275 * 5120; err = 0.30605469 * 5120; time = 0.0307s; samplesPerSecond = 166579.9
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.01551819 * 5120; err = 0.31933594 * 5120; time = 0.0307s; samplesPerSecond = 166531.1
07/12/2016 08:01:59:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.00542603 * 5120; err = 0.31074219 * 5120; time = 0.0306s; samplesPerSecond = 167314.8
07/12/2016 08:01:59: Finished Epoch[ 4 of 4]: [Training] ce = 1.00391188 * 81920; err = 0.31309814 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.555845s
07/12/2016 08:01:59: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech'
07/12/2016 08:02:00: CNTKCommandTrainEnd: speechTrain

07/12/2016 08:02:00: Action "train" complete.


07/12/2016 08:02:00: ##############################################################################
07/12/2016 08:02:00: #                                                                            #
07/12/2016 08:02:00: # Action "edit"                                                              #
07/12/2016 08:02:00: #                                                                            #
07/12/2016 08:02:00: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 08:02:00: Action "edit" complete.


07/12/2016 08:02:00: ##############################################################################
07/12/2016 08:02:00: #                                                                            #
07/12/2016 08:02:00: # Action "train"                                                             #
07/12/2016 08:02:00: #                                                                            #
07/12/2016 08:02:00: ##############################################################################

07/12/2016 08:02:00: CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu\TestData\CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames

07/12/2016 08:02:00: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0'.

Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *9]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *9]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *9], [363 x 1], [363 x 1] -> [363 x *9]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *9] -> [512 x *9]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *9] -> [132 x 1 x *9]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *9], [132 x 1 x *9], [132 x 1 x *9] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *9], [132 x 1 x *9] -> [1]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 08:02:00: Loaded model with 29 nodes on GPU 0.

07/12/2016 08:02:00: Training criterion node(s):
07/12/2016 08:02:00: 	ce = SequenceWithSoftmax

07/12/2016 08:02:00: Evaluation criterion node(s):

07/12/2016 08:02:00: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *9]] [features Gradient[363 x *9]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *9]] [logPrior Gradient[132 x 1]] }
000000D49846DFB0: {[HL2.b Value[512 x 1]] }
000000D4984EF5E0: {[HL1.W Value[512 x 363]] }
000000D4984F01C0: {[HL1.b Value[512 x 1]] }
000000D4984F0580: {[globalPrior Value[132 x 1]] }
000000D4A6E30960: {[globalMean Value[363 x 1]] }
000000D4A6E319A0: {[globalInvStd Value[363 x 1]] }
000000D4A6E323A0: {[features Value[363 x *9]] }
000000D4A9F20AF0: {[HL2.W Value[512 x 512]] }
000000D4AA37B0B0: {[ce Gradient[1]] }
000000D4AA37B1F0: {[scaledLogLikelihood Value[132 x 1 x *9]] }
000000D4AA37B6F0: {[err Value[1]] }
000000D4AA37B790: {[featNorm Value[363 x *9]] }
000000D4AA37B830: {[HL1.t Value[512 x *9]] }
000000D4AA37B8D0: {[HL1.t Gradient[512 x *9]] [HL1.y Value[512 x 1 x *9]] }
000000D4AA37B970: {[HL1.z Gradient[512 x 1 x *9]] [HL2.t Value[512 x 1 x *9]] }
000000D4AA37BA10: {[HL2.t Gradient[512 x 1 x *9]] [HL2.y Value[512 x 1 x *9]] }
000000D4AA37BAB0: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *9]] }
000000D4AA37BB50: {[logPrior Value[132 x 1]] }
000000D4AA37BBF0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *9]] }
000000D4AA37BD30: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *9]] [HL2.z Gradient[512 x 1 x *9]] [HL3.t Value[512 x 1 x *9]] }
000000D4AA37BE70: {[HL3.t Gradient[512 x 1 x *9]] [HL3.y Value[512 x 1 x *9]] }
000000D4AA37BF10: {[OL.b Value[132 x 1]] }
000000D4AA37C050: {[HL3.b Value[512 x 1]] }
000000D4AA37C0F0: {[ce Value[1]] }
000000D4AA37C230: {[OL.t Gradient[132 x 1 x *9]] [scaledLogLikelihood Gradient[132 x 1 x *9]] }
000000D4AA37C410: {[OL.b Gradient[132 x 1]] }
000000D4AA37C4B0: {[OL.W Value[132 x 512]] }
000000D4AA37C550: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *9]] [OL.z Gradient[132 x 1 x *9]] }
000000D4AA37C5F0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *9]] }
000000D4AA37C910: {[labels Value[132 x *9]] }
000000D4AA37CC30: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *9]] [HL3.z Gradient[512 x 1 x *9]] [OL.t Value[132 x 1 x *9]] }
000000D4AA37CE10: {[HL3.W Value[512 x 512]] }
000000D4AA37CF50: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *9]] }

07/12/2016 08:02:00: No PreCompute nodes found, skipping PreCompute step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-010
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

07/12/2016 08:02:00: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 08:02:07: Starting minibatch loop.
dengamma value 1.032757
dengamma value 0.969564
dengamma value 1.095462
dengamma value 1.045726
dengamma value 0.995933
dengamma value 1.077326
dengamma value 1.075679
dengamma value 1.016999
dengamma value 1.102098
dengamma value 1.012491
dengamma value 1.055656
dengamma value 1.034292
dengamma value 1.062859
dengamma value 0.986126
dengamma value 1.060935
dengamma value 1.009354
dengamma value 1.020526
dengamma value 1.030794
dengamma value 1.101264
dengamma value 1.045249
dengamma value 1.031667
dengamma value 1.057599
dengamma value 1.079425
07/12/2016 08:02:10:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.09127786 * 5904; err = 0.29302168 * 5904; time = 2.9055s; samplesPerSecond = 2032.0
dengamma value 0.996880
dengamma value 0.990551
dengamma value 1.091168
dengamma value 0.956241
dengamma value 1.038895
dengamma value 1.018873
dengamma value 0.995910
dengamma value 1.065590
dengamma value 1.038012
dengamma value 0.948791
dengamma value 1.026831
dengamma value 1.034887
dengamma value 1.026512
dengamma value 1.017196
dengamma value 1.094674
dengamma value 1.051813
dengamma value 1.069720
dengamma value 1.009909
dengamma value 1.000268
dengamma value 1.037221
07/12/2016 08:02:10:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.09297078 * 4860; err = 0.32242798 * 4860; time = 0.4195s; samplesPerSecond = 11586.5
dengamma value 1.057811
dengamma value 1.001036
dengamma value 1.114598
dengamma value 1.095112
dengamma value 1.053662
dengamma value 1.016394
dengamma value 1.006225
dengamma value 1.008509
dengamma value 1.109304
dengamma value 1.121039
dengamma value 1.071181
dengamma value 1.033525
dengamma value 1.014969
dengamma value 1.097760
dengamma value 1.058187
dengamma value 1.027374
dengamma value 1.032531
dengamma value 0.983894
dengamma value 1.105221
dengamma value 1.044745
dengamma value 1.049584
07/12/2016 08:02:11:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08435615 * 5868; err = 0.27062031 * 5868; time = 0.5436s; samplesPerSecond = 10795.2
dengamma value 1.051495
dengamma value 1.039902
dengamma value 1.047735
dengamma value 1.050429
dengamma value 1.005013
dengamma value 0.997931
dengamma value 0.956432
dengamma value 1.063984
dengamma value 1.013892
dengamma value 0.990853
dengamma value 1.049555
dengamma value 1.016966
dengamma value 1.033954
dengamma value 1.102519
dengamma value 1.011016
dengamma value 1.021572
dengamma value 1.085127
dengamma value 1.017757
dengamma value 1.003999
dengamma value 1.083824
dengamma value 1.030673
dengamma value 0.998829
07/12/2016 08:02:11:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08852146 * 4886; err = 0.30270160 * 4886; time = 0.4419s; samplesPerSecond = 11056.2
dengamma value 1.029679
dengamma value 1.021344
dengamma value 1.017159
dengamma value 0.998394
dengamma value 0.937212
dengamma value 1.146538
dengamma value 1.122054
dengamma value 1.088093
dengamma value 1.110607
dengamma value 1.028919
dengamma value 0.987975
dengamma value 1.121780
dengamma value 1.045983
dengamma value 1.074298
dengamma value 1.071327
dengamma value 1.033457
dengamma value 1.077761
dengamma value 0.925689
dengamma value 1.033129
dengamma value 1.018946
dengamma value 1.044335
dengamma value 1.101558
07/12/2016 08:02:12:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08991380 * 6486; err = 0.28908418 * 6486; time = 0.6012s; samplesPerSecond = 10788.9
dengamma value 1.000100
dengamma value 1.098516
dengamma value 1.048181
dengamma value 0.990050
dengamma value 1.051186
dengamma value 1.026527
dengamma value 0.983310
dengamma value 1.034247
dengamma value 1.000119
dengamma value 1.048330
dengamma value 0.972974
dengamma value 1.034854
dengamma value 1.004659
dengamma value 0.933993
dengamma value 1.039449
dengamma value 1.064415
dengamma value 0.976893
dengamma value 1.069436
dengamma value 1.090913
dengamma value 0.975718
dengamma value 1.100505
dengamma value 1.084852
dengamma value 1.027068
07/12/2016 08:02:12:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08925104 * 6464; err = 0.31714109 * 6464; time = 0.6013s; samplesPerSecond = 10750.0
dengamma value 1.063572
dengamma value 1.076298
dengamma value 0.971615
dengamma value 1.003895
dengamma value 1.062239
dengamma value 0.985909
dengamma value 1.060892
dengamma value 1.054168
dengamma value 1.084979
dengamma value 1.022775
dengamma value 1.054168
dengamma value 1.026212
dengamma value 1.048776
dengamma value 0.979147
dengamma value 1.011756
dengamma value 1.053237
dengamma value 1.060982
dengamma value 0.950305
dengamma value 1.056252
dengamma value 1.015271
dengamma value 0.965816
dengamma value 0.970137
dengamma value 1.098502
dengamma value 1.033751
07/12/2016 08:02:13:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08745918 * 6512; err = 0.30758600 * 6512; time = 0.5319s; samplesPerSecond = 12243.1
dengamma value 1.075582
dengamma value 0.997494
dengamma value 1.041740
dengamma value 1.051205
dengamma value 1.053046
dengamma value 1.005341
dengamma value 1.094028
dengamma value 1.031640
dengamma value 0.994981
dengamma value 1.061954
dengamma value 1.062553
dengamma value 1.079142
dengamma value 1.009333
dengamma value 0.959813
dengamma value 1.060115
dengamma value 1.065790
dengamma value 1.042933
dengamma value 1.069803
dengamma value 1.019871
dengamma value 1.127004
dengamma value 1.012768
07/12/2016 08:02:13:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.07932757 * 4968; err = 0.30253623 * 4968; time = 0.4908s; samplesPerSecond = 10121.9
dengamma value 0.993594
dengamma value 1.044949
dengamma value 1.045006
dengamma value 0.975136
dengamma value 1.091496
dengamma value 1.050147
dengamma value 1.061647
dengamma value 0.987752
dengamma value 0.998889
dengamma value 1.038222
dengamma value 1.073109
dengamma value 1.064864
dengamma value 1.024277
dengamma value 0.976175
dengamma value 1.055916
dengamma value 1.004048
dengamma value 1.059030
dengamma value 1.020209
dengamma value 0.979078
dengamma value 1.016194
dengamma value 1.041636
dengamma value 1.091996
dengamma value 1.062342
07/12/2016 08:02:14:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08577759 * 6484; err = 0.30166564 * 6484; time = 0.5633s; samplesPerSecond = 11511.4
dengamma value 1.121249
dengamma value 1.095207
dengamma value 0.924988
dengamma value 1.063007
dengamma value 1.078224
dengamma value 1.102692
dengamma value 1.046764
dengamma value 1.051471
dengamma value 0.973795
dengamma value 1.020476
dengamma value 0.990614
dengamma value 1.041219
dengamma value 1.103784
dengamma value 1.040862
dengamma value 1.044891
dengamma value 1.080505
dengamma value 1.024021
dengamma value 1.010473
dengamma value 1.026120
dengamma value 1.076359
dengamma value 1.099317
dengamma value 1.049512
dengamma value 1.042623
07/12/2016 08:02:14:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08589192 * 6374; err = 0.28820207 * 6374; time = 0.5849s; samplesPerSecond = 10897.9
dengamma value 1.089320
dengamma value 1.052790
dengamma value 1.039730
dengamma value 1.063531
dengamma value 1.043637
dengamma value 1.078351
dengamma value 1.030331
dengamma value 1.058814
dengamma value 1.056679
dengamma value 1.034396
dengamma value 1.015243
dengamma value 1.016887
dengamma value 1.020492
dengamma value 1.075720
dengamma value 1.053599
dengamma value 1.026635
dengamma value 1.048582
dengamma value 1.059595
dengamma value 1.051678
dengamma value 1.017593
dengamma value 1.037351
dengamma value 1.107821
dengamma value 1.003340
dengamma value 1.029750
07/12/2016 08:02:15:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08320636 * 6222; err = 0.31163613 * 6222; time = 0.5633s; samplesPerSecond = 11046.3
dengamma value 1.046968
dengamma value 1.019488
dengamma value 1.033094
dengamma value 1.066730
dengamma value 1.025725
dengamma value 0.910887
dengamma value 1.052629
dengamma value 1.087176
dengamma value 0.962368
dengamma value 1.116144
dengamma value 1.060437
dengamma value 1.044022
dengamma value 1.063341
dengamma value 1.061452
dengamma value 1.069051
dengamma value 0.980683
dengamma value 0.945075
dengamma value 1.084478
dengamma value 1.095283
dengamma value 1.066504
dengamma value 1.052151
dengamma value 1.052414
dengamma value 1.045536
dengamma value 1.107138
dengamma value 1.087823
dengamma value 1.046197
07/12/2016 08:02:16:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.07795667 * 7078; err = 0.29810681 * 7078; time = 0.6450s; samplesPerSecond = 10974.5
dengamma value 1.045500
dengamma value 1.108069
dengamma value 1.079850
dengamma value 1.073939
dengamma value 1.052683
dengamma value 1.036732
dengamma value 1.020200
dengamma value 1.043880
dengamma value 1.049205
dengamma value 1.016891
dengamma value 0.995483
dengamma value 1.055080
dengamma value 0.954227
dengamma value 1.041134
dengamma value 1.073659
dengamma value 1.071128
dengamma value 1.035917
dengamma value 1.047197
dengamma value 1.000965
dengamma value 1.017965
dengamma value 0.994344
dengamma value 1.032766
dengamma value 0.984951
dengamma value 1.024499
dengamma value 0.966444
07/12/2016 08:02:16:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08228781 * 6920; err = 0.31791908 * 6920; time = 0.6623s; samplesPerSecond = 10448.5
dengamma value 1.028942
dengamma value 1.100430
dengamma value 1.093422
dengamma value 1.055680
dengamma value 1.051398
dengamma value 0.942298
dengamma value 0.989154
dengamma value 1.077445
dengamma value 0.966613
dengamma value 1.141193
dengamma value 1.019959
dengamma value 1.001337
dengamma value 0.974706
07/12/2016 08:02:16: Finished Epoch[ 1 of 3]: [Training] ce = 0.08608338 * 82160; err = 0.30178919 * 82160; totalSamplesSeen = 82160; learningRatePerSample = 2e-006; epochTime=16.2491s
07/12/2016 08:02:17: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.1'

07/12/2016 08:02:17: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 82160), data subset 0 of 1, with 1 datapasses

07/12/2016 08:02:17: Starting minibatch loop.
dengamma value 1.038977
dengamma value 0.955309
dengamma value 1.052395
dengamma value 1.047000
dengamma value 1.044114
dengamma value 1.063036
dengamma value 1.099938
dengamma value 1.013486
dengamma value 1.004546
dengamma value 0.901192
dengamma value 1.041154
dengamma value 1.025397
dengamma value 0.948774
dengamma value 1.072856
dengamma value 1.128228
dengamma value 1.062745
dengamma value 1.053524
dengamma value 1.064913
dengamma value 1.056262
dengamma value 0.995531
dengamma value 0.986953
dengamma value 1.086228
07/12/2016 08:02:17:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.09458307 * 5756; err = 0.30385685 * 5756; time = 0.5152s; samplesPerSecond = 11171.7
dengamma value 1.051725
dengamma value 1.068501
dengamma value 0.969997
dengamma value 1.062443
dengamma value 1.184891
dengamma value 1.060039
dengamma value 1.071976
dengamma value 0.911944
dengamma value 1.015929
dengamma value 1.046494
dengamma value 1.060795
dengamma value 1.053874
dengamma value 1.023275
dengamma value 1.072470
dengamma value 0.994988
dengamma value 1.026603
dengamma value 1.056927
dengamma value 1.092476
dengamma value 1.049189
dengamma value 1.071695
dengamma value 1.035243
07/12/2016 08:02:18:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07550587 * 6398; err = 0.28884026 * 6398; time = 0.5296s; samplesPerSecond = 12081.4
dengamma value 1.056381
dengamma value 1.069480
dengamma value 1.028832
dengamma value 1.041987
dengamma value 1.039472
dengamma value 1.032414
dengamma value 1.088058
dengamma value 1.082833
dengamma value 1.032317
dengamma value 0.999326
dengamma value 1.028671
dengamma value 1.025340
dengamma value 1.031220
dengamma value 1.051678
dengamma value 1.098609
dengamma value 1.032294
dengamma value 1.052345
dengamma value 0.987748
dengamma value 1.035881
dengamma value 1.050074
dengamma value 1.038777
07/12/2016 08:02:18:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08779391 * 5248; err = 0.28258384 * 5248; time = 0.4620s; samplesPerSecond = 11359.3
dengamma value 1.071300
dengamma value 1.029675
dengamma value 1.069238
dengamma value 1.050698
dengamma value 1.056260
dengamma value 0.968047
dengamma value 1.063582
dengamma value 1.026445
dengamma value 1.041717
dengamma value 1.042052
dengamma value 1.054866
dengamma value 1.038842
dengamma value 0.970796
dengamma value 1.091280
dengamma value 1.107875
dengamma value 1.012356
dengamma value 1.141864
dengamma value 1.121787
dengamma value 1.002272
dengamma value 1.115100
dengamma value 0.995934
dengamma value 1.003363
dengamma value 1.027420
07/12/2016 08:02:19:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08279916 * 6174; err = 0.30174927 * 6174; time = 0.5189s; samplesPerSecond = 11898.1
dengamma value 1.043595
dengamma value 1.079361
dengamma value 1.105853
dengamma value 1.005652
dengamma value 1.019529
dengamma value 0.963082
dengamma value 0.997493
dengamma value 1.072859
dengamma value 0.948508
dengamma value 1.019090
dengamma value 1.017737
dengamma value 1.036704
dengamma value 1.042492
dengamma value 1.051911
dengamma value 1.077406
dengamma value 1.055331
dengamma value 1.008360
dengamma value 1.039189
dengamma value 1.083980
dengamma value 1.060021
dengamma value 1.014385
dengamma value 1.052154
07/12/2016 08:02:19:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.09362535 * 5866; err = 0.29219229 * 5866; time = 0.5459s; samplesPerSecond = 10746.5
dengamma value 1.021860
dengamma value 1.076885
dengamma value 1.008993
dengamma value 1.048635
dengamma value 1.003624
dengamma value 1.016742
dengamma value 1.081466
dengamma value 1.007631
dengamma value 1.066239
dengamma value 1.008822
dengamma value 1.122019
dengamma value 0.978203
dengamma value 1.050105
dengamma value 1.019045
dengamma value 1.069676
dengamma value 1.050993
dengamma value 1.024277
dengamma value 1.055200
dengamma value 0.979907
dengamma value 0.965100
07/12/2016 08:02:20:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08650434 * 5100; err = 0.30137255 * 5100; time = 0.4396s; samplesPerSecond = 11601.0
dengamma value 1.002112
dengamma value 1.030445
dengamma value 1.044825
dengamma value 1.053365
dengamma value 1.022302
dengamma value 1.040602
dengamma value 1.018151
dengamma value 1.098738
dengamma value 1.094300
dengamma value 1.057624
dengamma value 1.083697
dengamma value 1.059052
dengamma value 0.995483
dengamma value 1.030398
dengamma value 1.044387
dengamma value 1.035921
dengamma value 1.083343
dengamma value 1.048819
dengamma value 0.985316
dengamma value 1.111107
dengamma value 1.007180
07/12/2016 08:02:20:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08432659 * 6278; err = 0.29499841 * 6278; time = 0.6005s; samplesPerSecond = 10453.8
dengamma value 1.035433
dengamma value 1.103623
dengamma value 0.986593
dengamma value 1.064821
dengamma value 1.048515
dengamma value 1.002350
dengamma value 1.087225
dengamma value 1.043096
dengamma value 1.016547
dengamma value 1.123803
dengamma value 1.027731
dengamma value 1.063765
dengamma value 1.111649
dengamma value 1.030935
dengamma value 1.062185
dengamma value 1.074971
dengamma value 1.078124
dengamma value 1.020445
dengamma value 0.932590
dengamma value 1.036149
dengamma value 0.977805
dengamma value 1.048724
dengamma value 1.019881
dengamma value 1.084413
07/12/2016 08:02:21:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08294712 * 7772; err = 0.29825013 * 7772; time = 0.6784s; samplesPerSecond = 11456.9
dengamma value 1.036872
dengamma value 1.102347
dengamma value 1.031075
dengamma value 1.036024
dengamma value 1.020256
dengamma value 0.998249
dengamma value 0.995896
dengamma value 0.961422
dengamma value 1.078785
dengamma value 0.981762
dengamma value 1.114542
dengamma value 1.082346
dengamma value 1.067436
dengamma value 1.051913
dengamma value 0.998062
dengamma value 1.132654
dengamma value 1.095539
dengamma value 0.993237
dengamma value 1.072755
dengamma value 0.982407
dengamma value 1.016236
dengamma value 1.052810
dengamma value 1.040789
dengamma value 1.094791
dengamma value 1.038224
dengamma value 1.056322
07/12/2016 08:02:21:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08159326 * 6758; err = 0.29994081 * 6758; time = 0.5770s; samplesPerSecond = 11711.9
dengamma value 1.027399
dengamma value 1.050258
dengamma value 1.001207
dengamma value 1.054872
dengamma value 0.993972
dengamma value 1.073581
dengamma value 1.096813
dengamma value 1.059028
dengamma value 1.015024
dengamma value 0.958496
dengamma value 1.091761
dengamma value 1.006456
dengamma value 1.055898
dengamma value 1.061855
dengamma value 1.051611
dengamma value 1.121766
dengamma value 1.081330
dengamma value 1.053599
dengamma value 1.012336
dengamma value 1.059894
dengamma value 1.010308
dengamma value 1.073819
dengamma value 1.043635
dengamma value 1.032747
dengamma value 0.954952
07/12/2016 08:02:22:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.09133702 * 6700; err = 0.30850746 * 6700; time = 0.6035s; samplesPerSecond = 11101.3
dengamma value 1.007187
dengamma value 1.048099
dengamma value 0.930178
dengamma value 1.025709
dengamma value 1.053441
dengamma value 1.024403
dengamma value 1.125684
dengamma value 1.061082
dengamma value 1.077925
dengamma value 1.052459
dengamma value 0.993349
dengamma value 1.042252
dengamma value 1.057917
dengamma value 1.005396
dengamma value 1.029282
dengamma value 1.029266
dengamma value 1.062175
dengamma value 0.995947
dengamma value 1.067781
dengamma value 0.929909
dengamma value 1.002620
dengamma value 0.979064
dengamma value 1.099216
dengamma value 1.038565
07/12/2016 08:02:23:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.07938058 * 6692; err = 0.31201435 * 6692; time = 0.5891s; samplesPerSecond = 11360.6
dengamma value 1.020250
dengamma value 0.987441
dengamma value 1.000353
dengamma value 1.081775
dengamma value 1.036517
dengamma value 0.991133
dengamma value 1.002067
dengamma value 1.090808
dengamma value 1.034514
dengamma value 1.053404
dengamma value 0.997444
dengamma value 0.988904
dengamma value 1.041584
dengamma value 0.983481
dengamma value 1.010557
dengamma value 1.023170
dengamma value 0.958934
dengamma value 1.035631
dengamma value 1.088630
dengamma value 1.060691
dengamma value 1.146815
07/12/2016 08:02:23:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.09070166 * 5148; err = 0.31274281 * 5148; time = 0.4476s; samplesPerSecond = 11501.3
dengamma value 1.041669
dengamma value 1.056703
dengamma value 1.002357
dengamma value 1.099220
dengamma value 1.038379
dengamma value 1.046903
dengamma value 1.006626
dengamma value 1.010069
dengamma value 1.062992
dengamma value 0.994666
dengamma value 1.053525
dengamma value 1.028191
dengamma value 1.016947
dengamma value 1.021962
dengamma value 1.083623
dengamma value 1.041984
dengamma value 0.982647
dengamma value 1.053102
dengamma value 0.926460
dengamma value 1.035334
07/12/2016 08:02:24:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08722579 * 6040; err = 0.29768212 * 6040; time = 0.5593s; samplesPerSecond = 10798.3
dengamma value 0.963341
dengamma value 1.010833
dengamma value 1.044226
dengamma value 1.077477
dengamma value 1.083187
dengamma value 1.067264
dengamma value 1.057676
dengamma value 1.009121
07/12/2016 08:02:24: Finished Epoch[ 2 of 3]: [Training] ce = 0.08590803 * 81664; err = 0.29996082 * 81664; totalSamplesSeen = 163824; learningRatePerSample = 2e-006; epochTime=7.24932s
07/12/2016 08:02:24: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.2'

07/12/2016 08:02:24: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 164222), data subset 0 of 1, with 1 datapasses

07/12/2016 08:02:24: Starting minibatch loop.
dengamma value 1.054830
dengamma value 0.982479
dengamma value 1.002375
dengamma value 1.006926
dengamma value 1.004771
dengamma value 1.004634
dengamma value 1.027498
dengamma value 1.010226
dengamma value 1.043713
dengamma value 1.030543
dengamma value 1.083837
dengamma value 1.101742
dengamma value 1.091468
dengamma value 1.003284
dengamma value 0.984936
dengamma value 1.122028
dengamma value 1.026825
dengamma value 1.077749
dengamma value 1.030212
dengamma value 1.058537
dengamma value 1.051148
07/12/2016 08:02:25:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08821364 * 6038; err = 0.30755217 * 6038; time = 0.5736s; samplesPerSecond = 10526.1
dengamma value 1.067049
dengamma value 0.992416
dengamma value 1.051396
dengamma value 1.075893
dengamma value 1.102064
dengamma value 1.077958
dengamma value 1.055143
dengamma value 0.993268
dengamma value 0.999481
dengamma value 0.995893
dengamma value 1.025764
dengamma value 1.070869
dengamma value 1.024860
dengamma value 1.060110
dengamma value 1.008985
dengamma value 1.034168
dengamma value 0.993508
dengamma value 1.065643
dengamma value 1.011065
dengamma value 1.013269
dengamma value 1.021509
dengamma value 0.988403
dengamma value 1.043902
07/12/2016 08:02:25:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08883611 * 5874; err = 0.32890705 * 5874; time = 0.5082s; samplesPerSecond = 11557.4
dengamma value 1.019610
dengamma value 1.045180
dengamma value 1.033207
dengamma value 1.087364
dengamma value 1.037629
dengamma value 1.018905
dengamma value 1.006687
dengamma value 1.059461
dengamma value 1.038656
dengamma value 1.086555
dengamma value 0.995166
dengamma value 1.004893
dengamma value 1.025017
dengamma value 1.059532
dengamma value 1.027417
dengamma value 1.122324
dengamma value 1.075692
dengamma value 1.117877
dengamma value 0.986102
dengamma value 1.026405
dengamma value 1.095148
dengamma value 1.072743
07/12/2016 08:02:26:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.09078563 * 5856; err = 0.28005464 * 5856; time = 0.5266s; samplesPerSecond = 11120.5
dengamma value 1.005002
dengamma value 1.048406
dengamma value 0.996463
dengamma value 1.038539
dengamma value 1.147987
dengamma value 1.010833
dengamma value 0.993504
dengamma value 1.020961
dengamma value 0.974813
dengamma value 1.001111
dengamma value 0.937261
dengamma value 1.081090
dengamma value 0.978616
dengamma value 1.015254
dengamma value 1.109887
dengamma value 0.956348
dengamma value 1.052341
dengamma value 1.074278
dengamma value 1.046274
dengamma value 1.019824
dengamma value 1.039938
dengamma value 0.973126
07/12/2016 08:02:26:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.09287186 * 5256; err = 0.33295282 * 5256; time = 0.4396s; samplesPerSecond = 11956.4
dengamma value 1.051607
dengamma value 1.002023
dengamma value 1.066030
dengamma value 1.042836
dengamma value 0.939697
dengamma value 1.064248
dengamma value 1.023119
dengamma value 1.093417
dengamma value 1.042444
dengamma value 1.039609
dengamma value 1.088369
dengamma value 1.041511
dengamma value 0.997847
dengamma value 1.043417
dengamma value 1.072973
dengamma value 1.109007
dengamma value 1.056995
dengamma value 1.027009
dengamma value 1.101565
dengamma value 1.021603
dengamma value 1.022815
dengamma value 1.028554
dengamma value 1.072729
dengamma value 1.044762
dengamma value 1.047955
07/12/2016 08:02:27:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08832000 * 6180; err = 0.27831715 * 6180; time = 0.5784s; samplesPerSecond = 10685.0
dengamma value 0.995403
dengamma value 1.068192
dengamma value 1.049503
dengamma value 1.081532
dengamma value 1.012297
dengamma value 1.058930
dengamma value 1.060300
dengamma value 1.018904
dengamma value 1.069975
dengamma value 0.997851
dengamma value 1.032932
dengamma value 1.051829
dengamma value 1.095280
dengamma value 1.040843
dengamma value 1.001090
dengamma value 1.089938
dengamma value 1.144979
dengamma value 1.067041
dengamma value 1.056826
dengamma value 0.995889
dengamma value 1.034889
07/12/2016 08:02:27:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08065102 * 7358; err = 0.27833650 * 7358; time = 0.6433s; samplesPerSecond = 11437.2
dengamma value 1.050928
dengamma value 1.054587
dengamma value 1.083323
dengamma value 0.936593
dengamma value 1.032110
dengamma value 1.057219
dengamma value 1.019532
dengamma value 1.192952
dengamma value 1.052553
dengamma value 1.022539
dengamma value 1.029076
dengamma value 0.958965
dengamma value 1.053659
dengamma value 1.063854
dengamma value 1.036818
dengamma value 1.040544
dengamma value 1.065868
dengamma value 0.997319
dengamma value 1.120635
dengamma value 1.039206
dengamma value 1.085215
dengamma value 1.011713
07/12/2016 08:02:28:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08567907 * 6886; err = 0.29029916 * 6886; time = 0.6165s; samplesPerSecond = 11168.7
dengamma value 1.021362
dengamma value 1.047728
dengamma value 1.107545
dengamma value 1.032173
dengamma value 1.043145
dengamma value 1.025307
dengamma value 1.017580
dengamma value 1.059321
dengamma value 1.057958
dengamma value 1.088346
dengamma value 1.047492
dengamma value 1.051720
dengamma value 1.075923
dengamma value 1.009999
dengamma value 1.034628
dengamma value 1.081823
dengamma value 0.965303
dengamma value 1.021928
dengamma value 1.068017
dengamma value 1.074743
dengamma value 1.041421
07/12/2016 08:02:28:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08389184 * 5778; err = 0.32052613 * 5778; time = 0.5064s; samplesPerSecond = 11410.4
dengamma value 1.055806
dengamma value 1.084096
dengamma value 1.038115
dengamma value 1.047923
dengamma value 1.016194
dengamma value 0.990865
dengamma value 1.048808
dengamma value 1.084986
dengamma value 1.063697
dengamma value 1.076565
dengamma value 1.063097
dengamma value 1.019231
dengamma value 1.023694
dengamma value 1.038758
dengamma value 1.069080
dengamma value 1.097350
dengamma value 1.011641
dengamma value 1.065651
dengamma value 0.994761
dengamma value 1.010419
dengamma value 1.038314
07/12/2016 08:02:29:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08590247 * 6398; err = 0.31587996 * 6398; time = 0.5760s; samplesPerSecond = 11107.1
dengamma value 0.991319
dengamma value 1.137628
dengamma value 1.040504
dengamma value 0.997557
dengamma value 0.929965
dengamma value 1.002328
dengamma value 1.073986
dengamma value 1.212344
dengamma value 1.017958
dengamma value 1.098274
dengamma value 0.941764
dengamma value 0.995945
dengamma value 1.064018
dengamma value 1.053116
dengamma value 1.086414
dengamma value 1.055985
dengamma value 1.062819
dengamma value 0.945747
dengamma value 0.964780
dengamma value 1.096712
dengamma value 1.130161
07/12/2016 08:02:29:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08531621 * 5848; err = 0.31566347 * 5848; time = 0.5010s; samplesPerSecond = 11671.6
dengamma value 1.030384
dengamma value 0.977986
dengamma value 0.996124
dengamma value 1.086399
dengamma value 1.060937
dengamma value 0.991019
dengamma value 1.073108
dengamma value 0.999583
dengamma value 1.106756
dengamma value 0.962798
dengamma value 0.971494
dengamma value 1.048886
dengamma value 1.015328
dengamma value 1.067368
dengamma value 1.082199
dengamma value 1.064469
dengamma value 1.033065
dengamma value 1.012326
dengamma value 0.992427
dengamma value 1.041236
dengamma value 1.044309
dengamma value 0.999669
dengamma value 1.108056
dengamma value 1.017250
07/12/2016 08:02:30:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.09044150 * 6802; err = 0.30446927 * 6802; time = 0.5953s; samplesPerSecond = 11425.5
dengamma value 1.020736
dengamma value 1.028784
dengamma value 1.051253
dengamma value 1.098283
dengamma value 1.043896
dengamma value 1.027868
dengamma value 1.025118
dengamma value 1.091845
dengamma value 1.019019
dengamma value 1.062078
dengamma value 1.064697
dengamma value 1.019974
dengamma value 1.105154
dengamma value 1.066238
dengamma value 1.130299
dengamma value 1.079670
dengamma value 0.999562
dengamma value 1.088603
dengamma value 0.966009
dengamma value 1.155284
dengamma value 0.989459
dengamma value 1.020619
dengamma value 1.038428
dengamma value 1.148745
dengamma value 0.869876
dengamma value 1.053811
07/12/2016 08:02:31:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08594345 * 6888; err = 0.30313589 * 6888; time = 0.6510s; samplesPerSecond = 10580.5
dengamma value 1.041644
dengamma value 1.008944
dengamma value 1.056632
dengamma value 1.075810
dengamma value 1.094456
dengamma value 1.096199
dengamma value 1.070280
dengamma value 0.992962
dengamma value 0.985305
dengamma value 1.051816
dengamma value 1.034652
dengamma value 1.054677
dengamma value 0.989389
dengamma value 1.131411
dengamma value 0.972101
dengamma value 1.058538
dengamma value 1.037449
dengamma value 1.085097
dengamma value 0.972569
dengamma value 1.078072
dengamma value 1.055756
07/12/2016 08:02:31:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08382054 * 5168; err = 0.29218266 * 5168; time = 0.4974s; samplesPerSecond = 10389.8
dengamma value 0.976641
dengamma value 1.068466
dengamma value 1.037475
dengamma value 1.079175
dengamma value 1.053696
dengamma value 1.079171
07/12/2016 08:02:31: Finished Epoch[ 3 of 3]: [Training] ce = 0.08698309 * 82008; err = 0.30271437 * 82008; totalSamplesSeen = 245832; learningRatePerSample = 2e-006; epochTime=7.38014s
07/12/2016 08:02:31: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712075937.954252\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence'
07/12/2016 08:02:32: CNTKCommandTrainEnd: sequenceTrain

07/12/2016 08:02:32: Action "train" complete.

07/12/2016 08:02:32: __COMPLETED__