CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/cntk_sequence.cntk currentDirectory=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu DataDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 12 2016 13:55:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData
07/12/2016 14:27:16: -------------------------------------------------------------------
07/12/2016 14:27:16: Build info: 

07/12/2016 14:27:16: 		Built time: Jul 12 2016 13:55:00
07/12/2016 14:27:16: 		Last modified date: Tue Jul 12 04:28:35 2016
07/12/2016 14:27:16: 		Build type: debug
07/12/2016 14:27:16: 		Build target: GPU
07/12/2016 14:27:16: 		With 1bit-SGD: no
07/12/2016 14:27:16: 		Math lib: mkl
07/12/2016 14:27:16: 		CUDA_PATH: /usr/local/cuda-7.5
07/12/2016 14:27:16: 		CUB_PATH: /usr/local/cub-1.4.1
07/12/2016 14:27:16: 		CUDNN_PATH: /usr/local/cudnn-4.0
07/12/2016 14:27:16: 		Build Branch: HEAD
07/12/2016 14:27:16: 		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
07/12/2016 14:27:16: 		Built by philly on 2bc22072e267
07/12/2016 14:27:16: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
07/12/2016 14:27:16: -------------------------------------------------------------------
07/12/2016 14:27:17: -------------------------------------------------------------------
07/12/2016 14:27:17: GPU info:

07/12/2016 14:27:17: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:27:17: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:27:17: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:27:17: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:27:17: -------------------------------------------------------------------

07/12/2016 14:27:17: Running on localhost at 2016/07/12 14:27:17
07/12/2016 14:27:17: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/cntk_sequence.cntk  currentDirectory=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu  DataDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining  OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu  DeviceId=0  timestamping=true



07/12/2016 14:27:17: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/12/2016 14:27:17: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "$RunDir$/models/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.sequence.0"
    editPath  = "$ConfigDir$/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "$DataDir$/model.overalltying"
            transpFile = "$DataDir$/model.transprob"
        ]
        lattices = [
            denlatTocFile = "$DataDir$/*.lats.toc"
        ]
    ]
]
currentDirectory=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData
RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu
DataDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu
DeviceId=0
timestamping=true

07/12/2016 14:27:17: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/12/2016 14:27:17: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/12/2016 14:27:17: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf"
            labelMappingFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.overalltying"
            transpFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/*.lats.toc"
        ]
    ]
]
currentDirectory=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData
RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu
DataDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu
DeviceId=0
timestamping=true

07/12/2016 14:27:17: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/12/2016 14:27:17: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData
configparameters: cntk_sequence.cntk:DataDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf"
            labelMappingFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.overalltying"
            transpFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
07/12/2016 14:27:17: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/12/2016 14:27:17: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
07/12/2016 14:27:17: Precision = "float"
07/12/2016 14:27:17: CNTKModelPath: /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
07/12/2016 14:27:17: CNTKCommandTrainInfo: dptPre1 : 2
07/12/2016 14:27:17: CNTKModelPath: /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
07/12/2016 14:27:17: CNTKCommandTrainInfo: dptPre2 : 2
07/12/2016 14:27:17: CNTKModelPath: /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech
07/12/2016 14:27:17: CNTKCommandTrainInfo: speechTrain : 4
07/12/2016 14:27:17: CNTKModelPath: /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence
07/12/2016 14:27:17: CNTKCommandTrainInfo: sequenceTrain : 3
07/12/2016 14:27:17: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11

07/12/2016 14:27:17: ##############################################################################
07/12/2016 14:27:17: #                                                                            #
07/12/2016 14:27:17: # Action "train"                                                             #
07/12/2016 14:27:17: #                                                                            #
07/12/2016 14:27:17: ##############################################################################

07/12/2016 14:27:17: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 14:27:17: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 14:27:18: Created model with 19 nodes on GPU 0.

07/12/2016 14:27:18: Training criterion node(s):
07/12/2016 14:27:18: 	ce = CrossEntropyWithSoftmax

07/12/2016 14:27:18: Evaluation criterion node(s):

07/12/2016 14:27:18: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0x7f0bf820fa28: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
0x7f0bf820fb28: {[logPrior Value[132 x 1]] }
0x7f0bf8211258: {[featNorm Value[363 x *]] }
0x7f0bf82117c8: {[HL1.t Value[512 x *]] }
0x7f0bf8211b78: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
0x7f0bf8211cd8: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
0x7f0bf8211e38: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
0x7f0bf82127f8: {[ce Gradient[1]] }
0x7f0bf82129b8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
0x7f0bf8212b78: {[OL.t Gradient[132 x 1 x *]] }
0x7f0bf8212d38: {[OL.b Gradient[132 x 1]] }
0x7f0bf8b29078: {[globalInvStd Value[363 x 1]] }
0x7f0bf8b29a18: {[globalPrior Value[132 x 1]] }
0x7f0bf8b2a3b8: {[HL1.W Value[512 x 363]] }
0x7f0bf8b2b898: {[HL1.b Value[512 x 1]] }
0x7f0bf8b2cb68: {[OL.W Value[132 x 512]] }
0x7f0bf8b2d398: {[OL.b Value[132 x 1]] }
0x7f0bf8b4bbc8: {[err Value[1]] }
0x7f0bfb174d18: {[labels Value[132 x *]] }
0x7f0bfb1d5de8: {[globalMean Value[363 x 1]] }
0x7f0bfb9134f8: {[features Value[363 x *]] }
0x7f0bfe156cb8: {[scaledLogLikelihood Value[132 x 1 x *]] }
0x7f0bfe156e78: {[ce Value[1]] }

07/12/2016 14:27:18: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 14:27:18: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 14:27:19: Starting minibatch loop.
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.74183846 * 2560; err = 0.80195313 * 2560; time = 0.1792s; samplesPerSecond = 14284.6
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.91124763 * 2560; err = 0.70898438 * 2560; time = 0.0649s; samplesPerSecond = 39433.1
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.58015976 * 2560; err = 0.66640625 * 2560; time = 0.0646s; samplesPerSecond = 39637.1
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.27427139 * 2560; err = 0.58750000 * 2560; time = 0.0658s; samplesPerSecond = 38878.6
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.05503616 * 2560; err = 0.56093750 * 2560; time = 0.0641s; samplesPerSecond = 39910.8
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.91055145 * 2560; err = 0.52812500 * 2560; time = 0.0641s; samplesPerSecond = 39952.6
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.81562653 * 2560; err = 0.51171875 * 2560; time = 0.0651s; samplesPerSecond = 39322.3
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.68803253 * 2560; err = 0.48476562 * 2560; time = 0.0771s; samplesPerSecond = 33192.9
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.57382050 * 2560; err = 0.45429687 * 2560; time = 0.0661s; samplesPerSecond = 38749.1
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62090149 * 2560; err = 0.47304687 * 2560; time = 0.0656s; samplesPerSecond = 39000.6
07/12/2016 14:27:19:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.59272461 * 2560; err = 0.47500000 * 2560; time = 0.0661s; samplesPerSecond = 38738.6
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51520386 * 2560; err = 0.44531250 * 2560; time = 0.0645s; samplesPerSecond = 39660.4
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.49181976 * 2560; err = 0.45039062 * 2560; time = 0.0670s; samplesPerSecond = 38194.7
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.53703613 * 2560; err = 0.44804688 * 2560; time = 0.0673s; samplesPerSecond = 38011.0
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.43095398 * 2560; err = 0.41640625 * 2560; time = 0.0644s; samplesPerSecond = 39724.4
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.41503601 * 2560; err = 0.40078125 * 2560; time = 0.0648s; samplesPerSecond = 39498.9
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.38913574 * 2560; err = 0.41132812 * 2560; time = 0.0646s; samplesPerSecond = 39637.1
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.41207886 * 2560; err = 0.42226562 * 2560; time = 0.0654s; samplesPerSecond = 39151.5
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.39968262 * 2560; err = 0.40664062 * 2560; time = 0.0645s; samplesPerSecond = 39671.5
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42729187 * 2560; err = 0.42617187 * 2560; time = 0.0655s; samplesPerSecond = 39079.8
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.41336365 * 2560; err = 0.42343750 * 2560; time = 0.0659s; samplesPerSecond = 38875.6
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.33186951 * 2560; err = 0.39960937 * 2560; time = 0.0644s; samplesPerSecond = 39768.2
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.28581238 * 2560; err = 0.38710937 * 2560; time = 0.0640s; samplesPerSecond = 40010.0
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.34127502 * 2560; err = 0.40976563 * 2560; time = 0.0647s; samplesPerSecond = 39592.9
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.32666016 * 2560; err = 0.39726563 * 2560; time = 0.0651s; samplesPerSecond = 39336.8
07/12/2016 14:27:20:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.21437378 * 2560; err = 0.37265625 * 2560; time = 0.0641s; samplesPerSecond = 39952.6
07/12/2016 14:27:21:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23749695 * 2560; err = 0.37343750 * 2560; time = 0.0642s; samplesPerSecond = 39888.4
07/12/2016 14:27:21:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29956665 * 2560; err = 0.39023438 * 2560; time = 0.0654s; samplesPerSecond = 39131.2
07/12/2016 14:27:21:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.21198120 * 2560; err = 0.37382813 * 2560; time = 0.0640s; samplesPerSecond = 40000.0
07/12/2016 14:27:21:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20528259 * 2560; err = 0.36718750 * 2560; time = 0.0642s; samplesPerSecond = 39860.5
07/12/2016 14:27:21:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23613586 * 2560; err = 0.37343750 * 2560; time = 0.0647s; samplesPerSecond = 39556.2
07/12/2016 14:27:21:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.25615234 * 2560; err = 0.38164063 * 2560; time = 0.0596s; samplesPerSecond = 42952.3
07/12/2016 14:27:21: Finished Epoch[ 1 of 2]: [Training] ce = 1.62945061 * 81920; err = 0.46030273 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=3.10452s
07/12/2016 14:27:21: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech.1'

07/12/2016 14:27:21: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 14:27:21: Starting minibatch loop.
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.23230953 * 2560; err = 0.38320312 * 2560; time = 0.0657s; samplesPerSecond = 38993.5
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.20511341 * 2560; err = 0.37421875 * 2560; time = 0.0648s; samplesPerSecond = 39481.2
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.28783760 * 2560; err = 0.37421875 * 2560; time = 0.0643s; samplesPerSecond = 39822.0
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.22809334 * 2560; err = 0.37421875 * 2560; time = 0.0647s; samplesPerSecond = 39540.3
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.18090286 * 2560; err = 0.35468750 * 2560; time = 0.0635s; samplesPerSecond = 40315.0
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.28175354 * 2560; err = 0.37695312 * 2560; time = 0.0648s; samplesPerSecond = 39492.2
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.22251205 * 2560; err = 0.37382813 * 2560; time = 0.0634s; samplesPerSecond = 40367.1
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.17863007 * 2560; err = 0.36328125 * 2560; time = 0.0637s; samplesPerSecond = 40179.6
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.23061218 * 2560; err = 0.35742188 * 2560; time = 0.0636s; samplesPerSecond = 40275.0
07/12/2016 14:27:21:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18048782 * 2560; err = 0.37578125 * 2560; time = 0.0644s; samplesPerSecond = 39736.7
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.19648056 * 2560; err = 0.35976562 * 2560; time = 0.0634s; samplesPerSecond = 40361.4
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18896942 * 2560; err = 0.35429688 * 2560; time = 0.0632s; samplesPerSecond = 40476.2
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.16628113 * 2560; err = 0.35937500 * 2560; time = 0.0681s; samplesPerSecond = 37605.6
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.12856445 * 2560; err = 0.35195312 * 2560; time = 0.0646s; samplesPerSecond = 39613.8
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10083466 * 2560; err = 0.32617188 * 2560; time = 0.0658s; samplesPerSecond = 38886.3
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09875183 * 2560; err = 0.33906250 * 2560; time = 0.0665s; samplesPerSecond = 38515.9
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.18634949 * 2560; err = 0.35820313 * 2560; time = 0.0664s; samplesPerSecond = 38574.0
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.15709991 * 2560; err = 0.35195312 * 2560; time = 0.0662s; samplesPerSecond = 38691.2
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10971069 * 2560; err = 0.34960938 * 2560; time = 0.0661s; samplesPerSecond = 38714.0
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11317139 * 2560; err = 0.35000000 * 2560; time = 0.0662s; samplesPerSecond = 38698.2
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.08727722 * 2560; err = 0.32578125 * 2560; time = 0.0659s; samplesPerSecond = 38822.6
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.12296143 * 2560; err = 0.34101562 * 2560; time = 0.0659s; samplesPerSecond = 38849.7
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12966003 * 2560; err = 0.35078125 * 2560; time = 0.0659s; samplesPerSecond = 38849.1
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27489319 * 2560; err = 0.39257812 * 2560; time = 0.0660s; samplesPerSecond = 38768.5
07/12/2016 14:27:22:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.17423401 * 2560; err = 0.35156250 * 2560; time = 0.0661s; samplesPerSecond = 38737.4
07/12/2016 14:27:23:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13240051 * 2560; err = 0.35625000 * 2560; time = 0.0660s; samplesPerSecond = 38800.2
07/12/2016 14:27:23:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.13792114 * 2560; err = 0.34335938 * 2560; time = 0.0662s; samplesPerSecond = 38687.1
07/12/2016 14:27:23:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.13433228 * 2560; err = 0.33710937 * 2560; time = 0.0660s; samplesPerSecond = 38797.3
07/12/2016 14:27:23:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.05835876 * 2560; err = 0.33710937 * 2560; time = 0.0661s; samplesPerSecond = 38736.2
07/12/2016 14:27:23:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.09596558 * 2560; err = 0.33476563 * 2560; time = 0.0661s; samplesPerSecond = 38752.6
07/12/2016 14:27:23:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08180847 * 2560; err = 0.33242187 * 2560; time = 0.0656s; samplesPerSecond = 39029.7
07/12/2016 14:27:23:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06572876 * 2560; err = 0.33632812 * 2560; time = 0.0591s; samplesPerSecond = 43287.1
07/12/2016 14:27:23: Finished Epoch[ 2 of 2]: [Training] ce = 1.16156273 * 81920; err = 0.35460205 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=2.09468s
07/12/2016 14:27:23: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech'
07/12/2016 14:27:23: CNTKCommandTrainEnd: dptPre1

07/12/2016 14:27:23: Action "train" complete.


07/12/2016 14:27:23: ##############################################################################
07/12/2016 14:27:23: #                                                                            #
07/12/2016 14:27:23: # Action "edit"                                                              #
07/12/2016 14:27:23: #                                                                            #
07/12/2016 14:27:23: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 14:27:23: Action "edit" complete.


07/12/2016 14:27:23: ##############################################################################
07/12/2016 14:27:23: #                                                                            #
07/12/2016 14:27:23: # Action "train"                                                             #
07/12/2016 14:27:23: #                                                                            #
07/12/2016 14:27:23: ##############################################################################

07/12/2016 14:27:23: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 14:27:23: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 14:27:23: Loaded model with 24 nodes on GPU 0.

07/12/2016 14:27:23: Training criterion node(s):
07/12/2016 14:27:23: 	ce = CrossEntropyWithSoftmax

07/12/2016 14:27:23: Evaluation criterion node(s):

07/12/2016 14:27:23: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
0x7f0bef6a15d8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
0x7f0bef6a1798: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
0x7f0bef6abef8: {[err Value[1]] }
0x7f0bef6ba058: {[OL.b Value[132 x 1]] }
0x7f0bef6d1978: {[features Value[363 x *3]] }
0x7f0bef6dec08: {[HL2.W Value[512 x 512]] }
0x7f0bef6f25e8: {[globalInvStd Value[363 x 1]] }
0x7f0bef6f2ae8: {[labels Value[132 x *3]] }
0x7f0bef6f37d8: {[globalPrior Value[132 x 1]] }
0x7f0bef6f5fe8: {[HL1.t Value[512 x *3]] }
0x7f0bef6f61a8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
0x7f0bef6f8338: {[OL.W Value[132 x 512]] }
0x7f0bef6f8618: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
0x7f0bef6f87d8: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
0x7f0bef6f8998: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
0x7f0bef6f8b58: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
0x7f0bf82104f8: {[featNorm Value[363 x *3]] }
0x7f0bf82105f8: {[logPrior Value[132 x 1]] }
0x7f0bfb901468: {[ce Gradient[1]] }
0x7f0bfb901628: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
0x7f0bfb9017e8: {[OL.t Gradient[132 x 1 x *3]] }
0x7f0bfb9019a8: {[OL.b Gradient[132 x 1]] }
0x7f0bfb913888: {[HL2.b Value[512 x 1]] }
0x7f0bfb929708: {[HL1.W Value[512 x 363]] }
0x7f0bfb92a9c8: {[HL1.b Value[512 x 1]] }
0x7f0bfb9bcec8: {[globalMean Value[363 x 1]] }
0x7f0bfe1570d8: {[scaledLogLikelihood Value[132 x 1 x *3]] }
0x7f0bfe1ba248: {[ce Value[1]] }

07/12/2016 14:27:23: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 14:27:23: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 14:27:24: Starting minibatch loop.
07/12/2016 14:27:24:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 4.30124588 * 2560; err = 0.80703125 * 2560; time = 0.0872s; samplesPerSecond = 29348.7
07/12/2016 14:27:24:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.75448074 * 2560; err = 0.69960937 * 2560; time = 0.0836s; samplesPerSecond = 30604.4
07/12/2016 14:27:24:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.20926208 * 2560; err = 0.58515625 * 2560; time = 0.0831s; samplesPerSecond = 30807.7
07/12/2016 14:27:24:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.88578110 * 2560; err = 0.50117188 * 2560; time = 0.0837s; samplesPerSecond = 30601.1
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.71906204 * 2560; err = 0.47773437 * 2560; time = 0.0828s; samplesPerSecond = 30916.8
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.60130463 * 2560; err = 0.44648437 * 2560; time = 0.0833s; samplesPerSecond = 30737.5
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.56077118 * 2560; err = 0.45000000 * 2560; time = 0.0831s; samplesPerSecond = 30788.8
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.47116547 * 2560; err = 0.42460938 * 2560; time = 0.0828s; samplesPerSecond = 30916.8
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.38874512 * 2560; err = 0.40781250 * 2560; time = 0.0837s; samplesPerSecond = 30582.1
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41911163 * 2560; err = 0.42539063 * 2560; time = 0.0827s; samplesPerSecond = 30939.5
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.38730774 * 2560; err = 0.42148438 * 2560; time = 0.0828s; samplesPerSecond = 30917.5
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.36617889 * 2560; err = 0.41015625 * 2560; time = 0.0839s; samplesPerSecond = 30511.1
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.33381653 * 2560; err = 0.40781250 * 2560; time = 0.0835s; samplesPerSecond = 30674.5
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.39802246 * 2560; err = 0.40546875 * 2560; time = 0.0838s; samplesPerSecond = 30558.0
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.33336182 * 2560; err = 0.40195313 * 2560; time = 0.0837s; samplesPerSecond = 30577.8
07/12/2016 14:27:25:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.33834229 * 2560; err = 0.40195313 * 2560; time = 0.0835s; samplesPerSecond = 30658.7
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.26663208 * 2560; err = 0.37578125 * 2560; time = 0.0840s; samplesPerSecond = 30492.9
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28086243 * 2560; err = 0.39296875 * 2560; time = 0.0835s; samplesPerSecond = 30640.7
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.29481506 * 2560; err = 0.39531250 * 2560; time = 0.0836s; samplesPerSecond = 30604.4
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.27625122 * 2560; err = 0.39375000 * 2560; time = 0.0845s; samplesPerSecond = 30285.8
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.26905518 * 2560; err = 0.38984375 * 2560; time = 0.0836s; samplesPerSecond = 30615.1
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.21494751 * 2560; err = 0.36250000 * 2560; time = 0.0838s; samplesPerSecond = 30558.4
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.20699158 * 2560; err = 0.36914062 * 2560; time = 0.0841s; samplesPerSecond = 30457.3
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.25002136 * 2560; err = 0.37851563 * 2560; time = 0.0847s; samplesPerSecond = 30225.7
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.22617187 * 2560; err = 0.37656250 * 2560; time = 0.0849s; samplesPerSecond = 30163.8
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.14840393 * 2560; err = 0.35468750 * 2560; time = 0.0841s; samplesPerSecond = 30444.7
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.16649780 * 2560; err = 0.35468750 * 2560; time = 0.0838s; samplesPerSecond = 30535.8
07/12/2016 14:27:26:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.22885742 * 2560; err = 0.36992188 * 2560; time = 0.0848s; samplesPerSecond = 30205.8
07/12/2016 14:27:27:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.16533203 * 2560; err = 0.36484375 * 2560; time = 0.0835s; samplesPerSecond = 30649.5
07/12/2016 14:27:27:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.17502136 * 2560; err = 0.35664062 * 2560; time = 0.0838s; samplesPerSecond = 30546.4
07/12/2016 14:27:27:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.16159058 * 2560; err = 0.35195312 * 2560; time = 0.0841s; samplesPerSecond = 30453.0
07/12/2016 14:27:27:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.17113953 * 2560; err = 0.35429688 * 2560; time = 0.0784s; samplesPerSecond = 32645.1
07/12/2016 14:27:27: Finished Epoch[ 1 of 2]: [Training] ce = 1.49907970 * 81920; err = 0.42547607 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=3.55185s
07/12/2016 14:27:27: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.1'

07/12/2016 14:27:27: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 14:27:27: Starting minibatch loop.
07/12/2016 14:27:27:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.14215403 * 2560; err = 0.34882812 * 2560; time = 0.0858s; samplesPerSecond = 29852.1
07/12/2016 14:27:27:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.17049246 * 2560; err = 0.36328125 * 2560; time = 0.0837s; samplesPerSecond = 30598.2
07/12/2016 14:27:27:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.24373856 * 2560; err = 0.37460938 * 2560; time = 0.0839s; samplesPerSecond = 30496.9
07/12/2016 14:27:27:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.18655586 * 2560; err = 0.36445312 * 2560; time = 0.0828s; samplesPerSecond = 30902.9
07/12/2016 14:27:27:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.13848000 * 2560; err = 0.35039063 * 2560; time = 0.0825s; samplesPerSecond = 31019.4
07/12/2016 14:27:27:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.21884155 * 2560; err = 0.36757812 * 2560; time = 0.0836s; samplesPerSecond = 30613.2
07/12/2016 14:27:27:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14372940 * 2560; err = 0.35000000 * 2560; time = 0.0834s; samplesPerSecond = 30682.9
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.12769089 * 2560; err = 0.34960938 * 2560; time = 0.0834s; samplesPerSecond = 30678.2
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.14114227 * 2560; err = 0.33554688 * 2560; time = 0.0839s; samplesPerSecond = 30524.9
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12445145 * 2560; err = 0.34843750 * 2560; time = 0.0833s; samplesPerSecond = 30728.6
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.14137878 * 2560; err = 0.34101562 * 2560; time = 0.0837s; samplesPerSecond = 30603.7
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.12705154 * 2560; err = 0.33867188 * 2560; time = 0.0837s; samplesPerSecond = 30585.4
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10779419 * 2560; err = 0.34531250 * 2560; time = 0.0832s; samplesPerSecond = 30757.8
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.07003021 * 2560; err = 0.32500000 * 2560; time = 0.0840s; samplesPerSecond = 30474.0
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.05308990 * 2560; err = 0.31406250 * 2560; time = 0.0832s; samplesPerSecond = 30772.2
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.06392975 * 2560; err = 0.33085938 * 2560; time = 0.0834s; samplesPerSecond = 30713.1
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.14430847 * 2560; err = 0.35507813 * 2560; time = 0.0835s; samplesPerSecond = 30665.3
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.14809570 * 2560; err = 0.35859375 * 2560; time = 0.0841s; samplesPerSecond = 30425.1
07/12/2016 14:27:28:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.08184509 * 2560; err = 0.33515625 * 2560; time = 0.0839s; samplesPerSecond = 30506.0
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.07637024 * 2560; err = 0.33359375 * 2560; time = 0.0838s; samplesPerSecond = 30550.0
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.06249695 * 2560; err = 0.32500000 * 2560; time = 0.0833s; samplesPerSecond = 30725.7
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.09361877 * 2560; err = 0.33320312 * 2560; time = 0.0839s; samplesPerSecond = 30517.6
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12118683 * 2560; err = 0.34843750 * 2560; time = 0.0828s; samplesPerSecond = 30909.3
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.13457642 * 2560; err = 0.35195312 * 2560; time = 0.0827s; samplesPerSecond = 30969.5
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.09024963 * 2560; err = 0.33984375 * 2560; time = 0.0837s; samplesPerSecond = 30594.2
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07457275 * 2560; err = 0.33164063 * 2560; time = 0.0828s; samplesPerSecond = 30928.3
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.05975952 * 2560; err = 0.32070312 * 2560; time = 0.0828s; samplesPerSecond = 30914.1
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09778137 * 2560; err = 0.33242187 * 2560; time = 0.0833s; samplesPerSecond = 30720.9
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.01963196 * 2560; err = 0.32539062 * 2560; time = 0.0826s; samplesPerSecond = 30996.1
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.07533875 * 2560; err = 0.33515625 * 2560; time = 0.0832s; samplesPerSecond = 30775.5
07/12/2016 14:27:29:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06417236 * 2560; err = 0.33007812 * 2560; time = 0.0828s; samplesPerSecond = 30935.4
07/12/2016 14:27:30:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04990234 * 2560; err = 0.33359375 * 2560; time = 0.0775s; samplesPerSecond = 33039.9
07/12/2016 14:27:30: Finished Epoch[ 2 of 2]: [Training] ce = 1.11232681 * 81920; err = 0.34179688 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=2.67424s
07/12/2016 14:27:30: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech'
07/12/2016 14:27:30: CNTKCommandTrainEnd: dptPre2

07/12/2016 14:27:30: Action "train" complete.


07/12/2016 14:27:30: ##############################################################################
07/12/2016 14:27:30: #                                                                            #
07/12/2016 14:27:30: # Action "edit"                                                              #
07/12/2016 14:27:30: #                                                                            #
07/12/2016 14:27:30: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 14:27:30: Action "edit" complete.


07/12/2016 14:27:30: ##############################################################################
07/12/2016 14:27:30: #                                                                            #
07/12/2016 14:27:30: # Action "train"                                                             #
07/12/2016 14:27:30: #                                                                            #
07/12/2016 14:27:30: ##############################################################################

07/12/2016 14:27:30: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 14:27:30: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 14:27:30: Loaded model with 29 nodes on GPU 0.

07/12/2016 14:27:30: Training criterion node(s):
07/12/2016 14:27:30: 	ce = CrossEntropyWithSoftmax

07/12/2016 14:27:30: Evaluation criterion node(s):

07/12/2016 14:27:30: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0x7f0bef652e18: {[globalPrior Value[132 x 1]] }
0x7f0bef69e148: {[OL.t Gradient[132 x 1 x *6]] }
0x7f0bef69e308: {[OL.b Gradient[132 x 1]] }
0x7f0bef6a2908: {[globalInvStd Value[363 x 1]] }
0x7f0bef6a40e8: {[HL2.W Value[512 x 512]] }
0x7f0bef6a4258: {[HL1.t Value[512 x *6]] }
0x7f0bef6a4948: {[featNorm Value[363 x *6]] }
0x7f0bef6a6348: {[OL.b Value[132 x 1]] }
0x7f0bef6a6e88: {[globalMean Value[363 x 1]] }
0x7f0bef6ec768: {[OL.W Value[132 x 512]] }
0x7f0bf80003a8: {[HL1.b Value[512 x 1]] }
0x7f0bf82111f8: {[err Value[1]] }
0x7f0bf82113b8: {[scaledLogLikelihood Value[132 x 1 x *6]] }
0x7f0bf8229c78: {[HL3.W Value[512 x 512]] }
0x7f0bf8b2c078: {[labels Value[132 x *6]] }
0x7f0bf8b4cc68: {[HL3.b Value[512 x 1]] }
0x7f0bfb1d5ab8: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
0x7f0bfb1d5c78: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
0x7f0bfb1d5e38: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
0x7f0bfb1d5ff8: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
0x7f0bfb97ae48: {[ce Gradient[1]] }
0x7f0bfb97b008: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0x7f0bfb97b1d8: {[ce Value[1]] }
0x7f0bfb97b3e8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
0x7f0bfb97b548: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
0x7f0bfb97b708: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
0x7f0bfb97b7a8: {[logPrior Value[132 x 1]] }
0x7f0bfb98c0e8: {[HL1.W Value[512 x 363]] }
0x7f0bfb98c808: {[HL2.b Value[512 x 1]] }
0x7f0bfb99a588: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
0x7f0bfb99a748: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
0x7f0bfb99a908: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
0x7f0bfe157178: {[features Value[363 x *6]] }

07/12/2016 14:27:30: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 14:27:30: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 14:27:31: Starting minibatch loop.
07/12/2016 14:27:31:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.97086372 * 2560; err = 0.81445312 * 2560; time = 0.1061s; samplesPerSecond = 24120.9
07/12/2016 14:27:31:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.63975792 * 2560; err = 0.63320312 * 2560; time = 0.1031s; samplesPerSecond = 24839.7
07/12/2016 14:27:31:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.02565231 * 2560; err = 0.54257813 * 2560; time = 0.1026s; samplesPerSecond = 24952.7
07/12/2016 14:27:31:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74204865 * 2560; err = 0.47500000 * 2560; time = 0.1041s; samplesPerSecond = 24585.4
07/12/2016 14:27:31:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.58343964 * 2560; err = 0.45156250 * 2560; time = 0.1034s; samplesPerSecond = 24758.9
07/12/2016 14:27:31:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.47893143 * 2560; err = 0.42343750 * 2560; time = 0.1027s; samplesPerSecond = 24918.5
07/12/2016 14:27:31:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.43405457 * 2560; err = 0.40898438 * 2560; time = 0.1033s; samplesPerSecond = 24784.6
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35973663 * 2560; err = 0.39648438 * 2560; time = 0.1025s; samplesPerSecond = 24984.1
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.28108978 * 2560; err = 0.37968750 * 2560; time = 0.1032s; samplesPerSecond = 24797.8
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.29773560 * 2560; err = 0.39765625 * 2560; time = 0.1026s; samplesPerSecond = 24959.1
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.28441925 * 2560; err = 0.39062500 * 2560; time = 0.1033s; samplesPerSecond = 24773.1
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27777252 * 2560; err = 0.38164063 * 2560; time = 0.1025s; samplesPerSecond = 24967.1
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.23615112 * 2560; err = 0.37421875 * 2560; time = 0.1031s; samplesPerSecond = 24840.1
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.31171112 * 2560; err = 0.38671875 * 2560; time = 0.1025s; samplesPerSecond = 24982.7
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.25573883 * 2560; err = 0.37773438 * 2560; time = 0.1031s; samplesPerSecond = 24831.0
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.27382965 * 2560; err = 0.38398437 * 2560; time = 0.1031s; samplesPerSecond = 24841.3
07/12/2016 14:27:32:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.20634155 * 2560; err = 0.36406250 * 2560; time = 0.1060s; samplesPerSecond = 24142.7
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.20973816 * 2560; err = 0.36562500 * 2560; time = 0.1060s; samplesPerSecond = 24156.2
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.20688782 * 2560; err = 0.36718750 * 2560; time = 0.1059s; samplesPerSecond = 24166.7
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.20260315 * 2560; err = 0.37226562 * 2560; time = 0.1059s; samplesPerSecond = 24184.0
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.20553894 * 2560; err = 0.37187500 * 2560; time = 0.1058s; samplesPerSecond = 24187.2
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.14160156 * 2560; err = 0.34726563 * 2560; time = 0.1059s; samplesPerSecond = 24171.5
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.15316467 * 2560; err = 0.35273437 * 2560; time = 0.1059s; samplesPerSecond = 24171.2
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.19352417 * 2560; err = 0.35468750 * 2560; time = 0.1059s; samplesPerSecond = 24166.9
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.17192078 * 2560; err = 0.35937500 * 2560; time = 0.1044s; samplesPerSecond = 24521.1
07/12/2016 14:27:33:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.08281860 * 2560; err = 0.33867188 * 2560; time = 0.1048s; samplesPerSecond = 24435.6
07/12/2016 14:27:34:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.11028442 * 2560; err = 0.34453125 * 2560; time = 0.1042s; samplesPerSecond = 24574.3
07/12/2016 14:27:34:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.17454224 * 2560; err = 0.35312500 * 2560; time = 0.1049s; samplesPerSecond = 24414.4
07/12/2016 14:27:34:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.11068115 * 2560; err = 0.34531250 * 2560; time = 0.1042s; samplesPerSecond = 24567.2
07/12/2016 14:27:34:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.12955627 * 2560; err = 0.34296875 * 2560; time = 0.1049s; samplesPerSecond = 24400.0
07/12/2016 14:27:34:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12482300 * 2560; err = 0.34570312 * 2560; time = 0.1043s; samplesPerSecond = 24543.4
07/12/2016 14:27:34:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12771912 * 2560; err = 0.34453125 * 2560; time = 0.0990s; samplesPerSecond = 25869.3
07/12/2016 14:27:34: Finished Epoch[ 1 of 4]: [Training] ce = 1.40639620 * 81920; err = 0.40274658 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=4.20013s
07/12/2016 14:27:34: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.1'

07/12/2016 14:27:34: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 14:27:34: Starting minibatch loop.
07/12/2016 14:27:34:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.51739798 * 5120; err = 0.41425781 * 5120; time = 0.1521s; samplesPerSecond = 33661.6
07/12/2016 14:27:34:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.25793447 * 5120; err = 0.37539062 * 5120; time = 0.1457s; samplesPerSecond = 35143.4
07/12/2016 14:27:35:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.18638287 * 5120; err = 0.36718750 * 5120; time = 0.1447s; samplesPerSecond = 35393.1
07/12/2016 14:27:35:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.12794571 * 5120; err = 0.34218750 * 5120; time = 0.1442s; samplesPerSecond = 35509.4
07/12/2016 14:27:35:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14070625 * 5120; err = 0.34570312 * 5120; time = 0.1441s; samplesPerSecond = 35536.3
07/12/2016 14:27:35:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.14582825 * 5120; err = 0.34765625 * 5120; time = 0.1443s; samplesPerSecond = 35493.4
07/12/2016 14:27:35:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.11193542 * 5120; err = 0.34414062 * 5120; time = 0.1436s; samplesPerSecond = 35644.7
07/12/2016 14:27:35:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08574600 * 5120; err = 0.33789062 * 5120; time = 0.1445s; samplesPerSecond = 35428.8
07/12/2016 14:27:35:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.21058807 * 5120; err = 0.37363281 * 5120; time = 0.1443s; samplesPerSecond = 35477.7
07/12/2016 14:27:36:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.09668579 * 5120; err = 0.34335938 * 5120; time = 0.1436s; samplesPerSecond = 35643.4
07/12/2016 14:27:36:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05845032 * 5120; err = 0.32675781 * 5120; time = 0.1446s; samplesPerSecond = 35406.8
07/12/2016 14:27:36:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.10728455 * 5120; err = 0.34726563 * 5120; time = 0.1443s; samplesPerSecond = 35479.7
07/12/2016 14:27:36:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08716888 * 5120; err = 0.33593750 * 5120; time = 0.1437s; samplesPerSecond = 35640.7
07/12/2016 14:27:36:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.06778870 * 5120; err = 0.31855469 * 5120; time = 0.1442s; samplesPerSecond = 35505.3
07/12/2016 14:27:36:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04079590 * 5120; err = 0.32910156 * 5120; time = 0.1442s; samplesPerSecond = 35507.7
07/12/2016 14:27:36:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06249542 * 5120; err = 0.32968750 * 5120; time = 0.1334s; samplesPerSecond = 38390.9
07/12/2016 14:27:36: Finished Epoch[ 2 of 4]: [Training] ce = 1.14407091 * 81920; err = 0.34866943 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=2.3219s
07/12/2016 14:27:36: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.2'

07/12/2016 14:27:37: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

07/12/2016 14:27:37: Starting minibatch loop.
07/12/2016 14:27:37:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.11238871 * 5120; err = 0.34804687 * 5120; time = 0.1451s; samplesPerSecond = 35281.1
07/12/2016 14:27:37:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.09456148 * 5120; err = 0.34121094 * 5120; time = 0.1445s; samplesPerSecond = 35433.8
07/12/2016 14:27:37:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.10800076 * 5120; err = 0.34667969 * 5120; time = 0.1438s; samplesPerSecond = 35614.9
07/12/2016 14:27:37:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.16617966 * 5120; err = 0.35566406 * 5120; time = 0.1445s; samplesPerSecond = 35440.1
07/12/2016 14:27:37:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14173546 * 5120; err = 0.34550781 * 5120; time = 0.1446s; samplesPerSecond = 35397.7
07/12/2016 14:27:37:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07876053 * 5120; err = 0.33359375 * 5120; time = 0.1436s; samplesPerSecond = 35650.1
07/12/2016 14:27:38:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08043213 * 5120; err = 0.33437500 * 5120; time = 0.1443s; samplesPerSecond = 35471.3
07/12/2016 14:27:38:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07423630 * 5120; err = 0.33007812 * 5120; time = 0.1445s; samplesPerSecond = 35433.8
07/12/2016 14:27:38:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.02659454 * 5120; err = 0.31113281 * 5120; time = 0.1439s; samplesPerSecond = 35587.7
07/12/2016 14:27:38:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.04602737 * 5120; err = 0.31855469 * 5120; time = 0.1447s; samplesPerSecond = 35376.5
07/12/2016 14:27:38:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05524902 * 5120; err = 0.33613281 * 5120; time = 0.1456s; samplesPerSecond = 35162.4
07/12/2016 14:27:38:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07627411 * 5120; err = 0.33613281 * 5120; time = 0.1447s; samplesPerSecond = 35383.6
07/12/2016 14:27:38:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05101776 * 5120; err = 0.31660156 * 5120; time = 0.1454s; samplesPerSecond = 35222.4
07/12/2016 14:27:39:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.03016815 * 5120; err = 0.32480469 * 5120; time = 0.1457s; samplesPerSecond = 35136.6
07/12/2016 14:27:39:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04644623 * 5120; err = 0.32929687 * 5120; time = 0.1442s; samplesPerSecond = 35509.2
07/12/2016 14:27:39:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02751465 * 5120; err = 0.32265625 * 5120; time = 0.1345s; samplesPerSecond = 38060.1
07/12/2016 14:27:39: Finished Epoch[ 3 of 4]: [Training] ce = 1.07597418 * 81920; err = 0.33315430 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=2.3198s
07/12/2016 14:27:39: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.3'

07/12/2016 14:27:39: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

07/12/2016 14:27:39: Starting minibatch loop.
07/12/2016 14:27:39:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.03003817 * 5120; err = 0.31289062 * 5120; time = 0.1457s; samplesPerSecond = 35133.7
07/12/2016 14:27:40:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.04137832 * 4926; err = 0.32480715 * 4926; time = 0.4376s; samplesPerSecond = 11256.0
07/12/2016 14:27:40:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.03293953 * 5120; err = 0.33046875 * 5120; time = 0.1438s; samplesPerSecond = 35592.9
07/12/2016 14:27:40:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.00674477 * 5120; err = 0.31738281 * 5120; time = 0.1447s; samplesPerSecond = 35381.8
07/12/2016 14:27:40:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.02883911 * 5120; err = 0.31757812 * 5120; time = 0.1449s; samplesPerSecond = 35324.5
07/12/2016 14:27:40:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 0.97552528 * 5120; err = 0.30195312 * 5120; time = 0.1439s; samplesPerSecond = 35579.0
07/12/2016 14:27:40:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.99455070 * 5120; err = 0.31171875 * 5120; time = 0.1447s; samplesPerSecond = 35392.4
07/12/2016 14:27:40:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.02535477 * 5120; err = 0.31835938 * 5120; time = 0.1449s; samplesPerSecond = 35336.4
07/12/2016 14:27:41:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99763412 * 5120; err = 0.30800781 * 5120; time = 0.1438s; samplesPerSecond = 35594.6
07/12/2016 14:27:41:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.98428116 * 5120; err = 0.31269531 * 5120; time = 0.1451s; samplesPerSecond = 35294.5
07/12/2016 14:27:41:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.03535004 * 5120; err = 0.31992188 * 5120; time = 0.1443s; samplesPerSecond = 35477.2
07/12/2016 14:27:41:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.00379181 * 5120; err = 0.31523438 * 5120; time = 0.1442s; samplesPerSecond = 35513.4
07/12/2016 14:27:41:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.98212585 * 5120; err = 0.29453125 * 5120; time = 0.1445s; samplesPerSecond = 35438.7
07/12/2016 14:27:41:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02532196 * 5120; err = 0.32128906 * 5120; time = 0.1439s; samplesPerSecond = 35589.7
07/12/2016 14:27:41:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.98880615 * 5120; err = 0.32148437 * 5120; time = 0.1447s; samplesPerSecond = 35387.7
07/12/2016 14:27:42:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.99748993 * 5120; err = 0.31035156 * 5120; time = 0.1372s; samplesPerSecond = 37325.9
07/12/2016 14:27:42: Finished Epoch[ 4 of 4]: [Training] ce = 1.00940647 * 81920; err = 0.31486816 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=2.62119s
07/12/2016 14:27:42: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech'
07/12/2016 14:27:42: CNTKCommandTrainEnd: speechTrain

07/12/2016 14:27:42: Action "train" complete.


07/12/2016 14:27:42: ##############################################################################
07/12/2016 14:27:42: #                                                                            #
07/12/2016 14:27:42: # Action "edit"                                                              #
07/12/2016 14:27:42: #                                                                            #
07/12/2016 14:27:42: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 14:27:42: Action "edit" complete.


07/12/2016 14:27:42: ##############################################################################
07/12/2016 14:27:42: #                                                                            #
07/12/2016 14:27:42: # Action "train"                                                             #
07/12/2016 14:27:42: #                                                                            #
07/12/2016 14:27:42: ##############################################################################

07/12/2016 14:27:42: CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.overalltying', '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list', '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/TestData/CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames

07/12/2016 14:27:42: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0'.

Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *9]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *9]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *9], [363 x 1], [363 x 1] -> [363 x *9]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *9] -> [512 x *9]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *9] -> [132 x 1 x *9]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *9], [132 x 1 x *9], [132 x 1 x *9] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *9], [132 x 1 x *9] -> [1]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 14:27:42: Loaded model with 29 nodes on GPU 0.

07/12/2016 14:27:42: Training criterion node(s):
07/12/2016 14:27:42: 	ce = SequenceWithSoftmax

07/12/2016 14:27:42: Evaluation criterion node(s):

07/12/2016 14:27:42: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *9]] [features Gradient[363 x *9]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *9]] [logPrior Gradient[132 x 1]] }
0x7f0bef651918: {[scaledLogLikelihood Value[132 x 1 x *9]] }
0x7f0bef6536b8: {[ce Value[1]] }
0x7f0bef680328: {[features Value[363 x *9]] }
0x7f0bef69a088: {[HL1.z Gradient[512 x 1 x *9]] [HL2.t Value[512 x 1 x *9]] }
0x7f0bef69a248: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *9]] }
0x7f0bef69a408: {[HL2.t Gradient[512 x 1 x *9]] [HL2.y Value[512 x 1 x *9]] }
0x7f0bef69a5c8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *9]] [HL2.z Gradient[512 x 1 x *9]] [HL3.t Value[512 x 1 x *9]] }
0x7f0bef69a788: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *9]] }
0x7f0bef69a948: {[HL3.t Gradient[512 x 1 x *9]] [HL3.y Value[512 x 1 x *9]] }
0x7f0bef69f6f8: {[HL1.t Value[512 x *9]] }
0x7f0bef6edde8: {[HL1.b Value[512 x 1]] }
0x7f0bef6f92d8: {[HL1.W Value[512 x 363]] }
0x7f0bf8211cc8: {[globalMean Value[363 x 1]] }
0x7f0bf82161b8: {[labels Value[132 x *9]] }
0x7f0bf8b2e308: {[OL.b Value[132 x 1]] }
0x7f0bf8b2e958: {[HL2.W Value[512 x 512]] }
0x7f0bf8b4d3d8: {[ce Gradient[1]] }
0x7f0bf8b4d598: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *9]] [OL.z Gradient[132 x 1 x *9]] }
0x7f0bf8b4d758: {[OL.t Gradient[132 x 1 x *9]] [scaledLogLikelihood Gradient[132 x 1 x *9]] }
0x7f0bf8b4d918: {[OL.b Gradient[132 x 1]] }
0x7f0bfb913bb8: {[globalPrior Value[132 x 1]] }
0x7f0bfb982da8: {[globalInvStd Value[363 x 1]] }
0x7f0bfb983d48: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *9]] }
0x7f0bfb987208: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *9]] [HL3.z Gradient[512 x 1 x *9]] [OL.t Value[132 x 1 x *9]] }
0x7f0bfb9873c8: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *9]] }
0x7f0bfb98df68: {[HL1.t Gradient[512 x *9]] [HL1.y Value[512 x 1 x *9]] }
0x7f0bfb9a0988: {[HL3.W Value[512 x 512]] }
0x7f0bfb9bebf8: {[featNorm Value[363 x *9]] }
0x7f0bfe1afd88: {[HL3.b Value[512 x 1]] }
0x7f0bfe1bc158: {[OL.W Value[132 x 512]] }
0x7f0bfe1be828: {[logPrior Value[132 x 1]] }
0x7f0bfe1c35c8: {[err Value[1]] }
0x7f0bfe1cbed8: {[HL2.b Value[512 x 1]] }

07/12/2016 14:27:42: No PreCompute nodes found, skipping PreCompute step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-10
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

07/12/2016 14:27:42: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 14:28:05: Starting minibatch loop.
dengamma value 0.992986
dengamma value 1.088457
dengamma value 1.034215
dengamma value 1.088625
dengamma value 1.004511
dengamma value 1.081701
dengamma value 1.000733
dengamma value 1.011767
dengamma value 1.083404
dengamma value 0.981096
dengamma value 0.998095
dengamma value 1.057976
dengamma value 1.028988
dengamma value 1.052751
dengamma value 1.007402
dengamma value 1.086035
dengamma value 1.050786
dengamma value 1.076071
dengamma value 1.005301
dengamma value 1.035060
dengamma value 1.050496
dengamma value 1.095194
07/12/2016 14:28:13:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.07785053 * 5566; err = 0.37693137 * 5566; time = 8.2136s; samplesPerSecond = 677.7
dengamma value 0.978431
dengamma value 1.027638
dengamma value 1.014623
dengamma value 1.023894
dengamma value 0.995082
dengamma value 1.045380
dengamma value 1.075316
dengamma value 1.043811
dengamma value 1.058397
dengamma value 0.999333
dengamma value 0.972484
dengamma value 1.061869
dengamma value 1.032441
dengamma value 1.007533
dengamma value 1.045418
dengamma value 0.990305
dengamma value 1.013094
dengamma value 1.042767
dengamma value 1.015576
dengamma value 1.072943
dengamma value 1.020339
dengamma value 1.074907
dengamma value 1.037635
dengamma value 1.148049
dengamma value 1.052523
dengamma value 1.041828
07/12/2016 14:28:14:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.06744787 * 7398; err = 0.38510408 * 7398; time = 1.0160s; samplesPerSecond = 7281.6
dengamma value 1.089501
dengamma value 0.985178
dengamma value 1.021306
dengamma value 0.963911
dengamma value 1.030083
dengamma value 1.037196
dengamma value 1.032671
dengamma value 1.030629
dengamma value 1.081717
dengamma value 1.100498
dengamma value 1.010930
dengamma value 1.051458
dengamma value 1.038498
dengamma value 0.931934
dengamma value 1.103183
dengamma value 1.040763
dengamma value 1.101100
dengamma value 0.994602
dengamma value 1.014975
dengamma value 1.032132
dengamma value 1.015596
dengamma value 1.018538
dengamma value 1.015717
dengamma value 1.100572
dengamma value 1.050210
07/12/2016 14:28:15:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.06760303 * 6300; err = 0.39476190 * 6300; time = 0.8694s; samplesPerSecond = 7246.5
dengamma value 1.090093
dengamma value 1.181361
dengamma value 1.057085
dengamma value 1.020752
dengamma value 1.015996
dengamma value 0.999135
dengamma value 0.992521
dengamma value 1.029631
dengamma value 1.095399
dengamma value 1.026307
dengamma value 1.043188
dengamma value 1.038694
dengamma value 1.073902
dengamma value 1.057050
dengamma value 1.093476
dengamma value 1.050931
dengamma value 0.972741
dengamma value 1.037640
dengamma value 0.992664
dengamma value 0.962542
dengamma value 0.957699
dengamma value 1.046604
07/12/2016 14:28:16:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.07077522 * 5636; err = 0.35858765 * 5636; time = 0.7651s; samplesPerSecond = 7366.3
dengamma value 1.037616
dengamma value 1.030657
dengamma value 1.041491
dengamma value 1.065234
dengamma value 1.036632
dengamma value 1.061818
dengamma value 1.038592
dengamma value 1.030698
dengamma value 0.941251
dengamma value 1.020817
dengamma value 1.024889
dengamma value 1.053771
dengamma value 1.046222
dengamma value 0.922839
dengamma value 1.060649
dengamma value 0.993468
dengamma value 1.060336
dengamma value 1.033160
dengamma value 1.031635
dengamma value 0.977661
07/12/2016 14:28:17:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.07548063 * 6970; err = 0.35939742 * 6970; time = 0.9426s; samplesPerSecond = 7394.3
dengamma value 1.059160
dengamma value 1.021155
dengamma value 0.980459
dengamma value 1.047700
dengamma value 1.079169
dengamma value 1.058102
dengamma value 1.041047
dengamma value 1.062561
dengamma value 1.042681
dengamma value 1.050491
dengamma value 0.985257
dengamma value 1.076864
dengamma value 1.034502
dengamma value 1.034452
dengamma value 1.000379
dengamma value 1.075595
dengamma value 1.043015
dengamma value 0.985578
dengamma value 1.095933
dengamma value 1.016459
dengamma value 1.007137
dengamma value 1.056761
07/12/2016 14:28:18:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.07766474 * 6996; err = 0.33404803 * 6996; time = 0.9965s; samplesPerSecond = 7020.4
dengamma value 1.083381
dengamma value 1.009075
dengamma value 1.041500
dengamma value 1.037039
dengamma value 0.988764
dengamma value 0.945261
dengamma value 1.022317
dengamma value 0.982509
dengamma value 1.059416
dengamma value 1.004559
dengamma value 1.037461
dengamma value 0.965892
dengamma value 1.048144
dengamma value 1.089514
dengamma value 1.097895
dengamma value 0.980721
dengamma value 0.983704
dengamma value 1.055338
dengamma value 1.035659
dengamma value 1.101946
dengamma value 1.016736
dengamma value 1.033402
dengamma value 1.077137
dengamma value 1.010613
dengamma value 1.004026
07/12/2016 14:28:19:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08154633 * 6180; err = 0.34595469 * 6180; time = 0.8648s; samplesPerSecond = 7146.4
dengamma value 1.034637
dengamma value 1.062048
dengamma value 1.150289
dengamma value 0.986887
dengamma value 1.055837
dengamma value 1.023006
dengamma value 1.009378
dengamma value 1.068830
dengamma value 1.022151
dengamma value 1.023302
dengamma value 1.023646
dengamma value 1.021830
dengamma value 1.093459
dengamma value 0.968360
dengamma value 1.094907
dengamma value 1.041649
dengamma value 1.018842
dengamma value 1.096404
dengamma value 1.054738
dengamma value 1.085952
07/12/2016 14:28:19:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08147560 * 4860; err = 0.34650206 * 4860; time = 0.6853s; samplesPerSecond = 7091.6
dengamma value 1.035703
dengamma value 0.951433
dengamma value 1.024705
dengamma value 1.061783
dengamma value 1.024938
dengamma value 1.076418
dengamma value 1.006541
dengamma value 1.030521
dengamma value 0.991606
dengamma value 0.985308
dengamma value 1.011480
dengamma value 0.940226
dengamma value 1.027672
dengamma value 1.071396
dengamma value 1.035429
dengamma value 1.094702
dengamma value 1.027508
dengamma value 1.033709
dengamma value 1.033934
dengamma value 1.003045
dengamma value 1.076101
dengamma value 1.042401
07/12/2016 14:28:20:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.07518631 * 6046; err = 0.33956335 * 6046; time = 0.8542s; samplesPerSecond = 7078.3
dengamma value 1.046328
dengamma value 1.002830
dengamma value 1.013836
dengamma value 0.955662
dengamma value 1.054540
dengamma value 1.069253
dengamma value 1.030195
dengamma value 1.008916
dengamma value 1.032325
dengamma value 1.045402
dengamma value 1.010811
dengamma value 1.010427
dengamma value 1.046920
dengamma value 1.067374
dengamma value 0.947512
dengamma value 1.035283
dengamma value 1.003956
dengamma value 1.029704
dengamma value 0.983596
dengamma value 0.974364
dengamma value 0.965165
dengamma value 1.072963
dengamma value 1.003624
dengamma value 1.017409
07/12/2016 14:28:21:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08506722 * 6942; err = 0.32713915 * 6942; time = 0.9171s; samplesPerSecond = 7569.5
dengamma value 0.998950
dengamma value 1.034198
dengamma value 0.991236
dengamma value 0.934500
dengamma value 1.019193
dengamma value 1.043853
dengamma value 1.117081
dengamma value 0.982104
dengamma value 1.004546
dengamma value 1.136810
dengamma value 1.055002
dengamma value 0.999889
dengamma value 1.041881
dengamma value 1.033256
dengamma value 1.079809
dengamma value 0.993262
dengamma value 1.025269
dengamma value 1.026062
dengamma value 0.995790
dengamma value 1.008858
dengamma value 0.976532
dengamma value 1.059939
dengamma value 1.048562
07/12/2016 14:28:22:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08287063 * 5784; err = 0.34785615 * 5784; time = 0.7810s; samplesPerSecond = 7406.2
dengamma value 1.077216
dengamma value 1.061159
dengamma value 1.002597
dengamma value 1.002627
dengamma value 1.107784
dengamma value 1.008948
dengamma value 0.946550
dengamma value 0.963039
dengamma value 0.931308
dengamma value 1.012982
dengamma value 1.030246
dengamma value 0.957238
dengamma value 1.038274
dengamma value 0.985958
dengamma value 1.030999
dengamma value 0.971969
dengamma value 1.011088
dengamma value 1.008050
dengamma value 1.036645
dengamma value 0.987878
dengamma value 1.084318
07/12/2016 14:28:23:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08800524 * 6258; err = 0.34068392 * 6258; time = 0.8439s; samplesPerSecond = 7415.5
dengamma value 1.080763
dengamma value 1.082563
dengamma value 1.039704
dengamma value 1.054039
dengamma value 1.014309
dengamma value 1.107503
dengamma value 1.059367
dengamma value 1.017404
dengamma value 1.028721
dengamma value 1.061391
dengamma value 1.026786
dengamma value 1.051332
dengamma value 1.041773
dengamma value 1.039926
dengamma value 0.968420
dengamma value 0.996968
dengamma value 1.023431
dengamma value 0.994183
dengamma value 1.091584
dengamma value 0.988077
dengamma value 1.001509
dengamma value 0.957258
07/12/2016 14:28:24:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08035779 * 6116; err = 0.32096141 * 6116; time = 0.8615s; samplesPerSecond = 7099.2
dengamma value 1.007056
dengamma value 1.055935
dengamma value 0.997914
dengamma value 1.055889
07/12/2016 14:28:24: Finished Epoch[ 1 of 3]: [Training] ce = 0.07787671 * 82574; err = 0.35150289 * 82574; totalSamplesSeen = 82574; learningRatePerSample = 2e-06; epochTime=41.6735s
07/12/2016 14:28:24: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.1'

07/12/2016 14:28:24: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 82146), data subset 0 of 1, with 1 datapasses

07/12/2016 14:28:24: Starting minibatch loop.
dengamma value 0.973541
dengamma value 0.992038
dengamma value 0.980856
dengamma value 1.077685
dengamma value 1.055914
dengamma value 0.997152
dengamma value 1.051641
dengamma value 0.986913
dengamma value 1.017538
dengamma value 1.028091
dengamma value 1.033410
dengamma value 0.983375
dengamma value 1.043356
dengamma value 1.094528
dengamma value 0.959124
dengamma value 0.992013
dengamma value 1.010798
dengamma value 1.004158
dengamma value 1.045856
dengamma value 1.026034
dengamma value 1.028289
dengamma value 1.090897
07/12/2016 14:28:25:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08456054 * 5826; err = 0.33436320 * 5826; time = 0.8372s; samplesPerSecond = 6958.7
dengamma value 1.055866
dengamma value 1.084910
dengamma value 1.090750
dengamma value 1.050464
dengamma value 1.018927
dengamma value 1.011011
dengamma value 1.085250
dengamma value 1.095183
dengamma value 1.101911
dengamma value 1.057892
dengamma value 1.032715
dengamma value 0.928197
dengamma value 1.041039
dengamma value 1.067336
dengamma value 1.041676
dengamma value 1.024469
dengamma value 1.034994
dengamma value 1.024526
dengamma value 1.049860
dengamma value 1.012654
07/12/2016 14:28:26:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07912028 * 6380; err = 0.29326019 * 6380; time = 0.9300s; samplesPerSecond = 6860.1
dengamma value 1.082824
dengamma value 1.003476
dengamma value 1.098238
dengamma value 0.942669
dengamma value 0.976806
dengamma value 1.058835
dengamma value 1.091135
dengamma value 1.029967
dengamma value 1.077941
dengamma value 1.030307
dengamma value 0.961987
dengamma value 1.091660
dengamma value 0.998039
dengamma value 1.055431
dengamma value 1.031775
dengamma value 1.014259
dengamma value 1.031999
dengamma value 1.078713
dengamma value 1.020117
dengamma value 1.002308
dengamma value 1.097331
dengamma value 1.033593
dengamma value 0.985260
07/12/2016 14:28:27:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.07948666 * 6574; err = 0.30985701 * 6574; time = 0.9854s; samplesPerSecond = 6671.3
dengamma value 1.052547
dengamma value 1.044150
dengamma value 1.033839
dengamma value 1.041544
dengamma value 0.920603
dengamma value 0.942028
dengamma value 1.159290
dengamma value 1.085545
dengamma value 1.004561
dengamma value 1.019037
dengamma value 1.023957
dengamma value 0.960178
dengamma value 1.030691
dengamma value 0.987787
dengamma value 0.929883
dengamma value 1.071638
dengamma value 1.023773
dengamma value 1.077543
dengamma value 0.980671
dengamma value 0.987111
dengamma value 1.068147
dengamma value 1.102342
dengamma value 1.007210
07/12/2016 14:28:28:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08819653 * 6324; err = 0.32495256 * 6324; time = 0.8515s; samplesPerSecond = 7427.0
dengamma value 0.951242
dengamma value 0.939685
dengamma value 0.948154
dengamma value 1.038595
dengamma value 1.073904
dengamma value 1.103528
dengamma value 1.034161
dengamma value 1.111369
dengamma value 1.003122
dengamma value 0.966257
dengamma value 1.005747
dengamma value 1.013101
dengamma value 0.948788
dengamma value 1.029738
dengamma value 1.001844
dengamma value 0.993117
dengamma value 0.944073
dengamma value 0.995512
dengamma value 1.023535
dengamma value 1.060474
07/12/2016 14:28:28:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.09131846 * 4800; err = 0.34729167 * 4800; time = 0.6469s; samplesPerSecond = 7420.5
dengamma value 0.983206
dengamma value 1.037094
dengamma value 1.032743
dengamma value 1.043776
dengamma value 1.023219
dengamma value 1.130026
dengamma value 1.078573
dengamma value 1.018778
dengamma value 1.049783
dengamma value 1.002523
dengamma value 1.071358
dengamma value 1.014749
dengamma value 0.982229
dengamma value 0.988656
dengamma value 1.045060
dengamma value 1.042801
dengamma value 1.030231
dengamma value 1.009515
dengamma value 1.008060
dengamma value 1.035762
dengamma value 0.998682
dengamma value 1.039744
07/12/2016 14:28:29:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08229946 * 6176; err = 0.32966321 * 6176; time = 0.8162s; samplesPerSecond = 7566.7
dengamma value 1.068722
dengamma value 1.043967
dengamma value 1.038172
dengamma value 0.982462
dengamma value 1.060893
dengamma value 1.017722
dengamma value 1.079586
dengamma value 0.966171
dengamma value 1.025363
dengamma value 1.037303
dengamma value 1.064635
dengamma value 1.013041
dengamma value 1.060244
dengamma value 1.005727
dengamma value 1.068646
dengamma value 1.040111
dengamma value 1.060921
dengamma value 1.108569
dengamma value 1.003096
dengamma value 0.971634
dengamma value 1.071918
dengamma value 1.033737
dengamma value 1.032495
07/12/2016 14:28:30:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.09000332 * 5534; err = 0.29924106 * 5534; time = 0.8028s; samplesPerSecond = 6893.3
dengamma value 1.060171
dengamma value 0.992268
dengamma value 0.987584
dengamma value 1.006514
dengamma value 1.107882
dengamma value 0.951885
dengamma value 1.054534
dengamma value 1.047620
dengamma value 1.042817
dengamma value 1.046701
dengamma value 1.091808
dengamma value 1.085498
dengamma value 1.004214
dengamma value 1.074313
dengamma value 1.058818
dengamma value 1.029995
dengamma value 1.040182
dengamma value 0.979501
dengamma value 1.032872
dengamma value 1.014790
dengamma value 0.971282
dengamma value 1.064284
07/12/2016 14:28:31:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08164160 * 5936; err = 0.31367925 * 5936; time = 0.9265s; samplesPerSecond = 6407.1
dengamma value 0.923537
dengamma value 1.058190
dengamma value 1.059292
dengamma value 1.049381
dengamma value 1.035520
dengamma value 1.001077
dengamma value 1.064350
dengamma value 1.006067
dengamma value 1.014284
dengamma value 1.059981
dengamma value 0.904921
dengamma value 1.035259
dengamma value 1.133510
dengamma value 1.005524
dengamma value 0.991552
dengamma value 1.088628
dengamma value 1.079964
dengamma value 1.068613
dengamma value 1.069741
dengamma value 1.084421
dengamma value 1.074914
07/12/2016 14:28:32:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08312504 * 5248; err = 0.31478659 * 5248; time = 0.7316s; samplesPerSecond = 7172.9
dengamma value 1.022754
dengamma value 1.008857
dengamma value 1.008443
dengamma value 1.003566
dengamma value 1.056114
dengamma value 1.043045
dengamma value 1.003042
dengamma value 0.974277
dengamma value 1.045790
dengamma value 1.000672
dengamma value 1.094426
dengamma value 1.081282
dengamma value 0.977951
dengamma value 1.006474
dengamma value 1.073809
dengamma value 1.010193
dengamma value 1.075449
dengamma value 1.090083
dengamma value 1.044400
dengamma value 1.029721
dengamma value 1.020805
dengamma value 1.043333
dengamma value 1.006636
dengamma value 1.050518
dengamma value 1.032097
dengamma value 1.056619
07/12/2016 14:28:33:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08355082 * 6888; err = 0.29761905 * 6888; time = 0.9820s; samplesPerSecond = 7014.2
dengamma value 1.050247
dengamma value 1.049871
dengamma value 1.013003
dengamma value 1.049427
dengamma value 0.983453
dengamma value 1.018271
dengamma value 1.006584
dengamma value 1.069380
dengamma value 1.051872
dengamma value 1.008574
dengamma value 1.027247
dengamma value 1.043990
dengamma value 1.059891
dengamma value 0.998788
dengamma value 1.069115
dengamma value 0.988200
dengamma value 0.998027
dengamma value 1.112964
dengamma value 1.027905
dengamma value 1.002195
dengamma value 1.002471
dengamma value 1.016301
dengamma value 0.951762
dengamma value 1.003819
07/12/2016 14:28:33:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08774738 * 6572; err = 0.32927572 * 6572; time = 0.8951s; samplesPerSecond = 7342.0
dengamma value 0.979479
dengamma value 1.068139
dengamma value 1.003699
dengamma value 1.056977
dengamma value 1.021838
dengamma value 0.999370
dengamma value 0.996631
dengamma value 1.030052
dengamma value 1.004891
dengamma value 0.992252
dengamma value 1.074785
dengamma value 1.029242
dengamma value 1.016686
dengamma value 1.005652
dengamma value 1.027453
dengamma value 1.003019
dengamma value 1.011197
dengamma value 1.077629
dengamma value 0.989975
dengamma value 1.047293
dengamma value 1.048959
dengamma value 1.125793
dengamma value 1.063761
dengamma value 0.999778
07/12/2016 14:28:34:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08744931 * 6622; err = 0.30141951 * 6622; time = 0.9778s; samplesPerSecond = 6772.5
dengamma value 1.034227
dengamma value 1.061248
dengamma value 1.008749
dengamma value 0.986420
dengamma value 1.026533
dengamma value 0.988458
dengamma value 0.946330
dengamma value 0.986231
dengamma value 0.925725
dengamma value 1.022002
dengamma value 1.061377
dengamma value 1.131266
dengamma value 1.070547
dengamma value 1.066178
dengamma value 1.012122
dengamma value 1.027633
dengamma value 0.990689
dengamma value 1.058072
dengamma value 1.010965
dengamma value 0.962556
dengamma value 1.023305
dengamma value 1.067911
dengamma value 1.026105
07/12/2016 14:28:35:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.09115039 * 5824; err = 0.30099588 * 5824; time = 0.7663s; samplesPerSecond = 7600.0
dengamma value 0.964473
dengamma value 1.025303
dengamma value 1.073139
dengamma value 0.974991
dengamma value 1.003476
dengamma value 1.079636
dengamma value 1.041012
dengamma value 1.129341
dengamma value 0.994892
07/12/2016 14:28:36: Finished Epoch[ 2 of 3]: [Training] ce = 0.08519769 * 81776; err = 0.31346605 * 81776; totalSamplesSeen = 164350; learningRatePerSample = 2e-06; epochTime=11.5998s
07/12/2016 14:28:36: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.2'

07/12/2016 14:28:36: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163922), data subset 0 of 1, with 1 datapasses

07/12/2016 14:28:36: Starting minibatch loop.
dengamma value 1.048384
dengamma value 1.051558
dengamma value 1.049713
dengamma value 1.063464
dengamma value 1.085899
dengamma value 1.005926
dengamma value 1.029272
dengamma value 0.989144
dengamma value 0.979321
dengamma value 0.999128
dengamma value 1.018461
dengamma value 1.020106
dengamma value 1.008192
dengamma value 0.991301
dengamma value 1.034675
dengamma value 1.120998
dengamma value 0.984444
dengamma value 1.096099
dengamma value 1.010394
dengamma value 1.073685
dengamma value 0.975953
dengamma value 1.033130
dengamma value 1.076034
07/12/2016 14:28:36:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08352455 * 5074; err = 0.32380765 * 5074; time = 0.7617s; samplesPerSecond = 6661.5
dengamma value 0.988379
dengamma value 1.072246
dengamma value 1.055444
dengamma value 1.055646
dengamma value 0.982152
dengamma value 0.973578
dengamma value 0.994665
dengamma value 0.973128
dengamma value 1.075379
dengamma value 1.051541
dengamma value 1.029807
dengamma value 1.019331
dengamma value 1.007272
dengamma value 1.000733
dengamma value 1.068532
dengamma value 1.066407
dengamma value 1.017376
dengamma value 1.029569
dengamma value 1.058637
dengamma value 0.933031
dengamma value 1.025085
dengamma value 1.007877
07/12/2016 14:28:37:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08364093 * 7136; err = 0.30689462 * 7136; time = 0.9640s; samplesPerSecond = 7402.8
dengamma value 1.080566
dengamma value 0.999342
dengamma value 0.956325
dengamma value 1.005005
dengamma value 1.052520
dengamma value 1.010600
dengamma value 1.134906
dengamma value 0.977904
dengamma value 1.014165
dengamma value 1.024560
dengamma value 1.070953
dengamma value 1.051638
dengamma value 1.083155
dengamma value 1.004681
dengamma value 1.063097
dengamma value 1.006967
dengamma value 1.010038
dengamma value 1.004955
dengamma value 1.062783
dengamma value 1.091564
dengamma value 1.002004
dengamma value 0.997777
dengamma value 0.983557
07/12/2016 14:28:38:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.09537808 * 5504; err = 0.31013808 * 5504; time = 0.8053s; samplesPerSecond = 6834.9
dengamma value 1.058348
dengamma value 1.030138
dengamma value 1.025858
dengamma value 1.053928
dengamma value 1.054512
dengamma value 0.986938
dengamma value 1.055399
dengamma value 1.069551
dengamma value 0.993995
dengamma value 1.121059
dengamma value 1.007708
dengamma value 1.037802
dengamma value 1.049902
dengamma value 1.080870
dengamma value 0.960291
dengamma value 1.073815
dengamma value 0.997079
dengamma value 1.021823
dengamma value 0.912773
dengamma value 1.153425
dengamma value 1.031745
07/12/2016 14:28:39:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08441664 * 6028; err = 0.31320504 * 6028; time = 0.8736s; samplesPerSecond = 6900.0
dengamma value 1.034851
dengamma value 1.066263
dengamma value 1.011693
dengamma value 1.044450
dengamma value 1.002375
dengamma value 0.978814
dengamma value 1.029983
dengamma value 1.004322
dengamma value 1.024800
dengamma value 1.075050
dengamma value 1.048464
dengamma value 1.031091
dengamma value 1.042516
dengamma value 1.052250
dengamma value 1.051102
dengamma value 1.014289
dengamma value 0.997382
dengamma value 1.123612
dengamma value 1.016660
dengamma value 1.018105
dengamma value 1.019208
07/12/2016 14:28:40:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08660844 * 6028; err = 0.31320504 * 6028; time = 0.8136s; samplesPerSecond = 7409.3
dengamma value 1.028414
dengamma value 1.028823
dengamma value 0.982867
dengamma value 0.985429
dengamma value 1.030468
dengamma value 1.079012
dengamma value 1.084616
dengamma value 1.058541
dengamma value 0.950657
dengamma value 1.145611
dengamma value 0.987736
dengamma value 0.982940
dengamma value 1.056817
dengamma value 1.037662
dengamma value 1.034591
dengamma value 1.060480
dengamma value 0.979617
dengamma value 0.980182
dengamma value 1.062362
dengamma value 1.002928
dengamma value 1.046397
dengamma value 1.012777
dengamma value 1.005189
dengamma value 1.014118
07/12/2016 14:28:41:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08629550 * 6782; err = 0.31023297 * 6782; time = 0.9159s; samplesPerSecond = 7404.8
dengamma value 1.057211
dengamma value 1.049733
dengamma value 1.080247
dengamma value 1.031345
dengamma value 1.067163
dengamma value 1.034545
dengamma value 1.044548
dengamma value 1.098292
dengamma value 1.043419
dengamma value 0.996025
dengamma value 1.048405
dengamma value 1.083324
dengamma value 0.969364
dengamma value 1.130983
dengamma value 1.090772
dengamma value 0.989126
dengamma value 1.027759
dengamma value 1.077720
dengamma value 1.037588
dengamma value 0.961417
dengamma value 1.008913
07/12/2016 14:28:42:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08128527 * 5458; err = 0.28343716 * 5458; time = 0.8282s; samplesPerSecond = 6590.1
dengamma value 1.050454
dengamma value 1.008855
dengamma value 1.047926
dengamma value 1.061420
dengamma value 1.144371
dengamma value 1.088023
dengamma value 0.974333
dengamma value 1.077998
dengamma value 1.054920
dengamma value 1.030973
dengamma value 1.048350
dengamma value 1.059941
dengamma value 1.126182
dengamma value 1.129800
dengamma value 1.023074
dengamma value 1.054952
dengamma value 0.992964
dengamma value 1.005004
dengamma value 0.983745
dengamma value 1.085903
dengamma value 1.066330
dengamma value 1.084766
dengamma value 1.085072
dengamma value 1.095408
dengamma value 1.016242
07/12/2016 14:28:43:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08121447 * 6610; err = 0.28532526 * 6610; time = 0.9332s; samplesPerSecond = 7083.4
dengamma value 1.066075
dengamma value 1.052003
dengamma value 1.097564
dengamma value 1.061436
dengamma value 1.061136
dengamma value 0.961077
dengamma value 1.013819
dengamma value 1.075384
dengamma value 1.090218
dengamma value 1.045090
dengamma value 1.017937
dengamma value 0.936395
dengamma value 1.112966
dengamma value 1.076166
dengamma value 0.999845
dengamma value 1.042002
dengamma value 0.895639
dengamma value 1.029094
dengamma value 1.092818
dengamma value 1.028777
dengamma value 1.025314
dengamma value 1.062184
dengamma value 1.024334
07/12/2016 14:28:43:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08680930 * 5854; err = 0.29552443 * 5854; time = 0.7812s; samplesPerSecond = 7493.2
dengamma value 1.026321
dengamma value 1.099002
dengamma value 1.073585
dengamma value 0.872636
dengamma value 1.012932
dengamma value 1.062504
dengamma value 0.963464
dengamma value 1.066350
dengamma value 0.990668
dengamma value 1.025045
dengamma value 1.006481
dengamma value 1.006446
dengamma value 1.087044
dengamma value 1.081368
dengamma value 1.130388
dengamma value 1.050253
dengamma value 1.041936
dengamma value 1.081586
dengamma value 1.015194
dengamma value 0.964308
dengamma value 1.028773
dengamma value 1.053595
dengamma value 0.985099
07/12/2016 14:28:44:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08973218 * 4674; err = 0.32627300 * 4674; time = 0.6694s; samplesPerSecond = 6982.4
dengamma value 0.993347
dengamma value 1.024956
dengamma value 1.053040
dengamma value 1.050923
dengamma value 1.024427
dengamma value 1.066900
dengamma value 1.032569
dengamma value 0.990498
dengamma value 1.104198
dengamma value 1.021795
dengamma value 1.023562
dengamma value 1.038330
dengamma value 1.049545
dengamma value 0.971939
dengamma value 0.906126
dengamma value 0.961065
dengamma value 1.002619
dengamma value 1.039097
dengamma value 1.020017
dengamma value 0.962557
dengamma value 1.079176
07/12/2016 14:28:45:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08304400 * 6248; err = 0.31306018 * 6248; time = 0.8601s; samplesPerSecond = 7263.9
dengamma value 0.951977
dengamma value 1.048675
dengamma value 1.030409
dengamma value 1.060669
dengamma value 1.099588
dengamma value 1.000691
dengamma value 1.039731
dengamma value 1.053186
dengamma value 1.095736
dengamma value 0.994640
dengamma value 0.992169
dengamma value 0.988555
dengamma value 1.039230
dengamma value 1.020074
dengamma value 0.961378
dengamma value 1.077723
dengamma value 1.076507
dengamma value 1.015245
dengamma value 1.046102
dengamma value 1.095038
dengamma value 1.079587
dengamma value 1.038384
dengamma value 1.049211
07/12/2016 14:28:46:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08047197 * 7094; err = 0.29517902 * 7094; time = 0.9824s; samplesPerSecond = 7221.2
dengamma value 1.042526
dengamma value 1.062204
dengamma value 1.018432
dengamma value 1.040195
dengamma value 1.032750
dengamma value 1.038533
dengamma value 1.045912
dengamma value 0.973022
dengamma value 0.971991
dengamma value 1.058504
dengamma value 1.062277
dengamma value 0.992036
dengamma value 1.005152
dengamma value 1.080352
dengamma value 0.986575
dengamma value 1.066985
dengamma value 1.049958
dengamma value 1.036242
dengamma value 1.009788
dengamma value 1.025691
dengamma value 1.028109
dengamma value 0.987054
07/12/2016 14:28:47:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08733660 * 6246; err = 0.30131284 * 6246; time = 0.8907s; samplesPerSecond = 7012.6
dengamma value 0.991328
dengamma value 1.034478
dengamma value 1.023219
dengamma value 1.056987
dengamma value 1.037826
dengamma value 0.962611
dengamma value 0.988899
dengamma value 1.124784
dengamma value 1.040453
dengamma value 0.974041
dengamma value 0.939866
dengamma value 1.028158
dengamma value 1.028158
07/12/2016 14:28:47: Finished Epoch[ 3 of 3]: [Training] ce = 0.08547504 * 81970; err = 0.30630719 * 81970; totalSamplesSeen = 246320; learningRatePerSample = 2e-06; epochTime=11.535s
07/12/2016 14:28:47: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence'
07/12/2016 14:28:47: CNTKCommandTrainEnd: sequenceTrain

07/12/2016 14:28:47: Action "train" complete.

07/12/2016 14:28:47: __COMPLETED__