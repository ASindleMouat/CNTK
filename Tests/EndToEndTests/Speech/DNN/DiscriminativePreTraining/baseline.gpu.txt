CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 12 2016 13:55:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
07/12/2016 14:18:22: -------------------------------------------------------------------
07/12/2016 14:18:22: Build info: 

07/12/2016 14:18:22: 		Built time: Jul 12 2016 13:55:00
07/12/2016 14:18:22: 		Last modified date: Tue Jul 12 04:28:35 2016
07/12/2016 14:18:22: 		Build type: debug
07/12/2016 14:18:22: 		Build target: GPU
07/12/2016 14:18:22: 		With 1bit-SGD: no
07/12/2016 14:18:22: 		Math lib: mkl
07/12/2016 14:18:22: 		CUDA_PATH: /usr/local/cuda-7.5
07/12/2016 14:18:22: 		CUB_PATH: /usr/local/cub-1.4.1
07/12/2016 14:18:22: 		CUDNN_PATH: /usr/local/cudnn-4.0
07/12/2016 14:18:22: 		Build Branch: HEAD
07/12/2016 14:18:22: 		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
07/12/2016 14:18:22: 		Built by philly on 2bc22072e267
07/12/2016 14:18:22: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
07/12/2016 14:18:22: -------------------------------------------------------------------
07/12/2016 14:18:23: -------------------------------------------------------------------
07/12/2016 14:18:23: GPU info:

07/12/2016 14:18:23: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:18:23: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:18:23: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:18:23: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/12/2016 14:18:23: -------------------------------------------------------------------

07/12/2016 14:18:23: Running on localhost at 2016/07/12 14:18:23
07/12/2016 14:18:23: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining  OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu  DeviceId=0  timestamping=true



07/12/2016 14:18:23: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/12/2016 14:18:23: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu
DeviceId=0
timestamping=true

07/12/2016 14:18:23: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/12/2016 14:18:23: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/12/2016 14:18:23: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu
DeviceId=0
timestamping=true

07/12/2016 14:18:23: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/12/2016 14:18:23: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
07/12/2016 14:18:23: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/12/2016 14:18:23: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
07/12/2016 14:18:23: Precision = "float"
07/12/2016 14:18:23: CNTKModelPath: /tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
07/12/2016 14:18:23: CNTKCommandTrainInfo: dptPre1 : 2
07/12/2016 14:18:23: CNTKModelPath: /tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
07/12/2016 14:18:23: CNTKCommandTrainInfo: dptPre2 : 2
07/12/2016 14:18:23: CNTKModelPath: /tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech
07/12/2016 14:18:23: CNTKCommandTrainInfo: speechTrain : 4
07/12/2016 14:18:23: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

07/12/2016 14:18:23: ##############################################################################
07/12/2016 14:18:23: #                                                                            #
07/12/2016 14:18:23: # Action "train"                                                             #
07/12/2016 14:18:23: #                                                                            #
07/12/2016 14:18:23: ##############################################################################

07/12/2016 14:18:23: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 14:18:23: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 14:18:23: Created model with 19 nodes on GPU 0.

07/12/2016 14:18:23: Training criterion node(s):
07/12/2016 14:18:23: 	ce = CrossEntropyWithSoftmax

07/12/2016 14:18:23: Evaluation criterion node(s):

07/12/2016 14:18:23: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0x7fc5e1f26328: {[err Value[1]] }
0x7fc5e1f27918: {[featNorm Value[363 x *]] }
0x7fc5e1f27e88: {[HL1.t Value[512 x *]] }
0x7fc5e1f28238: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
0x7fc5e1f28398: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
0x7fc5e1f28558: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
0x7fc5e1f28718: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
0x7fc5e1f29228: {[ce Gradient[1]] }
0x7fc5e1f293e8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
0x7fc5e1f295a8: {[OL.t Gradient[132 x 1 x *]] }
0x7fc5e1f29768: {[OL.b Gradient[132 x 1]] }
0x7fc5e2121088: {[features Value[363 x *]] }
0x7fc5e21405d8: {[scaledLogLikelihood Value[132 x 1 x *]] }
0x7fc5e2140798: {[ce Value[1]] }
0x7fc5e2140a98: {[logPrior Value[132 x 1]] }
0x7fc5e2228458: {[globalInvStd Value[363 x 1]] }
0x7fc5e2228e48: {[globalPrior Value[132 x 1]] }
0x7fc5e22297e8: {[HL1.W Value[512 x 363]] }
0x7fc5e222acc8: {[HL1.b Value[512 x 1]] }
0x7fc5e222b998: {[OL.W Value[132 x 512]] }
0x7fc5e222c798: {[OL.b Value[132 x 1]] }
0x7fc5e2cceec8: {[globalMean Value[363 x 1]] }
0x7fc5e2cd3bc8: {[labels Value[132 x *]] }

07/12/2016 14:18:23: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 14:18:23: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 14:18:24: Starting minibatch loop.
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.74183846 * 2560; err = 0.80195313 * 2560; time = 0.1743s; samplesPerSecond = 14685.1
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.91124763 * 2560; err = 0.70898438 * 2560; time = 0.0626s; samplesPerSecond = 40893.3
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.58015976 * 2560; err = 0.66640625 * 2560; time = 0.0632s; samplesPerSecond = 40517.9
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.27427139 * 2560; err = 0.58750000 * 2560; time = 0.0637s; samplesPerSecond = 40164.4
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.05503616 * 2560; err = 0.56093750 * 2560; time = 0.0627s; samplesPerSecond = 40804.6
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.91055145 * 2560; err = 0.52812500 * 2560; time = 0.0626s; samplesPerSecond = 40918.1
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.81562653 * 2560; err = 0.51171875 * 2560; time = 0.0635s; samplesPerSecond = 40285.1
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.68803253 * 2560; err = 0.48476562 * 2560; time = 0.0625s; samplesPerSecond = 40946.2
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.57382050 * 2560; err = 0.45429687 * 2560; time = 0.0654s; samplesPerSecond = 39165.3
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62090149 * 2560; err = 0.47304687 * 2560; time = 0.0632s; samplesPerSecond = 40487.8
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.59272461 * 2560; err = 0.47500000 * 2560; time = 0.0640s; samplesPerSecond = 40017.5
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51520386 * 2560; err = 0.44531250 * 2560; time = 0.0632s; samplesPerSecond = 40475.6
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.49181976 * 2560; err = 0.45039062 * 2560; time = 0.0631s; samplesPerSecond = 40555.1
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.53703613 * 2560; err = 0.44804688 * 2560; time = 0.0639s; samplesPerSecond = 40063.9
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.43095398 * 2560; err = 0.41640625 * 2560; time = 0.0626s; samplesPerSecond = 40901.8
07/12/2016 14:18:25:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.41503601 * 2560; err = 0.40078125 * 2560; time = 0.0626s; samplesPerSecond = 40871.1
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.38913574 * 2560; err = 0.41132812 * 2560; time = 0.0625s; samplesPerSecond = 40941.0
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.41207886 * 2560; err = 0.42226562 * 2560; time = 0.0637s; samplesPerSecond = 40178.3
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.39968262 * 2560; err = 0.40664062 * 2560; time = 0.0638s; samplesPerSecond = 40139.2
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42729187 * 2560; err = 0.42617187 * 2560; time = 0.0626s; samplesPerSecond = 40869.8
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.41336365 * 2560; err = 0.42343750 * 2560; time = 0.0635s; samplesPerSecond = 40287.0
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.33186951 * 2560; err = 0.39960937 * 2560; time = 0.0632s; samplesPerSecond = 40505.0
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.28581238 * 2560; err = 0.38710937 * 2560; time = 0.0630s; samplesPerSecond = 40660.1
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.34127502 * 2560; err = 0.40976563 * 2560; time = 0.0632s; samplesPerSecond = 40517.9
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.32666016 * 2560; err = 0.39726563 * 2560; time = 0.0636s; samplesPerSecond = 40263.0
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.21437378 * 2560; err = 0.37265625 * 2560; time = 0.0625s; samplesPerSecond = 40964.6
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23749695 * 2560; err = 0.37343750 * 2560; time = 0.0625s; samplesPerSecond = 40959.3
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29956665 * 2560; err = 0.39023438 * 2560; time = 0.0630s; samplesPerSecond = 40644.0
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.21198120 * 2560; err = 0.37382813 * 2560; time = 0.0628s; samplesPerSecond = 40745.5
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20528259 * 2560; err = 0.36718750 * 2560; time = 0.0625s; samplesPerSecond = 40960.0
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23613586 * 2560; err = 0.37343750 * 2560; time = 0.0624s; samplesPerSecond = 40996.7
07/12/2016 14:18:26:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.25615234 * 2560; err = 0.38164063 * 2560; time = 0.0585s; samplesPerSecond = 43760.7
07/12/2016 14:18:26: Finished Epoch[ 1 of 2]: [Training] ce = 1.62945061 * 81920; err = 0.46030273 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=3.00139s
07/12/2016 14:18:26: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech.1'

07/12/2016 14:18:26: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 14:18:26: Starting minibatch loop.
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.23230953 * 2560; err = 0.38320312 * 2560; time = 0.0645s; samplesPerSecond = 39671.5
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.20511341 * 2560; err = 0.37421875 * 2560; time = 0.0630s; samplesPerSecond = 40627.8
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.28783760 * 2560; err = 0.37421875 * 2560; time = 0.0639s; samplesPerSecond = 40086.4
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.22809334 * 2560; err = 0.37421875 * 2560; time = 0.0623s; samplesPerSecond = 41068.4
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.18090286 * 2560; err = 0.35468750 * 2560; time = 0.0622s; samplesPerSecond = 41172.8
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.28175354 * 2560; err = 0.37695312 * 2560; time = 0.0625s; samplesPerSecond = 40982.3
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.22251205 * 2560; err = 0.37382813 * 2560; time = 0.0634s; samplesPerSecond = 40374.1
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.17863007 * 2560; err = 0.36328125 * 2560; time = 0.0623s; samplesPerSecond = 41102.0
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.23061218 * 2560; err = 0.35742188 * 2560; time = 0.0625s; samplesPerSecond = 40985.6
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18048782 * 2560; err = 0.37578125 * 2560; time = 0.0631s; samplesPerSecond = 40598.2
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.19648056 * 2560; err = 0.35976562 * 2560; time = 0.0627s; samplesPerSecond = 40836.5
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18896942 * 2560; err = 0.35429688 * 2560; time = 0.0623s; samplesPerSecond = 41108.0
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.16628113 * 2560; err = 0.35937500 * 2560; time = 0.0623s; samplesPerSecond = 41111.9
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.12856445 * 2560; err = 0.35195312 * 2560; time = 0.0650s; samplesPerSecond = 39365.2
07/12/2016 14:18:27:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10083466 * 2560; err = 0.32617188 * 2560; time = 0.0627s; samplesPerSecond = 40837.8
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09875183 * 2560; err = 0.33906250 * 2560; time = 0.0627s; samplesPerSecond = 40834.6
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.18634949 * 2560; err = 0.35820313 * 2560; time = 0.0631s; samplesPerSecond = 40567.3
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.15709991 * 2560; err = 0.35195312 * 2560; time = 0.0629s; samplesPerSecond = 40668.5
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10971069 * 2560; err = 0.34960938 * 2560; time = 0.0626s; samplesPerSecond = 40926.6
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11317139 * 2560; err = 0.35000000 * 2560; time = 0.0625s; samplesPerSecond = 40980.3
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.08727722 * 2560; err = 0.32578125 * 2560; time = 0.0633s; samplesPerSecond = 40417.4
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.12296143 * 2560; err = 0.34101562 * 2560; time = 0.0625s; samplesPerSecond = 40937.7
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12966003 * 2560; err = 0.35078125 * 2560; time = 0.0650s; samplesPerSecond = 39381.0
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27489319 * 2560; err = 0.39257812 * 2560; time = 0.0652s; samplesPerSecond = 39279.5
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.17423401 * 2560; err = 0.35156250 * 2560; time = 0.0640s; samplesPerSecond = 39981.3
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13240051 * 2560; err = 0.35625000 * 2560; time = 0.0631s; samplesPerSecond = 40564.1
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.13792114 * 2560; err = 0.34335938 * 2560; time = 0.0630s; samplesPerSecond = 40612.4
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.13433228 * 2560; err = 0.33710937 * 2560; time = 0.0645s; samplesPerSecond = 39670.9
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.05835876 * 2560; err = 0.33710937 * 2560; time = 0.0640s; samplesPerSecond = 40027.5
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.09596558 * 2560; err = 0.33476563 * 2560; time = 0.0646s; samplesPerSecond = 39619.3
07/12/2016 14:18:28:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08180847 * 2560; err = 0.33242187 * 2560; time = 0.0641s; samplesPerSecond = 39953.2
07/12/2016 14:18:29:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06572876 * 2560; err = 0.33632812 * 2560; time = 0.0592s; samplesPerSecond = 43257.9
07/12/2016 14:18:29: Finished Epoch[ 2 of 2]: [Training] ce = 1.16156273 * 81920; err = 0.35460205 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=2.03107s
07/12/2016 14:18:29: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech'
07/12/2016 14:18:29: CNTKCommandTrainEnd: dptPre1

07/12/2016 14:18:29: Action "train" complete.


07/12/2016 14:18:29: ##############################################################################
07/12/2016 14:18:29: #                                                                            #
07/12/2016 14:18:29: # Action "edit"                                                              #
07/12/2016 14:18:29: #                                                                            #
07/12/2016 14:18:29: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 14:18:29: Action "edit" complete.


07/12/2016 14:18:29: ##############################################################################
07/12/2016 14:18:29: #                                                                            #
07/12/2016 14:18:29: # Action "train"                                                             #
07/12/2016 14:18:29: #                                                                            #
07/12/2016 14:18:29: ##############################################################################

07/12/2016 14:18:29: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 14:18:29: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 14:18:29: Loaded model with 24 nodes on GPU 0.

07/12/2016 14:18:29: Training criterion node(s):
07/12/2016 14:18:29: 	ce = CrossEntropyWithSoftmax

07/12/2016 14:18:29: Evaluation criterion node(s):

07/12/2016 14:18:29: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
0x7fc5d7c76438: {[globalMean Value[363 x 1]] }
0x7fc5d7ca7f48: {[HL2.W Value[512 x 512]] }
0x7fc5d7cb65f8: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
0x7fc5d7cb67b8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
0x7fc5d7cb6978: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
0x7fc5d7cd3288: {[ce Gradient[1]] }
0x7fc5d7cd3448: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
0x7fc5d7cdd808: {[ce Value[1]] }
0x7fc5d7cddb68: {[scaledLogLikelihood Value[132 x 1 x *3]] }
0x7fc5d7ceb248: {[featNorm Value[363 x *3]] }
0x7fc5d7ceb3f8: {[logPrior Value[132 x 1]] }
0x7fc5d7ceee08: {[HL1.b Value[512 x 1]] }
0x7fc5d7cf81e8: {[globalPrior Value[132 x 1]] }
0x7fc5d7cf8338: {[OL.t Gradient[132 x 1 x *3]] }
0x7fc5d7cf84f8: {[OL.b Gradient[132 x 1]] }
0x7fc5d7cfbf78: {[labels Value[132 x *3]] }
0x7fc5e21024e8: {[globalInvStd Value[363 x 1]] }
0x7fc5e211c1e8: {[err Value[1]] }
0x7fc5e21a3f98: {[OL.b Value[132 x 1]] }
0x7fc5e21a4b08: {[HL2.b Value[512 x 1]] }
0x7fc5e21c18e8: {[HL1.t Value[512 x *3]] }
0x7fc5e21c1af8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
0x7fc5e222a578: {[OL.W Value[132 x 512]] }
0x7fc5e50fae88: {[features Value[363 x *3]] }
0x7fc5e6e81f88: {[HL1.W Value[512 x 363]] }
0x7fc5e6eb8078: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
0x7fc5e6eb8238: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
0x7fc5e6eb83f8: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }

07/12/2016 14:18:29: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 14:18:29: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 14:18:30: Starting minibatch loop.
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 4.30124588 * 2560; err = 0.80703125 * 2560; time = 0.0872s; samplesPerSecond = 29362.5
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.75448074 * 2560; err = 0.69960937 * 2560; time = 0.0833s; samplesPerSecond = 30716.8
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.20926208 * 2560; err = 0.58515625 * 2560; time = 0.0823s; samplesPerSecond = 31102.7
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.88578110 * 2560; err = 0.50117188 * 2560; time = 0.0824s; samplesPerSecond = 31079.7
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.71906204 * 2560; err = 0.47773437 * 2560; time = 0.0830s; samplesPerSecond = 30850.8
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.60130463 * 2560; err = 0.44648437 * 2560; time = 0.0823s; samplesPerSecond = 31118.6
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.56077118 * 2560; err = 0.45000000 * 2560; time = 0.0826s; samplesPerSecond = 30989.0
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.47116547 * 2560; err = 0.42460938 * 2560; time = 0.0825s; samplesPerSecond = 31028.4
07/12/2016 14:18:30:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.38874512 * 2560; err = 0.40781250 * 2560; time = 0.0821s; samplesPerSecond = 31190.2
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41911163 * 2560; err = 0.42539063 * 2560; time = 0.0832s; samplesPerSecond = 30785.9
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.38730774 * 2560; err = 0.42148438 * 2560; time = 0.0822s; samplesPerSecond = 31148.9
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.36617889 * 2560; err = 0.41015625 * 2560; time = 0.0831s; samplesPerSecond = 30802.2
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.33381653 * 2560; err = 0.40781250 * 2560; time = 0.0831s; samplesPerSecond = 30812.6
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.39802246 * 2560; err = 0.40546875 * 2560; time = 0.0822s; samplesPerSecond = 31125.0
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.33336182 * 2560; err = 0.40195313 * 2560; time = 0.0823s; samplesPerSecond = 31110.6
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.33834229 * 2560; err = 0.40195313 * 2560; time = 0.0829s; samplesPerSecond = 30886.5
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.26663208 * 2560; err = 0.37578125 * 2560; time = 0.0822s; samplesPerSecond = 31143.6
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28086243 * 2560; err = 0.39296875 * 2560; time = 0.0827s; samplesPerSecond = 30960.5
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.29481506 * 2560; err = 0.39531250 * 2560; time = 0.0825s; samplesPerSecond = 31031.8
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.27625122 * 2560; err = 0.39375000 * 2560; time = 0.0822s; samplesPerSecond = 31139.4
07/12/2016 14:18:31:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.26905518 * 2560; err = 0.38984375 * 2560; time = 0.0832s; samplesPerSecond = 30767.4
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.21494751 * 2560; err = 0.36250000 * 2560; time = 0.0822s; samplesPerSecond = 31136.0
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.20699158 * 2560; err = 0.36914062 * 2560; time = 0.0822s; samplesPerSecond = 31144.3
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.25002136 * 2560; err = 0.37851563 * 2560; time = 0.0833s; samplesPerSecond = 30745.6
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.22617187 * 2560; err = 0.37656250 * 2560; time = 0.0822s; samplesPerSecond = 31129.9
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.14840393 * 2560; err = 0.35468750 * 2560; time = 0.0824s; samplesPerSecond = 31057.8
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.16649780 * 2560; err = 0.35468750 * 2560; time = 0.0827s; samplesPerSecond = 30965.7
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.22885742 * 2560; err = 0.36992188 * 2560; time = 0.0822s; samplesPerSecond = 31160.6
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.16533203 * 2560; err = 0.36484375 * 2560; time = 0.0828s; samplesPerSecond = 30921.6
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.17502136 * 2560; err = 0.35664062 * 2560; time = 0.0822s; samplesPerSecond = 31151.9
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.16159058 * 2560; err = 0.35195312 * 2560; time = 0.0822s; samplesPerSecond = 31133.3
07/12/2016 14:18:32:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.17113953 * 2560; err = 0.35429688 * 2560; time = 0.0782s; samplesPerSecond = 32757.1
07/12/2016 14:18:32: Finished Epoch[ 1 of 2]: [Training] ce = 1.49907970 * 81920; err = 0.42547607 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=3.51234s
07/12/2016 14:18:32: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.1'

07/12/2016 14:18:32: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 14:18:32: Starting minibatch loop.
07/12/2016 14:18:32:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.14215403 * 2560; err = 0.34882812 * 2560; time = 0.0837s; samplesPerSecond = 30597.1
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.17049246 * 2560; err = 0.36328125 * 2560; time = 0.0842s; samplesPerSecond = 30413.9
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.24373856 * 2560; err = 0.37460938 * 2560; time = 0.0836s; samplesPerSecond = 30624.2
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.18655586 * 2560; err = 0.36445312 * 2560; time = 0.0822s; samplesPerSecond = 31140.5
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.13848000 * 2560; err = 0.35039063 * 2560; time = 0.0829s; samplesPerSecond = 30872.8
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.21884155 * 2560; err = 0.36757812 * 2560; time = 0.0827s; samplesPerSecond = 30938.4
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14372940 * 2560; err = 0.35000000 * 2560; time = 0.0834s; samplesPerSecond = 30700.6
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.12769089 * 2560; err = 0.34960938 * 2560; time = 0.0837s; samplesPerSecond = 30590.9
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.14114227 * 2560; err = 0.33554688 * 2560; time = 0.0848s; samplesPerSecond = 30200.1
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12445145 * 2560; err = 0.34843750 * 2560; time = 0.0835s; samplesPerSecond = 30648.0
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.14137878 * 2560; err = 0.34101562 * 2560; time = 0.0824s; samplesPerSecond = 31071.4
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.12705154 * 2560; err = 0.33867188 * 2560; time = 0.0824s; samplesPerSecond = 31070.6
07/12/2016 14:18:33:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10779419 * 2560; err = 0.34531250 * 2560; time = 0.0830s; samplesPerSecond = 30846.3
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.07003021 * 2560; err = 0.32500000 * 2560; time = 0.0823s; samplesPerSecond = 31123.5
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.05308990 * 2560; err = 0.31406250 * 2560; time = 0.0822s; samplesPerSecond = 31143.6
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.06392975 * 2560; err = 0.33085938 * 2560; time = 0.0830s; samplesPerSecond = 30858.6
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.14430847 * 2560; err = 0.35507813 * 2560; time = 0.0821s; samplesPerSecond = 31184.5
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.14809570 * 2560; err = 0.35859375 * 2560; time = 0.0827s; samplesPerSecond = 30959.8
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.08184509 * 2560; err = 0.33515625 * 2560; time = 0.0819s; samplesPerSecond = 31242.0
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.07637024 * 2560; err = 0.33359375 * 2560; time = 0.0819s; samplesPerSecond = 31252.7
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.06249695 * 2560; err = 0.32500000 * 2560; time = 0.0846s; samplesPerSecond = 30272.6
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.09361877 * 2560; err = 0.33320312 * 2560; time = 0.0826s; samplesPerSecond = 31004.0
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12118683 * 2560; err = 0.34843750 * 2560; time = 0.0825s; samplesPerSecond = 31036.7
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.13457642 * 2560; err = 0.35195312 * 2560; time = 0.0830s; samplesPerSecond = 30860.5
07/12/2016 14:18:34:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.09024963 * 2560; err = 0.33984375 * 2560; time = 0.0823s; samplesPerSecond = 31105.7
07/12/2016 14:18:35:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07457275 * 2560; err = 0.33164063 * 2560; time = 0.0827s; samplesPerSecond = 30966.5
07/12/2016 14:18:35:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.05975952 * 2560; err = 0.32070312 * 2560; time = 0.0833s; samplesPerSecond = 30726.0
07/12/2016 14:18:35:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09778137 * 2560; err = 0.33242187 * 2560; time = 0.0830s; samplesPerSecond = 30846.3
07/12/2016 14:18:35:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.01963196 * 2560; err = 0.32539062 * 2560; time = 0.0835s; samplesPerSecond = 30642.9
07/12/2016 14:18:35:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.07533875 * 2560; err = 0.33515625 * 2560; time = 0.0823s; samplesPerSecond = 31106.5
07/12/2016 14:18:35:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06417236 * 2560; err = 0.33007812 * 2560; time = 0.0821s; samplesPerSecond = 31187.2
07/12/2016 14:18:35:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04990234 * 2560; err = 0.33359375 * 2560; time = 0.0777s; samplesPerSecond = 32961.2
07/12/2016 14:18:35: Finished Epoch[ 2 of 2]: [Training] ce = 1.11232681 * 81920; err = 0.34179688 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=2.65782s
07/12/2016 14:18:35: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech'
07/12/2016 14:18:35: CNTKCommandTrainEnd: dptPre2

07/12/2016 14:18:35: Action "train" complete.


07/12/2016 14:18:35: ##############################################################################
07/12/2016 14:18:35: #                                                                            #
07/12/2016 14:18:35: # Action "edit"                                                              #
07/12/2016 14:18:35: #                                                                            #
07/12/2016 14:18:35: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 14:18:35: Action "edit" complete.


07/12/2016 14:18:35: ##############################################################################
07/12/2016 14:18:35: #                                                                            #
07/12/2016 14:18:35: # Action "train"                                                             #
07/12/2016 14:18:35: #                                                                            #
07/12/2016 14:18:35: ##############################################################################

07/12/2016 14:18:35: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 14:18:35: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 14:18:35: Loaded model with 29 nodes on GPU 0.

07/12/2016 14:18:35: Training criterion node(s):
07/12/2016 14:18:35: 	ce = CrossEntropyWithSoftmax

07/12/2016 14:18:35: Evaluation criterion node(s):

07/12/2016 14:18:35: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0x7fc5d7c68778: {[HL3.b Value[512 x 1]] }
0x7fc5d7c68bd8: {[HL1.b Value[512 x 1]] }
0x7fc5d7c69168: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
0x7fc5d7c692c8: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
0x7fc5d7c69488: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
0x7fc5d7c9c8f8: {[HL2.b Value[512 x 1]] }
0x7fc5d7cab0a8: {[err Value[1]] }
0x7fc5d7cab268: {[scaledLogLikelihood Value[132 x 1 x *6]] }
0x7fc5d7cab428: {[ce Value[1]] }
0x7fc5d7cab6a8: {[HL1.t Value[512 x *6]] }
0x7fc5e0101ea8: {[labels Value[132 x *6]] }
0x7fc5e0102f38: {[HL1.W Value[512 x 363]] }
0x7fc5e0104a28: {[globalInvStd Value[363 x 1]] }
0x7fc5e217ca28: {[globalPrior Value[132 x 1]] }
0x7fc5e218b438: {[globalMean Value[363 x 1]] }
0x7fc5e218ccf8: {[features Value[363 x *6]] }
0x7fc5e218df08: {[logPrior Value[132 x 1]] }
0x7fc5e21923d8: {[OL.W Value[132 x 512]] }
0x7fc5e2192848: {[HL3.W Value[512 x 512]] }
0x7fc5e2193b08: {[HL2.W Value[512 x 512]] }
0x7fc5e2198858: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
0x7fc5e21a3a28: {[OL.b Value[132 x 1]] }
0x7fc5e21a4238: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
0x7fc5e21baeb8: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
0x7fc5e21bb018: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
0x7fc5e21bb1d8: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
0x7fc5e21bb398: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
0x7fc5e21bb558: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
0x7fc5e2ccf0b8: {[featNorm Value[363 x *6]] }
0x7fc5e6ec4358: {[ce Gradient[1]] }
0x7fc5e6ec4518: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0x7fc5e6ec46d8: {[OL.t Gradient[132 x 1 x *6]] }
0x7fc5e6ec4898: {[OL.b Gradient[132 x 1]] }

07/12/2016 14:18:35: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 14:18:35: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 14:18:36: Starting minibatch loop.
07/12/2016 14:18:36:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.97086372 * 2560; err = 0.81445312 * 2560; time = 0.1082s; samplesPerSecond = 23669.7
07/12/2016 14:18:36:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.63975792 * 2560; err = 0.63320312 * 2560; time = 0.1030s; samplesPerSecond = 24853.2
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.02565231 * 2560; err = 0.54257813 * 2560; time = 0.1034s; samplesPerSecond = 24764.7
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74204865 * 2560; err = 0.47500000 * 2560; time = 0.1034s; samplesPerSecond = 24753.9
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.58343964 * 2560; err = 0.45156250 * 2560; time = 0.1029s; samplesPerSecond = 24882.4
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.47893143 * 2560; err = 0.42343750 * 2560; time = 0.1034s; samplesPerSecond = 24750.3
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.43405457 * 2560; err = 0.40898438 * 2560; time = 0.1023s; samplesPerSecond = 25024.7
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35973663 * 2560; err = 0.39648438 * 2560; time = 0.1026s; samplesPerSecond = 24963.4
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.28108978 * 2560; err = 0.37968750 * 2560; time = 0.1021s; samplesPerSecond = 25061.2
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.29773560 * 2560; err = 0.39765625 * 2560; time = 0.1028s; samplesPerSecond = 24912.4
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.28441925 * 2560; err = 0.39062500 * 2560; time = 0.1020s; samplesPerSecond = 25094.8
07/12/2016 14:18:37:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27777252 * 2560; err = 0.38164063 * 2560; time = 0.1026s; samplesPerSecond = 24957.1
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.23615112 * 2560; err = 0.37421875 * 2560; time = 0.1023s; samplesPerSecond = 25025.4
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.31171112 * 2560; err = 0.38671875 * 2560; time = 0.1043s; samplesPerSecond = 24543.4
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.25573883 * 2560; err = 0.37773438 * 2560; time = 0.1031s; samplesPerSecond = 24829.1
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.27382965 * 2560; err = 0.38398437 * 2560; time = 0.1034s; samplesPerSecond = 24760.4
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.20634155 * 2560; err = 0.36406250 * 2560; time = 0.1036s; samplesPerSecond = 24717.6
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.20973816 * 2560; err = 0.36562500 * 2560; time = 0.1029s; samplesPerSecond = 24876.8
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.20688782 * 2560; err = 0.36718750 * 2560; time = 0.1034s; samplesPerSecond = 24764.2
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.20260315 * 2560; err = 0.37226562 * 2560; time = 0.1029s; samplesPerSecond = 24883.4
07/12/2016 14:18:38:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.20553894 * 2560; err = 0.37187500 * 2560; time = 0.1032s; samplesPerSecond = 24800.7
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.14160156 * 2560; err = 0.34726563 * 2560; time = 0.1033s; samplesPerSecond = 24775.0
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.15316467 * 2560; err = 0.35273437 * 2560; time = 0.1029s; samplesPerSecond = 24866.7
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.19352417 * 2560; err = 0.35468750 * 2560; time = 0.1023s; samplesPerSecond = 25022.7
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.17192078 * 2560; err = 0.35937500 * 2560; time = 0.1027s; samplesPerSecond = 24924.1
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.08281860 * 2560; err = 0.33867188 * 2560; time = 0.1020s; samplesPerSecond = 25097.8
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.11028442 * 2560; err = 0.34453125 * 2560; time = 0.1023s; samplesPerSecond = 25018.6
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.17454224 * 2560; err = 0.35312500 * 2560; time = 0.1026s; samplesPerSecond = 24947.6
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.11068115 * 2560; err = 0.34531250 * 2560; time = 0.1019s; samplesPerSecond = 25114.5
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.12955627 * 2560; err = 0.34296875 * 2560; time = 0.1027s; samplesPerSecond = 24936.4
07/12/2016 14:18:39:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12482300 * 2560; err = 0.34570312 * 2560; time = 0.1020s; samplesPerSecond = 25110.1
07/12/2016 14:18:40:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12771912 * 2560; err = 0.34453125 * 2560; time = 0.0980s; samplesPerSecond = 26125.1
07/12/2016 14:18:40: Finished Epoch[ 1 of 4]: [Training] ce = 1.40639620 * 81920; err = 0.40274658 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=4.16083s
07/12/2016 14:18:40: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.1'

07/12/2016 14:18:40: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 14:18:40: Starting minibatch loop.
07/12/2016 14:18:40:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.51739798 * 5120; err = 0.41425781 * 5120; time = 0.1482s; samplesPerSecond = 34536.3
07/12/2016 14:18:40:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.25793447 * 5120; err = 0.37539062 * 5120; time = 0.1431s; samplesPerSecond = 35789.9
07/12/2016 14:18:40:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.18638287 * 5120; err = 0.36718750 * 5120; time = 0.1431s; samplesPerSecond = 35784.7
07/12/2016 14:18:40:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.12794571 * 5120; err = 0.34218750 * 5120; time = 0.1453s; samplesPerSecond = 35240.6
07/12/2016 14:18:40:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14070625 * 5120; err = 0.34570312 * 5120; time = 0.1438s; samplesPerSecond = 35611.7
07/12/2016 14:18:40:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.14582825 * 5120; err = 0.34765625 * 5120; time = 0.1446s; samplesPerSecond = 35408.3
07/12/2016 14:18:41:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.11193542 * 5120; err = 0.34414062 * 5120; time = 0.1437s; samplesPerSecond = 35641.2
07/12/2016 14:18:41:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08574600 * 5120; err = 0.33789062 * 5120; time = 0.1430s; samplesPerSecond = 35794.9
07/12/2016 14:18:41:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.21058807 * 5120; err = 0.37363281 * 5120; time = 0.1435s; samplesPerSecond = 35682.4
07/12/2016 14:18:41:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.09668579 * 5120; err = 0.34335938 * 5120; time = 0.1433s; samplesPerSecond = 35718.3
07/12/2016 14:18:41:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05845032 * 5120; err = 0.32675781 * 5120; time = 0.1430s; samplesPerSecond = 35812.7
07/12/2016 14:18:41:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.10728455 * 5120; err = 0.34726563 * 5120; time = 0.1435s; samplesPerSecond = 35682.4
07/12/2016 14:18:42:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08716888 * 5120; err = 0.33593750 * 5120; time = 0.1431s; samplesPerSecond = 35783.7
07/12/2016 14:18:42:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.06778870 * 5120; err = 0.31855469 * 5120; time = 0.1433s; samplesPerSecond = 35735.5
07/12/2016 14:18:42:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04079590 * 5120; err = 0.32910156 * 5120; time = 0.1441s; samplesPerSecond = 35520.3
07/12/2016 14:18:42:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06249542 * 5120; err = 0.32968750 * 5120; time = 0.1326s; samplesPerSecond = 38616.4
07/12/2016 14:18:42: Finished Epoch[ 2 of 4]: [Training] ce = 1.14407091 * 81920; err = 0.34866943 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=2.30717s
07/12/2016 14:18:42: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.2'

07/12/2016 14:18:42: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

07/12/2016 14:18:42: Starting minibatch loop.
07/12/2016 14:18:42:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.11238871 * 5120; err = 0.34804687 * 5120; time = 0.1438s; samplesPerSecond = 35608.0
07/12/2016 14:18:42:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.09456148 * 5120; err = 0.34121094 * 5120; time = 0.1435s; samplesPerSecond = 35681.9
07/12/2016 14:18:42:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.10800076 * 5120; err = 0.34667969 * 5120; time = 0.1433s; samplesPerSecond = 35737.5
07/12/2016 14:18:43:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.16617966 * 5120; err = 0.35566406 * 5120; time = 0.1428s; samplesPerSecond = 35852.6
07/12/2016 14:18:43:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14173546 * 5120; err = 0.34550781 * 5120; time = 0.1439s; samplesPerSecond = 35587.9
07/12/2016 14:18:43:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07876053 * 5120; err = 0.33359375 * 5120; time = 0.1435s; samplesPerSecond = 35687.2
07/12/2016 14:18:43:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08043213 * 5120; err = 0.33437500 * 5120; time = 0.1433s; samplesPerSecond = 35732.5
07/12/2016 14:18:43:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07423630 * 5120; err = 0.33007812 * 5120; time = 0.1434s; samplesPerSecond = 35702.3
07/12/2016 14:18:43:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.02659454 * 5120; err = 0.31113281 * 5120; time = 0.1431s; samplesPerSecond = 35771.4
07/12/2016 14:18:43:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.04602737 * 5120; err = 0.31855469 * 5120; time = 0.1434s; samplesPerSecond = 35694.9
07/12/2016 14:18:44:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05524902 * 5120; err = 0.33613281 * 5120; time = 0.1437s; samplesPerSecond = 35625.3
07/12/2016 14:18:44:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07627411 * 5120; err = 0.33613281 * 5120; time = 0.1429s; samplesPerSecond = 35818.2
07/12/2016 14:18:44:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05101776 * 5120; err = 0.31660156 * 5120; time = 0.1440s; samplesPerSecond = 35560.5
07/12/2016 14:18:44:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.03016815 * 5120; err = 0.32480469 * 5120; time = 0.1438s; samplesPerSecond = 35605.7
07/12/2016 14:18:44:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04644623 * 5120; err = 0.32929687 * 5120; time = 0.1429s; samplesPerSecond = 35823.2
07/12/2016 14:18:44:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02751465 * 5120; err = 0.32265625 * 5120; time = 0.1339s; samplesPerSecond = 38239.5
07/12/2016 14:18:44: Finished Epoch[ 3 of 4]: [Training] ce = 1.07597418 * 81920; err = 0.33315430 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=2.30067s
07/12/2016 14:18:44: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.3'

07/12/2016 14:18:44: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

07/12/2016 14:18:44: Starting minibatch loop.
07/12/2016 14:18:45:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.03003817 * 5120; err = 0.31289062 * 5120; time = 0.1447s; samplesPerSecond = 35385.8
07/12/2016 14:18:45:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.04137832 * 4926; err = 0.32480715 * 4926; time = 0.4373s; samplesPerSecond = 11264.1
07/12/2016 14:18:45:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.03293953 * 5120; err = 0.33046875 * 5120; time = 0.1433s; samplesPerSecond = 35731.7
07/12/2016 14:18:45:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.00674477 * 5120; err = 0.31738281 * 5120; time = 0.1436s; samplesPerSecond = 35661.8
07/12/2016 14:18:45:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.02883911 * 5120; err = 0.31757812 * 5120; time = 0.1437s; samplesPerSecond = 35621.4
07/12/2016 14:18:46:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 0.97552528 * 5120; err = 0.30195312 * 5120; time = 0.1430s; samplesPerSecond = 35815.5
07/12/2016 14:18:46:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.99455070 * 5120; err = 0.31171875 * 5120; time = 0.1445s; samplesPerSecond = 35423.0
07/12/2016 14:18:46:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.02535477 * 5120; err = 0.31835938 * 5120; time = 0.1439s; samplesPerSecond = 35583.7
07/12/2016 14:18:46:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99763412 * 5120; err = 0.30800781 * 5120; time = 0.1431s; samplesPerSecond = 35781.7
07/12/2016 14:18:46:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.98428116 * 5120; err = 0.31269531 * 5120; time = 0.1438s; samplesPerSecond = 35596.8
07/12/2016 14:18:46:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.03535004 * 5120; err = 0.31992188 * 5120; time = 0.1438s; samplesPerSecond = 35603.3
07/12/2016 14:18:46:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.00379181 * 5120; err = 0.31523438 * 5120; time = 0.1432s; samplesPerSecond = 35762.2
07/12/2016 14:18:47:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.98212585 * 5120; err = 0.29453125 * 5120; time = 0.1438s; samplesPerSecond = 35597.3
07/12/2016 14:18:47:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02532196 * 5120; err = 0.32128906 * 5120; time = 0.1450s; samplesPerSecond = 35313.8
07/12/2016 14:18:47:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.98880615 * 5120; err = 0.32148437 * 5120; time = 0.1432s; samplesPerSecond = 35742.7
07/12/2016 14:18:47:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.99748993 * 5120; err = 0.31035156 * 5120; time = 0.1367s; samplesPerSecond = 37453.5
07/12/2016 14:18:47: Finished Epoch[ 4 of 4]: [Training] ce = 1.00940647 * 81920; err = 0.31486816 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=2.60971s
07/12/2016 14:18:47: SGD: Saving checkpoint model '/tmp/cntk-test-20160712141820.956525/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech'
07/12/2016 14:18:47: CNTKCommandTrainEnd: speechTrain

07/12/2016 14:18:47: Action "train" complete.

07/12/2016 14:18:47: __COMPLETED__