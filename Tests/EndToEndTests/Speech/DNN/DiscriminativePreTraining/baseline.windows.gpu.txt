CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3565 @ 3.20GHz
    Hardware threads: 8
    Total Memory: 12580436 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/debug/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 12 2016 06:34:05
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
07/12/2016 07:24:57: -------------------------------------------------------------------
07/12/2016 07:24:57: Build info: 

07/12/2016 07:24:57: 		Built time: Jul 12 2016 06:34:05
07/12/2016 07:24:57: 		Last modified date: Fri Jul  8 10:29:45 2016
07/12/2016 07:24:57: 		Build type: Debug
07/12/2016 07:24:57: 		Build target: GPU
07/12/2016 07:24:57: 		With 1bit-SGD: no
07/12/2016 07:24:57: 		Math lib: mkl
07/12/2016 07:24:57: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
07/12/2016 07:24:57: 		CUB_PATH: C:\src\cub-1.4.1
07/12/2016 07:24:57: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
07/12/2016 07:24:57: 		Build Branch: HEAD
07/12/2016 07:24:57: 		Build SHA1: 906792b54561dcf6226f205d6edea86e04842f47
07/12/2016 07:24:57: 		Built by svcphil on liana-08-w
07/12/2016 07:24:57: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
07/12/2016 07:24:57: -------------------------------------------------------------------
07/12/2016 07:25:11: -------------------------------------------------------------------
07/12/2016 07:25:11: GPU info:

07/12/2016 07:25:11: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
07/12/2016 07:25:11: -------------------------------------------------------------------

07/12/2016 07:25:11: Running on cntk-muc01 at 2016/07/12 07:25:11
07/12/2016 07:25:11: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu  DeviceId=0  timestamping=true



07/12/2016 07:25:11: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/12/2016 07:25:11: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu
DeviceId=0
timestamping=true

07/12/2016 07:25:11: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/12/2016 07:25:11: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/12/2016 07:25:11: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu
DeviceId=0
timestamping=true

07/12/2016 07:25:11: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/12/2016 07:25:11: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
07/12/2016 07:25:11: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/12/2016 07:25:11: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
07/12/2016 07:25:11: Precision = "float"
07/12/2016 07:25:11: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
07/12/2016 07:25:11: CNTKCommandTrainInfo: dptPre1 : 2
07/12/2016 07:25:11: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
07/12/2016 07:25:11: CNTKCommandTrainInfo: dptPre2 : 2
07/12/2016 07:25:11: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech
07/12/2016 07:25:11: CNTKCommandTrainInfo: speechTrain : 4
07/12/2016 07:25:11: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

07/12/2016 07:25:11: ##############################################################################
07/12/2016 07:25:11: #                                                                            #
07/12/2016 07:25:11: # Action "train"                                                             #
07/12/2016 07:25:11: #                                                                            #
07/12/2016 07:25:11: ##############################################################################

07/12/2016 07:25:11: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 07:25:13: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 07:25:14: Created model with 19 nodes on GPU 0.

07/12/2016 07:25:14: Training criterion node(s):
07/12/2016 07:25:14: 	ce = CrossEntropyWithSoftmax

07/12/2016 07:25:14: Evaluation criterion node(s):

07/12/2016 07:25:14: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0000001208E70EB0: {[features Value[363 x *]] }
000000123269BAE0: {[globalMean Value[363 x 1]] }
000000123269BBB0: {[OL.b Value[132 x 1]] }
000000123269BC80: {[OL.W Value[132 x 512]] }
000000123269BD50: {[HL1.W Value[512 x 363]] }
000000123269BE20: {[err Value[1]] }
000000123269BEF0: {[globalInvStd Value[363 x 1]] }
000000123269BFC0: {[globalPrior Value[132 x 1]] }
000000123269C090: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
000000123269C570: {[ce Gradient[1]] }
000000123269C640: {[scaledLogLikelihood Value[132 x 1 x *]] }
000000123269C710: {[ce Value[1]] }
000000123269C7E0: {[featNorm Value[363 x *]] }
000000123269C8B0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
000000123269C980: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
000000123269CA50: {[OL.t Gradient[132 x 1 x *]] }
000000123269CE60: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
000000123269D1A0: {[OL.b Gradient[132 x 1]] }
000000123269D270: {[labels Value[132 x *]] }
000000123269D4E0: {[HL1.b Value[512 x 1]] }
000000123269D5B0: {[HL1.t Value[512 x *]] }
000000123269D750: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
000000123269D820: {[logPrior Value[132 x 1]] }

07/12/2016 07:25:14: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 07:25:14: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 07:25:18: Starting minibatch loop.
07/12/2016 07:25:18:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.82139473 * 2560; err = 0.83359375 * 2560; time = 0.3855s; samplesPerSecond = 6640.7
07/12/2016 07:25:18:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.94043388 * 2560; err = 0.72187500 * 2560; time = 0.1557s; samplesPerSecond = 16440.9
07/12/2016 07:25:19:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.51675339 * 2560; err = 0.65664062 * 2560; time = 0.1542s; samplesPerSecond = 16600.0
07/12/2016 07:25:19:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.26993408 * 2560; err = 0.61562500 * 2560; time = 0.1543s; samplesPerSecond = 16589.2
07/12/2016 07:25:19:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.01949539 * 2560; err = 0.55781250 * 2560; time = 0.1541s; samplesPerSecond = 16612.1
07/12/2016 07:25:19:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.89216309 * 2560; err = 0.52734375 * 2560; time = 0.1556s; samplesPerSecond = 16455.8
07/12/2016 07:25:19:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.85436096 * 2560; err = 0.52539063 * 2560; time = 0.1559s; samplesPerSecond = 16422.3
07/12/2016 07:25:19:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.71935120 * 2560; err = 0.50273437 * 2560; time = 0.1572s; samplesPerSecond = 16284.6
07/12/2016 07:25:19:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.61281128 * 2560; err = 0.47304687 * 2560; time = 0.1566s; samplesPerSecond = 16349.1
07/12/2016 07:25:20:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.55461884 * 2560; err = 0.45703125 * 2560; time = 0.1566s; samplesPerSecond = 16349.9
07/12/2016 07:25:20:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.56578522 * 2560; err = 0.45742187 * 2560; time = 0.1566s; samplesPerSecond = 16345.4
07/12/2016 07:25:20:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.56222534 * 2560; err = 0.46875000 * 2560; time = 0.1566s; samplesPerSecond = 16347.5
07/12/2016 07:25:20:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.48975220 * 2560; err = 0.44375000 * 2560; time = 0.1566s; samplesPerSecond = 16346.8
07/12/2016 07:25:20:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.51242676 * 2560; err = 0.45273438 * 2560; time = 0.1565s; samplesPerSecond = 16358.0
07/12/2016 07:25:20:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46072083 * 2560; err = 0.42968750 * 2560; time = 0.1564s; samplesPerSecond = 16370.0
07/12/2016 07:25:21:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.42187805 * 2560; err = 0.42343750 * 2560; time = 0.1567s; samplesPerSecond = 16341.0
07/12/2016 07:25:21:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.40252686 * 2560; err = 0.42656250 * 2560; time = 0.1566s; samplesPerSecond = 16351.7
07/12/2016 07:25:21:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.36772766 * 2560; err = 0.40625000 * 2560; time = 0.1568s; samplesPerSecond = 16322.8
07/12/2016 07:25:21:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.32636719 * 2560; err = 0.40429688 * 2560; time = 0.1565s; samplesPerSecond = 16360.0
07/12/2016 07:25:21:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.34658813 * 2560; err = 0.39531250 * 2560; time = 0.1542s; samplesPerSecond = 16600.2
07/12/2016 07:25:21:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.37981262 * 2560; err = 0.41718750 * 2560; time = 0.1545s; samplesPerSecond = 16574.9
07/12/2016 07:25:21:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.30067749 * 2560; err = 0.39062500 * 2560; time = 0.1562s; samplesPerSecond = 16388.9
07/12/2016 07:25:22:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.34448547 * 2560; err = 0.40429688 * 2560; time = 0.1555s; samplesPerSecond = 16464.6
07/12/2016 07:25:22:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.34487000 * 2560; err = 0.40742187 * 2560; time = 0.1565s; samplesPerSecond = 16354.0
07/12/2016 07:25:22:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.33879700 * 2560; err = 0.39609375 * 2560; time = 0.1576s; samplesPerSecond = 16247.9
07/12/2016 07:25:22:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.30860291 * 2560; err = 0.39257813 * 2560; time = 0.1545s; samplesPerSecond = 16569.6
07/12/2016 07:25:22:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.28979187 * 2560; err = 0.37265625 * 2560; time = 0.1542s; samplesPerSecond = 16599.3
07/12/2016 07:25:22:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.28291931 * 2560; err = 0.39296875 * 2560; time = 0.1543s; samplesPerSecond = 16593.4
07/12/2016 07:25:23:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.24534607 * 2560; err = 0.37539062 * 2560; time = 0.1543s; samplesPerSecond = 16595.6
07/12/2016 07:25:23:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.21101685 * 2560; err = 0.36562500 * 2560; time = 0.1561s; samplesPerSecond = 16397.4
07/12/2016 07:25:23:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.25861206 * 2560; err = 0.37695313 * 2560; time = 0.1566s; samplesPerSecond = 16345.8
07/12/2016 07:25:23:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.22863464 * 2560; err = 0.36367187 * 2560; time = 0.1436s; samplesPerSecond = 17830.8
07/12/2016 07:25:23: Finished Epoch[ 1 of 2]: [Training] ce = 1.63096504 * 81920; err = 0.46358643 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=9.05254s
07/12/2016 07:25:23: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech.1'

07/12/2016 07:25:23: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 07:25:23: Starting minibatch loop.
07/12/2016 07:25:23:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.24892588 * 2560; err = 0.37382813 * 2560; time = 0.1566s; samplesPerSecond = 16349.2
07/12/2016 07:25:23:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.19152651 * 2560; err = 0.35351563 * 2560; time = 0.1561s; samplesPerSecond = 16396.2
07/12/2016 07:25:24:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.28531933 * 2560; err = 0.38359375 * 2560; time = 0.1573s; samplesPerSecond = 16277.5
07/12/2016 07:25:24:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.28695564 * 2560; err = 0.38046875 * 2560; time = 0.1537s; samplesPerSecond = 16660.4
07/12/2016 07:25:24:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.23508911 * 2560; err = 0.38320312 * 2560; time = 0.1558s; samplesPerSecond = 16433.5
07/12/2016 07:25:24:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.23924332 * 2560; err = 0.37851563 * 2560; time = 0.1579s; samplesPerSecond = 16216.0
07/12/2016 07:25:24:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.21547470 * 2560; err = 0.38125000 * 2560; time = 0.1557s; samplesPerSecond = 16439.0
07/12/2016 07:25:24:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16492844 * 2560; err = 0.35664062 * 2560; time = 0.1552s; samplesPerSecond = 16493.1
07/12/2016 07:25:25:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.19786148 * 2560; err = 0.37343750 * 2560; time = 0.1546s; samplesPerSecond = 16563.1
07/12/2016 07:25:25:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.17275467 * 2560; err = 0.35781250 * 2560; time = 0.1559s; samplesPerSecond = 16422.2
07/12/2016 07:25:25:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.28627548 * 2560; err = 0.38593750 * 2560; time = 0.1559s; samplesPerSecond = 16420.6
07/12/2016 07:25:25:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.20148163 * 2560; err = 0.37226562 * 2560; time = 0.1556s; samplesPerSecond = 16450.1
07/12/2016 07:25:25:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.20188904 * 2560; err = 0.35195312 * 2560; time = 0.1537s; samplesPerSecond = 16654.6
07/12/2016 07:25:25:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.20350189 * 2560; err = 0.35937500 * 2560; time = 0.1557s; samplesPerSecond = 16440.7
07/12/2016 07:25:25:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.16490021 * 2560; err = 0.35546875 * 2560; time = 0.1540s; samplesPerSecond = 16628.8
07/12/2016 07:25:26:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.16687012 * 2560; err = 0.36015625 * 2560; time = 0.1543s; samplesPerSecond = 16586.5
07/12/2016 07:25:26:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.19524231 * 2560; err = 0.35625000 * 2560; time = 0.1558s; samplesPerSecond = 16434.5
07/12/2016 07:25:26:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.18535919 * 2560; err = 0.36992188 * 2560; time = 0.1557s; samplesPerSecond = 16443.6
07/12/2016 07:25:26:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.16953735 * 2560; err = 0.36250000 * 2560; time = 0.1548s; samplesPerSecond = 16536.0
07/12/2016 07:25:26:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.24088135 * 2560; err = 0.37851563 * 2560; time = 0.1562s; samplesPerSecond = 16393.7
07/12/2016 07:25:26:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.08954315 * 2560; err = 0.33554688 * 2560; time = 0.1552s; samplesPerSecond = 16491.0
07/12/2016 07:25:27:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.17553406 * 2560; err = 0.34843750 * 2560; time = 0.1545s; samplesPerSecond = 16565.2
07/12/2016 07:25:27:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.08467407 * 2560; err = 0.32656250 * 2560; time = 0.1555s; samplesPerSecond = 16460.0
07/12/2016 07:25:27:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.05924072 * 2560; err = 0.32382813 * 2560; time = 0.1557s; samplesPerSecond = 16440.3
07/12/2016 07:25:27:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.11544800 * 2560; err = 0.34921875 * 2560; time = 0.1554s; samplesPerSecond = 16476.8
07/12/2016 07:25:27:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13805237 * 2560; err = 0.35156250 * 2560; time = 0.1542s; samplesPerSecond = 16596.5
07/12/2016 07:25:27:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.07155151 * 2560; err = 0.33554688 * 2560; time = 0.1556s; samplesPerSecond = 16449.3
07/12/2016 07:25:27:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.16022949 * 2560; err = 0.34570313 * 2560; time = 0.1555s; samplesPerSecond = 16459.2
07/12/2016 07:25:28:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.09731750 * 2560; err = 0.34843750 * 2560; time = 0.1555s; samplesPerSecond = 16465.5
07/12/2016 07:25:28:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10340576 * 2560; err = 0.33710937 * 2560; time = 0.1559s; samplesPerSecond = 16418.9
07/12/2016 07:25:28:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.09576416 * 2560; err = 0.34453125 * 2560; time = 0.1534s; samplesPerSecond = 16683.3
07/12/2016 07:25:28:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.10093079 * 2560; err = 0.34843750 * 2560; time = 0.1406s; samplesPerSecond = 18204.4
07/12/2016 07:25:28: Finished Epoch[ 2 of 2]: [Training] ce = 1.17330341 * 81920; err = 0.35842285 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=4.97832s
07/12/2016 07:25:28: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech'
07/12/2016 07:25:28: CNTKCommandTrainEnd: dptPre1

07/12/2016 07:25:28: Action "train" complete.


07/12/2016 07:25:28: ##############################################################################
07/12/2016 07:25:28: #                                                                            #
07/12/2016 07:25:28: # Action "edit"                                                              #
07/12/2016 07:25:28: #                                                                            #
07/12/2016 07:25:28: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 07:25:28: Action "edit" complete.


07/12/2016 07:25:28: ##############################################################################
07/12/2016 07:25:28: #                                                                            #
07/12/2016 07:25:28: # Action "train"                                                             #
07/12/2016 07:25:28: #                                                                            #
07/12/2016 07:25:28: ##############################################################################

07/12/2016 07:25:28: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 07:25:30: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 07:25:30: Loaded model with 24 nodes on GPU 0.

07/12/2016 07:25:30: Training criterion node(s):
07/12/2016 07:25:30: 	ce = CrossEntropyWithSoftmax

07/12/2016 07:25:30: Evaluation criterion node(s):

07/12/2016 07:25:30: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
000000123269BAE0: {[HL2.b Value[512 x 1]] }
000000123269BBB0: {[scaledLogLikelihood Value[132 x 1 x *3]] }
000000123269BC80: {[labels Value[132 x *3]] }
000000123269BE20: {[HL1.W Value[512 x 363]] }
000000123269BEF0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
000000123269BFC0: {[HL1.b Value[512 x 1]] }
000000123269C090: {[logPrior Value[132 x 1]] }
000000123269C160: {[HL1.t Value[512 x *3]] }
000000123269C230: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
000000123269C300: {[features Value[363 x *3]] }
000000123269C3D0: {[OL.b Value[132 x 1]] }
000000123269C4A0: {[globalPrior Value[132 x 1]] }
000000123269C640: {[err Value[1]] }
000000123269C710: {[ce Value[1]] }
000000123269C7E0: {[featNorm Value[363 x *3]] }
000000123269C8B0: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
000000123269C980: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
000000123269CA50: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
000000123269CCC0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
000000123269CD90: {[ce Gradient[1]] }
000000123269CE60: {[OL.b Gradient[132 x 1]] }
000000123269D0D0: {[globalMean Value[363 x 1]] }
000000123269D340: {[OL.W Value[132 x 512]] }
000000123269D410: {[OL.t Gradient[132 x 1 x *3]] }
000000123269D4E0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
000000123269D680: {[globalInvStd Value[363 x 1]] }
000000123269D750: {[HL2.W Value[512 x 512]] }
000000123269D820: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }

07/12/2016 07:25:30: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 07:25:30: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 07:25:34: Starting minibatch loop.
07/12/2016 07:25:34:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 4.35745811 * 2560; err = 0.80976563 * 2560; time = 0.1886s; samplesPerSecond = 13570.2
07/12/2016 07:25:35:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.79570580 * 2560; err = 0.69257813 * 2560; time = 0.1822s; samplesPerSecond = 14047.1
07/12/2016 07:25:35:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.14324646 * 2560; err = 0.58906250 * 2560; time = 0.1798s; samplesPerSecond = 14240.9
07/12/2016 07:25:35:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.88513947 * 2560; err = 0.51484375 * 2560; time = 0.1833s; samplesPerSecond = 13969.1
07/12/2016 07:25:35:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.67164993 * 2560; err = 0.46796875 * 2560; time = 0.1797s; samplesPerSecond = 14247.2
07/12/2016 07:25:35:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.56040955 * 2560; err = 0.44648437 * 2560; time = 0.1805s; samplesPerSecond = 14180.9
07/12/2016 07:25:36:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.54447632 * 2560; err = 0.45507813 * 2560; time = 0.1832s; samplesPerSecond = 13973.6
07/12/2016 07:25:36:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.45865936 * 2560; err = 0.43046875 * 2560; time = 0.2189s; samplesPerSecond = 11695.2
07/12/2016 07:25:36:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.41931305 * 2560; err = 0.41367188 * 2560; time = 0.1861s; samplesPerSecond = 13753.5
07/12/2016 07:25:36:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.36146545 * 2560; err = 0.42148438 * 2560; time = 0.1826s; samplesPerSecond = 14017.6
07/12/2016 07:25:36:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.39081421 * 2560; err = 0.40664062 * 2560; time = 0.1824s; samplesPerSecond = 14031.3
07/12/2016 07:25:37:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.37024078 * 2560; err = 0.40625000 * 2560; time = 0.1804s; samplesPerSecond = 14186.8
07/12/2016 07:25:37:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.32750397 * 2560; err = 0.39765625 * 2560; time = 0.1799s; samplesPerSecond = 14232.1
07/12/2016 07:25:37:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.34634552 * 2560; err = 0.40742187 * 2560; time = 0.1800s; samplesPerSecond = 14220.7
07/12/2016 07:25:37:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.33963623 * 2560; err = 0.40351562 * 2560; time = 0.1817s; samplesPerSecond = 14090.9
07/12/2016 07:25:37:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.32297668 * 2560; err = 0.40703125 * 2560; time = 0.1808s; samplesPerSecond = 14158.4
07/12/2016 07:25:37:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29603577 * 2560; err = 0.39687500 * 2560; time = 0.1812s; samplesPerSecond = 14125.9
07/12/2016 07:25:38:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28720703 * 2560; err = 0.39101562 * 2560; time = 0.1811s; samplesPerSecond = 14135.7
07/12/2016 07:25:38:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.25335693 * 2560; err = 0.38281250 * 2560; time = 0.1821s; samplesPerSecond = 14058.1
07/12/2016 07:25:38:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.27086792 * 2560; err = 0.38476563 * 2560; time = 0.1835s; samplesPerSecond = 13954.1
07/12/2016 07:25:38:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.26213684 * 2560; err = 0.38398437 * 2560; time = 0.1834s; samplesPerSecond = 13954.9
07/12/2016 07:25:38:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.21245422 * 2560; err = 0.37656250 * 2560; time = 0.1832s; samplesPerSecond = 13971.9
07/12/2016 07:25:39:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.23672485 * 2560; err = 0.36640625 * 2560; time = 0.1814s; samplesPerSecond = 14110.3
07/12/2016 07:25:39:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.22503357 * 2560; err = 0.38320312 * 2560; time = 0.1801s; samplesPerSecond = 14211.4
07/12/2016 07:25:39:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.17672729 * 2560; err = 0.33906250 * 2560; time = 0.1808s; samplesPerSecond = 14158.6
07/12/2016 07:25:39:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.19516296 * 2560; err = 0.35976562 * 2560; time = 0.1834s; samplesPerSecond = 13957.6
07/12/2016 07:25:39:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.22902832 * 2560; err = 0.36406250 * 2560; time = 0.1799s; samplesPerSecond = 14229.3
07/12/2016 07:25:39:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.24520874 * 2560; err = 0.38984375 * 2560; time = 0.1803s; samplesPerSecond = 14196.4
07/12/2016 07:25:40:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.21765137 * 2560; err = 0.36328125 * 2560; time = 0.1800s; samplesPerSecond = 14221.4
07/12/2016 07:25:40:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.17650452 * 2560; err = 0.35742188 * 2560; time = 0.1799s; samplesPerSecond = 14231.4
07/12/2016 07:25:40:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.20307007 * 2560; err = 0.36093750 * 2560; time = 0.1814s; samplesPerSecond = 14110.3
07/12/2016 07:25:40:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.15778809 * 2560; err = 0.34179688 * 2560; time = 0.1676s; samplesPerSecond = 15274.0
07/12/2016 07:25:40: Finished Epoch[ 1 of 2]: [Training] ce = 1.49812498 * 81920; err = 0.42536621 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=9.70049s
07/12/2016 07:25:40: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.1'

07/12/2016 07:25:40: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 07:25:40: Starting minibatch loop.
07/12/2016 07:25:41:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.16856174 * 2560; err = 0.34492187 * 2560; time = 0.1850s; samplesPerSecond = 13835.1
07/12/2016 07:25:41:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.10856857 * 2560; err = 0.31992188 * 2560; time = 0.1843s; samplesPerSecond = 13888.8
07/12/2016 07:25:41:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.19007549 * 2560; err = 0.36367187 * 2560; time = 0.1846s; samplesPerSecond = 13867.7
07/12/2016 07:25:41:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.17703590 * 2560; err = 0.35234375 * 2560; time = 0.1840s; samplesPerSecond = 13916.6
07/12/2016 07:25:41:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.14981766 * 2560; err = 0.35468750 * 2560; time = 0.1848s; samplesPerSecond = 13854.4
07/12/2016 07:25:41:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14499779 * 2560; err = 0.35234375 * 2560; time = 0.1839s; samplesPerSecond = 13918.3
07/12/2016 07:25:42:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.15067291 * 2560; err = 0.35703125 * 2560; time = 0.1836s; samplesPerSecond = 13943.0
07/12/2016 07:25:42:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.13103180 * 2560; err = 0.35078125 * 2560; time = 0.1826s; samplesPerSecond = 14021.6
07/12/2016 07:25:42:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.14212036 * 2560; err = 0.35390625 * 2560; time = 0.1829s; samplesPerSecond = 13999.6
07/12/2016 07:25:42:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.10146179 * 2560; err = 0.33437500 * 2560; time = 0.1839s; samplesPerSecond = 13919.3
07/12/2016 07:25:42:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16786346 * 2560; err = 0.35625000 * 2560; time = 0.1805s; samplesPerSecond = 14180.3
07/12/2016 07:25:43:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.13780289 * 2560; err = 0.34218750 * 2560; time = 0.1826s; samplesPerSecond = 14021.9
07/12/2016 07:25:43:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.15288086 * 2560; err = 0.33828125 * 2560; time = 0.1805s; samplesPerSecond = 14181.6
07/12/2016 07:25:43:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.13797455 * 2560; err = 0.34804687 * 2560; time = 0.1809s; samplesPerSecond = 14152.4
07/12/2016 07:25:43:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10251465 * 2560; err = 0.33281250 * 2560; time = 0.1804s; samplesPerSecond = 14191.1
07/12/2016 07:25:43:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.08231812 * 2560; err = 0.33515625 * 2560; time = 0.1812s; samplesPerSecond = 14129.5
07/12/2016 07:25:43:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.12229462 * 2560; err = 0.33984375 * 2560; time = 0.1839s; samplesPerSecond = 13920.3
07/12/2016 07:25:44:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11593323 * 2560; err = 0.34531250 * 2560; time = 0.1838s; samplesPerSecond = 13925.2
07/12/2016 07:25:44:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.13078003 * 2560; err = 0.34609375 * 2560; time = 0.1843s; samplesPerSecond = 13893.9
07/12/2016 07:25:44:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.18909912 * 2560; err = 0.36367187 * 2560; time = 0.1822s; samplesPerSecond = 14046.9
07/12/2016 07:25:44:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.06661682 * 2560; err = 0.33046875 * 2560; time = 0.1832s; samplesPerSecond = 13972.0
07/12/2016 07:25:44:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.15032043 * 2560; err = 0.34765625 * 2560; time = 0.1841s; samplesPerSecond = 13905.7
07/12/2016 07:25:45:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.06511230 * 2560; err = 0.32734375 * 2560; time = 0.1837s; samplesPerSecond = 13936.1
07/12/2016 07:25:45:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.05144958 * 2560; err = 0.31640625 * 2560; time = 0.1811s; samplesPerSecond = 14136.1
07/12/2016 07:25:45:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.08416748 * 2560; err = 0.33476563 * 2560; time = 0.1813s; samplesPerSecond = 14116.5
07/12/2016 07:25:45:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07935486 * 2560; err = 0.33945313 * 2560; time = 0.1837s; samplesPerSecond = 13935.6
07/12/2016 07:25:45:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.05736389 * 2560; err = 0.32695313 * 2560; time = 0.1819s; samplesPerSecond = 14075.2
07/12/2016 07:25:45:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.10845032 * 2560; err = 0.33671875 * 2560; time = 0.1834s; samplesPerSecond = 13957.0
07/12/2016 07:25:46:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.05507813 * 2560; err = 0.31679687 * 2560; time = 0.1835s; samplesPerSecond = 13947.6
07/12/2016 07:25:46:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.06679688 * 2560; err = 0.32890625 * 2560; time = 0.1802s; samplesPerSecond = 14204.2
07/12/2016 07:25:46:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06694031 * 2560; err = 0.33242187 * 2560; time = 0.1807s; samplesPerSecond = 14167.4
07/12/2016 07:25:46:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06564636 * 2560; err = 0.34101562 * 2560; time = 0.1694s; samplesPerSecond = 15110.9
07/12/2016 07:25:46: Finished Epoch[ 2 of 2]: [Training] ce = 1.11628447 * 81920; err = 0.34095459 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=5.85651s
07/12/2016 07:25:46: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech'
07/12/2016 07:25:46: CNTKCommandTrainEnd: dptPre2

07/12/2016 07:25:46: Action "train" complete.


07/12/2016 07:25:46: ##############################################################################
07/12/2016 07:25:46: #                                                                            #
07/12/2016 07:25:46: # Action "edit"                                                              #
07/12/2016 07:25:46: #                                                                            #
07/12/2016 07:25:46: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/12/2016 07:25:47: Action "edit" complete.


07/12/2016 07:25:47: ##############################################################################
07/12/2016 07:25:47: #                                                                            #
07/12/2016 07:25:47: # Action "train"                                                             #
07/12/2016 07:25:47: #                                                                            #
07/12/2016 07:25:47: ##############################################################################

07/12/2016 07:25:47: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/12/2016 07:25:49: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/12/2016 07:25:49: Loaded model with 29 nodes on GPU 0.

07/12/2016 07:25:49: Training criterion node(s):
07/12/2016 07:25:49: 	ce = CrossEntropyWithSoftmax

07/12/2016 07:25:49: Evaluation criterion node(s):

07/12/2016 07:25:49: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0000001225A05610: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0000001225A057B0: {[ce Gradient[1]] }
0000001225A05AF0: {[OL.t Gradient[132 x 1 x *6]] }
0000001225A05D60: {[OL.b Gradient[132 x 1]] }
000000123269BBB0: {[globalMean Value[363 x 1]] }
000000123269BC80: {[HL2.b Value[512 x 1]] }
000000123269BEF0: {[HL1.b Value[512 x 1]] }
000000123269BFC0: {[globalInvStd Value[363 x 1]] }
000000123269C090: {[HL3.b Value[512 x 1]] }
000000123269C160: {[HL3.W Value[512 x 512]] }
000000123269C230: {[featNorm Value[363 x *6]] }
000000123269C300: {[features Value[363 x *6]] }
000000123269C4A0: {[HL2.W Value[512 x 512]] }
000000123269C640: {[logPrior Value[132 x 1]] }
000000123269C710: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
000000123269C7E0: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
000000123269C8B0: {[OL.b Value[132 x 1]] }
000000123269C980: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
000000123269CA50: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
000000123269CB20: {[globalPrior Value[132 x 1]] }
000000123269CBF0: {[ce Value[1]] }
000000123269CCC0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
000000123269CD90: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
000000123269CF30: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
000000123269D000: {[HL1.W Value[512 x 363]] }
000000123269D0D0: {[labels Value[132 x *6]] }
000000123269D1A0: {[HL1.t Value[512 x *6]] }
000000123269D270: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
000000123269D340: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
000000123269D410: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
000000123269D680: {[scaledLogLikelihood Value[132 x 1 x *6]] }
000000123269D750: {[OL.W Value[132 x 512]] }
000000123269D820: {[err Value[1]] }

07/12/2016 07:25:49: No PreCompute nodes found, skipping PreCompute step.

07/12/2016 07:25:49: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/12/2016 07:25:53: Starting minibatch loop.
07/12/2016 07:25:53:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.15954781 * 2560; err = 0.84453125 * 2560; time = 0.2233s; samplesPerSecond = 11463.3
07/12/2016 07:25:53:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.66692467 * 2560; err = 0.64023438 * 2560; time = 0.2065s; samplesPerSecond = 12398.8
07/12/2016 07:25:53:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.02206192 * 2560; err = 0.56523437 * 2560; time = 0.2074s; samplesPerSecond = 12342.2
07/12/2016 07:25:53:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.76702805 * 2560; err = 0.49375000 * 2560; time = 0.2103s; samplesPerSecond = 12172.5
07/12/2016 07:25:54:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.54464188 * 2560; err = 0.43789062 * 2560; time = 0.2104s; samplesPerSecond = 12167.4
07/12/2016 07:25:54:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.44837112 * 2560; err = 0.41523437 * 2560; time = 0.2087s; samplesPerSecond = 12269.0
07/12/2016 07:25:54:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.43359833 * 2560; err = 0.42187500 * 2560; time = 0.2066s; samplesPerSecond = 12388.3
07/12/2016 07:25:54:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36250305 * 2560; err = 0.40390625 * 2560; time = 0.2102s; samplesPerSecond = 12179.1
07/12/2016 07:25:55:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.31944885 * 2560; err = 0.38125000 * 2560; time = 0.2065s; samplesPerSecond = 12399.8
07/12/2016 07:25:55:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.26297455 * 2560; err = 0.39062500 * 2560; time = 0.2091s; samplesPerSecond = 12245.1
07/12/2016 07:25:55:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.30148926 * 2560; err = 0.38554688 * 2560; time = 0.2072s; samplesPerSecond = 12354.4
07/12/2016 07:25:55:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27950897 * 2560; err = 0.37851563 * 2560; time = 0.2078s; samplesPerSecond = 12320.5
07/12/2016 07:25:55:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.23256378 * 2560; err = 0.36093750 * 2560; time = 0.2077s; samplesPerSecond = 12326.1
07/12/2016 07:25:56:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.24608154 * 2560; err = 0.38359375 * 2560; time = 0.2087s; samplesPerSecond = 12266.5
07/12/2016 07:25:56:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.24440918 * 2560; err = 0.37343750 * 2560; time = 0.2097s; samplesPerSecond = 12206.3
07/12/2016 07:25:56:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.24601135 * 2560; err = 0.37968750 * 2560; time = 0.2076s; samplesPerSecond = 12330.5
07/12/2016 07:25:56:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.22501221 * 2560; err = 0.38164063 * 2560; time = 0.2088s; samplesPerSecond = 12261.1
07/12/2016 07:25:56:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.21055298 * 2560; err = 0.37304688 * 2560; time = 0.2078s; samplesPerSecond = 12320.1
07/12/2016 07:25:57:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.18856812 * 2560; err = 0.35937500 * 2560; time = 0.2098s; samplesPerSecond = 12203.6
07/12/2016 07:25:57:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.19142151 * 2560; err = 0.35742188 * 2560; time = 0.2089s; samplesPerSecond = 12253.9
07/12/2016 07:25:57:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.18467102 * 2560; err = 0.36015625 * 2560; time = 0.2108s; samplesPerSecond = 12146.1
07/12/2016 07:25:57:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.14170532 * 2560; err = 0.35742188 * 2560; time = 0.2098s; samplesPerSecond = 12203.8
07/12/2016 07:25:57:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.17152405 * 2560; err = 0.35351563 * 2560; time = 0.2074s; samplesPerSecond = 12344.8
07/12/2016 07:25:58:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.16666870 * 2560; err = 0.36445312 * 2560; time = 0.2106s; samplesPerSecond = 12155.6
07/12/2016 07:25:58:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.12550659 * 2560; err = 0.32812500 * 2560; time = 0.2098s; samplesPerSecond = 12204.4
07/12/2016 07:25:58:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.14731445 * 2560; err = 0.35117188 * 2560; time = 0.2119s; samplesPerSecond = 12078.9
07/12/2016 07:25:58:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.16336975 * 2560; err = 0.34414062 * 2560; time = 0.2080s; samplesPerSecond = 12307.4
07/12/2016 07:25:58:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.18981628 * 2560; err = 0.36796875 * 2560; time = 0.2078s; samplesPerSecond = 12321.7
07/12/2016 07:25:59:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.16020813 * 2560; err = 0.35820313 * 2560; time = 0.2086s; samplesPerSecond = 12271.4
07/12/2016 07:25:59:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.13352051 * 2560; err = 0.34882812 * 2560; time = 0.2109s; samplesPerSecond = 12138.3
07/12/2016 07:25:59:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.16027832 * 2560; err = 0.34375000 * 2560; time = 0.2118s; samplesPerSecond = 12089.6
07/12/2016 07:25:59:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.10745239 * 2560; err = 0.32265625 * 2560; time = 0.1987s; samplesPerSecond = 12885.2
07/12/2016 07:25:59: Finished Epoch[ 1 of 4]: [Training] ce = 1.41264858 * 81920; err = 0.40400391 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=10.6094s
07/12/2016 07:25:59: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.1'

07/12/2016 07:26:00: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/12/2016 07:26:00: Starting minibatch loop.
07/12/2016 07:26:00:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.09969606 * 5120; err = 0.32890625 * 5120; time = 0.3203s; samplesPerSecond = 15986.7
07/12/2016 07:26:00:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.38625898 * 5120; err = 0.39238281 * 5120; time = 0.3052s; samplesPerSecond = 16777.0
07/12/2016 07:26:01:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.39283295 * 5120; err = 0.39921875 * 5120; time = 0.3053s; samplesPerSecond = 16768.8
07/12/2016 07:26:01:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.21238747 * 5120; err = 0.36933594 * 5120; time = 0.3046s; samplesPerSecond = 16806.9
07/12/2016 07:26:01:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14539108 * 5120; err = 0.35800781 * 5120; time = 0.3017s; samplesPerSecond = 16971.5
07/12/2016 07:26:01:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.15972404 * 5120; err = 0.35585937 * 5120; time = 0.3001s; samplesPerSecond = 17063.6
07/12/2016 07:26:02:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.12575455 * 5120; err = 0.33769531 * 5120; time = 0.3028s; samplesPerSecond = 16911.0
07/12/2016 07:26:02:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.06435776 * 5120; err = 0.32519531 * 5120; time = 0.3045s; samplesPerSecond = 16812.7
07/12/2016 07:26:02:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09842834 * 5120; err = 0.34101562 * 5120; time = 0.3015s; samplesPerSecond = 16982.2
07/12/2016 07:26:03:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.13542252 * 5120; err = 0.34707031 * 5120; time = 0.3051s; samplesPerSecond = 16782.1
07/12/2016 07:26:03:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.11245499 * 5120; err = 0.34257813 * 5120; time = 0.3012s; samplesPerSecond = 16999.1
07/12/2016 07:26:03:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.05484772 * 5120; err = 0.32265625 * 5120; time = 0.3036s; samplesPerSecond = 16866.7
07/12/2016 07:26:04:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05807037 * 5120; err = 0.33027344 * 5120; time = 0.3030s; samplesPerSecond = 16897.6
07/12/2016 07:26:04:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.08376770 * 5120; err = 0.33515625 * 5120; time = 0.3016s; samplesPerSecond = 16974.6
07/12/2016 07:26:04:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05951996 * 5120; err = 0.32792969 * 5120; time = 0.3041s; samplesPerSecond = 16834.0
07/12/2016 07:26:04:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06747437 * 5120; err = 0.33535156 * 5120; time = 0.2780s; samplesPerSecond = 18416.9
07/12/2016 07:26:04: Finished Epoch[ 2 of 4]: [Training] ce = 1.14102430 * 81920; err = 0.34678955 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=4.87627s
07/12/2016 07:26:05: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.2'

07/12/2016 07:26:05: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

07/12/2016 07:26:05: Starting minibatch loop.
07/12/2016 07:26:05:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.06460552 * 5120; err = 0.32851562 * 5120; time = 0.3057s; samplesPerSecond = 16747.1
07/12/2016 07:26:05:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.11498146 * 5120; err = 0.33867188 * 5120; time = 0.3050s; samplesPerSecond = 16789.0
07/12/2016 07:26:06:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.09012623 * 5120; err = 0.34101562 * 5120; time = 0.3044s; samplesPerSecond = 16822.2
07/12/2016 07:26:06:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.15339394 * 5120; err = 0.36074219 * 5120; time = 0.3105s; samplesPerSecond = 16492.1
07/12/2016 07:26:06:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.08357811 * 5120; err = 0.33886719 * 5120; time = 0.3050s; samplesPerSecond = 16786.3
07/12/2016 07:26:07:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07425880 * 5120; err = 0.33261719 * 5120; time = 0.3037s; samplesPerSecond = 16859.5
07/12/2016 07:26:07:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08439636 * 5120; err = 0.33125000 * 5120; time = 0.3025s; samplesPerSecond = 16928.3
07/12/2016 07:26:07:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.06031113 * 5120; err = 0.31562500 * 5120; time = 0.3043s; samplesPerSecond = 16823.3
07/12/2016 07:26:07:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09160309 * 5120; err = 0.33808594 * 5120; time = 0.3030s; samplesPerSecond = 16899.3
07/12/2016 07:26:08:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.10127335 * 5120; err = 0.34179688 * 5120; time = 0.3056s; samplesPerSecond = 16753.5
07/12/2016 07:26:08:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.07826767 * 5120; err = 0.33554688 * 5120; time = 0.3040s; samplesPerSecond = 16843.8
07/12/2016 07:26:08:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06313019 * 5120; err = 0.34121094 * 5120; time = 0.3023s; samplesPerSecond = 16939.1
07/12/2016 07:26:09:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.03379974 * 5120; err = 0.31933594 * 5120; time = 0.3036s; samplesPerSecond = 16865.7
07/12/2016 07:26:09:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.01062927 * 5120; err = 0.32304688 * 5120; time = 0.3053s; samplesPerSecond = 16772.0
07/12/2016 07:26:09:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04116974 * 5120; err = 0.31757812 * 5120; time = 0.3051s; samplesPerSecond = 16780.4
07/12/2016 07:26:10:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.05690460 * 5120; err = 0.32128906 * 5120; time = 0.2797s; samplesPerSecond = 18305.6
07/12/2016 07:26:10: Finished Epoch[ 3 of 4]: [Training] ce = 1.07515182 * 81920; err = 0.33282471 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=4.88309s
07/12/2016 07:26:10: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.3'

07/12/2016 07:26:10: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

07/12/2016 07:26:10: Starting minibatch loop.
07/12/2016 07:26:10:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.01812296 * 5120; err = 0.31796875 * 5120; time = 0.3029s; samplesPerSecond = 16905.2
07/12/2016 07:26:13:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.01859186 * 4926; err = 0.31628096 * 4926; time = 2.7346s; samplesPerSecond = 1801.4
07/12/2016 07:26:13:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.03707047 * 5120; err = 0.32382813 * 5120; time = 0.3072s; samplesPerSecond = 16664.7
07/12/2016 07:26:13:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 0.99918213 * 5120; err = 0.31425781 * 5120; time = 0.3034s; samplesPerSecond = 16875.1
07/12/2016 07:26:14:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.02076111 * 5120; err = 0.31972656 * 5120; time = 0.3024s; samplesPerSecond = 16929.9
07/12/2016 07:26:14:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00163460 * 5120; err = 0.31171875 * 5120; time = 0.3073s; samplesPerSecond = 16662.4
07/12/2016 07:26:14:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.01088715 * 5120; err = 0.31269531 * 5120; time = 0.3078s; samplesPerSecond = 16632.6
07/12/2016 07:26:15:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 0.99696808 * 5120; err = 0.30644531 * 5120; time = 0.3056s; samplesPerSecond = 16755.0
07/12/2016 07:26:15:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99020691 * 5120; err = 0.30585937 * 5120; time = 0.3042s; samplesPerSecond = 16829.7
07/12/2016 07:26:15:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.99083939 * 5120; err = 0.31230469 * 5120; time = 0.3027s; samplesPerSecond = 16915.7
07/12/2016 07:26:16:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.00112152 * 5120; err = 0.31289062 * 5120; time = 0.3043s; samplesPerSecond = 16825.3
07/12/2016 07:26:16:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.98365173 * 5120; err = 0.30937500 * 5120; time = 0.3048s; samplesPerSecond = 16800.6
07/12/2016 07:26:16:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.98795166 * 5120; err = 0.31054688 * 5120; time = 0.3055s; samplesPerSecond = 16761.9
07/12/2016 07:26:16:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.98707275 * 5120; err = 0.30605469 * 5120; time = 0.3029s; samplesPerSecond = 16901.9
07/12/2016 07:26:17:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.01551819 * 5120; err = 0.31933594 * 5120; time = 0.3016s; samplesPerSecond = 16978.6
07/12/2016 07:26:17:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.00542603 * 5120; err = 0.31074219 * 5120; time = 0.2851s; samplesPerSecond = 17961.2
07/12/2016 07:26:17: Finished Epoch[ 4 of 4]: [Training] ce = 1.00391188 * 81920; err = 0.31309814 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=7.33057s
07/12/2016 07:26:17: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160712072448.15279\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech'
07/12/2016 07:26:17: CNTKCommandTrainEnd: speechTrain

07/12/2016 07:26:17: Action "train" complete.

07/12/2016 07:26:17: __COMPLETED__